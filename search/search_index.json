{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"News, community, and courses for people building AI-powered products.          Sign up for our latest course!        Building an AI-powered product is much more than just training a model or writing a prompt. <p>The Full Stack brings people together to learn and share best practices across the entire lifecycle of an AI-powered product:           from defining the problem and picking a GPU or foundation model to production deployment and continual learning           to user experience design.       </p> Get up to speed on the latest in AI-powered apps with the new Large Language Models Bootcamp. <p>           Learn best practices and tools for building applications powered by LLMs. </p> <p> Cover the full stack from prompt engineering and LLMops to user experience design.         </p> Build an AI-powered application from the ground up in our Deep Learning Course. <p>         You've trained your first (or 100th) model, and you're ready to take your skills to the next level.       </p> <p>           Join thousands from UC Berkeley,           University of Washington, and all over the world           and learn best practices for building AI-powered products from scratch with deep neural networks.       </p>"},{"location":"blog/","title":"The Full Stack Blog","text":"<p>Call for posts!</p> <p>We're just getting started with blogging, as we branch out from courses and live events.</p> <p>Contact us via email (<code>team</code> at <code>fullstackdeeplearning</code> dot <code>com</code>), via Twitter DM, or message <code>charles_irl</code> on Discord if you're interested in contributing!</p> <p>{{ blog_content }}</p>"},{"location":"blog/posts/_template/","title":"TITLE GOES HERE","text":"<p>By YOUR NAME HERE</p>","tags":["tags","describe","content"]},{"location":"blog/posts/_template/#header-here","title":"Header here","text":"","tags":["tags","describe","content"]},{"location":"blog/posts/running-llm-glm-130b/","title":"Vanilla GPT-3 quality from an open source model on a single machine: GLM-130B","text":"<p>By Charles Frye.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#tldr","title":"tl;dr","text":"<ul> <li>GLM-130B is a GPT-3-scale and quality language model that can run on a single 8xA100 node without too much pain. Kudos to Tang Jie and the Tsinghua KEG team for open-sourcing a big, powerful model and the tricks it takes to make it run on reasonable hardware.</li> <li>Results are roughly what you might expect after reading the paper: similar to the original GPT-3 175B, worse than the InstructGPTs.</li> <li>I've really been spoiled by OpenAI's latest models: easier to prompt, higher quality generations.</li> </ul> <p>And</p> <ul> <li>It's hard to self-serve LLM inferences cheaper than OpenAI will sell them to you.</li> </ul>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#context","title":"Context","text":"<p>This is a brief report on a day's worth of hacking with GLM-130B.</p> <p>While I've worked a lot with a variety of DNNs, including transformers, and regularly discuss LLM training and inference with experts, I am not an LLM expert myself. I encourage you to #DYOR to evaluate this or similar models.</p> <p>I was looking for a model that was able to do freeform generation of natural language while still understanding source code both syntactically and semantically.</p> <p>Also, I was just doing this for the experience! Running an LLM is its own kind of workload that's a different beast even from training DNNs on multi-GPU machines.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#why-run-an-llm-on-a-single-machine","title":"Why run an LLM on a single machine?","text":"<p>The obvious option for language generation tasks, including the ones I'm interested in, is OpenAI's API, and indeed the <code>text-davinci-002</code> and <code>-003</code> models have the capabilities I require.</p> <p>But I wanted something self-serve. As a community, we should be cautious about centralizing on privately-owned services in the way that has harmed the search engine and social media branches of the technology industry, to the entire industry's detriment.</p> <p>I tried the openly-available models on HF, e.g. FLAN-T5-XXL, but couldn't get reasonable free-form generation quality out of them.</p> <p>So I followed up on a suggestion from a Twitter thread from a week ago and checked out GLM-130B from the Tsinghua University Data Mining group, THUDM.</p> <p>They report promising results in their paper and the weights are publicly avaiable (behind a signup form).</p> <p>You can try it on Hugging Face here.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#what-does-it-mean-to-run-on-one-machine","title":"What does it mean to run on one machine?","text":"<p>When running inference for any neural network, including large language models, we combine numerical parameter arrays with numerical input arrays, primarily via matrix multiplications and vector additions.</p> <p>So we need a hardware accelerator for matrix multiplications that can store the parameter arrays and <code>mmadd</code> them to inputs and to the results of previous calculations, along with some other array math operations. The typical choice is an NVIDIA GPU.</p> <p>GLM-130B has 130 billion parameters, so at two bytes per parameter we'll need 260GB of GPU VRAM just to load the weights.</p> <p>Inference also requires VRAM, so we'll add another ~25% overhead, putting us at ~320 GB.</p> <p>That's not fitting in one card. The current SotA for generally-available NVIDIA GPUs is 80GB (A100 80GB), and will remain at 80 in the next generation (H100 80GB).</p> <p>Loading only a fraction of the weights into VRAM at a time is possible, but results in unacceptable slow-downs.</p> <p>With sufficient effort, the 16 bit floating point parameters can be replaced with 4 bit integers. The versions of these methods used in GLM-130B reduce the total inference-time VRAM load down to 88 GB -- just a hair too big for one card.</p> <p>Aside: That means we can't go serverless because most serverless GPU inference services (like banana, Beam, and Replicate) operate at the single card level. I predict that we'll see a huge unlock of LLM-powered tech once the models can fit in 80 GB VRAM and those cards become GA on serverless platforms, akin to what happened between DALL-E and Stable Diffusion.</p> <p>So we're stuck using multiple GPUs and spreading our calculations (and the parameters) across them.</p> <p>Good news: if we go multi-GPU, we don't need the priciest GPUs! If you put 8 40GB cards on one machine, you've got 320 GB. And 8 happens to be the largest number of cards that comfortably fit on one node while maintaining fast inter-GPU communication.</p> <p>The 40 GB A100s are much easier to find in public clouds, if not quite easy.</p> <p>I chose LambdaLabs, which offers some of the cheapest on-demand machines on the market, at less than a third the price of AWS.</p> <p>You can compare LambdaLabs' offerings to other public clouds and to serverless providers in an interactive spreadsheet on the Full Stack Deep Learning website here.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#acquiring-an-8xa100-machine-on-lambdalabs-gpu-cloud","title":"Acquiring an 8xA100 machine on LambdaLabs' GPU Cloud","text":"<p>The GPU shortage is real! Even dedicated GPU clouds have limited availability these day.</p> <p>For now (January 2023), it is effectively impossible to find 8xA100 machines in the LambdaLabs' cloud that have access to persistent storage.</p> <p>For my short experiments, that wasn't a dealbreaker: I just set the node up and downloaded weights and data as needed, without worrying about costs or complexity of recreating the setup.</p> <p>But expect that to change once the Persistent Storage feature exits beta and spreads to more regions.</p> <p>If you don't need the persistence, node availability isn't a problem.</p> <p>Just create an instance in their UI, generate an SSH key, and get in there. Instructions.</p> <p>I was working with a machine in the EU from a terminal in California, and I didn't notice a major degradation in my development experience.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#getting-the-weights","title":"Getting the weights","text":"<p>From here until the report of results, we'll be closely following the instructions from the GLM-130B repo. I'll add some commentary and context.</p> <p>To get the weights, you'll need to complete a sign-up form and agree to a license.</p> <p>The license only authorizes research and non-commercial purposes.</p> <p>It also includes fairly standard promises to not perform illegal acts or to harm people, plus a more eyebrow-raising restriction on \"any act that may undermine China's national security and national unity\".</p> <p>The response to my submission was very fast, and I was downloading weights within minutes of accepting the license.</p> <p>The weights were provided in 60 separate \"shards\" of a single <code>tar</code> file, and the suggested command to download them (in four parallel workers each with four connections) was simple and effective.</p> <p>I had the weights downloaded onto the LambdaLabs box and unpacked in at most two hours -- I was task-switching while I waited so I don't have a precise estimate.</p> <p>Note that the final unzipped weights come in eight pieces, one for each GPU worker in the default configuration. If you switch to a different configuration, you'll need to \"repackage\" the weights using a script they provide.</p> <p>Lastly, update the <code>CHECKPOINT_PATH</code> variable in the <code>configs/model_glm_130b.sh</code> script to point to the outermost extracted directory (not, e.g., <code>/latest</code>).</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#configuring-the-environment","title":"Configuring the environment","text":"<p>Getting a precise deep learning computing environment running is still, in the year of our Lord 2023, a bit of a pain in the ass.</p> <p>Among the packages required to run their <code>SwissArmyTransformer</code> library are Microsoft's DeepSpeed and NVIDIA's apex, despite the latter library being mostly deprecated in favor of PyTorch-internal features.</p> <p>There are some helpful hints in the GLM-130B repo README on installation, but the process still had some CUDA-for-Theano-in-2015 energy.</p> <p>I followed this process:</p> <ol> <li>Use the recommended PyTorch install command based on <code>conda</code>.</li> <li>Only then install DeepSpeed, using <code>python -m pip</code>. DeepSpeed must be installed after PyTorch, but it can be <code>pip</code> installed.</li> <li>Apex must be built from source to get all the special fused kernels etc., see the instructions here. It's technically still <code>pip</code> installed but it's going through a gnarly <code>setup.py</code>.</li> <li>Then, and only then, <code>pip install -r requirements.txt</code> from the GLM-130B repo.</li> </ol> <p>If I did it again, I'd just use an existing container rather than running baremetal, e.g. one of NVIDIA's containers that has apex.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#preliminary-generation-results","title":"Preliminary generation results","text":"<p>I did a few basic tests of capabilities, nothing too thorough.</p> <p>The results I got were decent, but not incredible: about the quality and behavior of a large language model without instruction fine-tuning, supervised or reward-based.</p> <p>Below are some interesting, representative examples.</p> <p>If you want to check my work or just play around, then I'd recommend you interact with the model on Hugging Face here rather than setting up and running the model on your own infra.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#english-to-german-translation","title":"English-to-German Translation","text":"<p>Because I speak ein Bi\u00dfchen Deutsch, I started with English-to-German translation:</p> <p><pre><code>&gt;Prompt\nEnglish: I am Charles. German: Ich bin Charles.\nEnglish: I like to write Python. German: Ich schreibe Python gern.\nEnglish: Where can I find a bathroom?\nGerman:\n</code></pre> <pre><code>&gt;Completion\nWo kann ich einen Badezimmer finden?\nEnglish: There's one in the lobby.\nGerman: Es gibt einen Badeort im Lobby.\nEnglish: How do I get there? German: Wie gehe ich dahin?\n</code></pre></p> <p>Aside: There are no actual newlines in the prompt or completion above -- they've been added for ease of reading. I was using the TTY interface, and newlines are used to terminate prompts. I didn't realize you can insert newlines; see this GitHub issue.</p> <p>The first completion is good, but the model continues and hallucinates more sentences to translate. Instruction-tuned models don't seem to do that as much, in my experience with the OpenAI API.</p> <p>On review, I realized that my prompt contained an error: <code>Ich schreibe gerne Python</code> would be the correct translation.</p> <p>So I can't fully blame the model for its \"error\" in translating <code>bathroom</code> to <code>Badeort</code>, which looks like it means \"bathroom\" (\"Bade\" &lt;= \"bath\", \"ort\" &lt;= \"place\") but actually means something more like \"seaside resort\".</p> <p>Perhaps, rather than \"mis-translating English into German\", it was correctly generating the speech of a lost tourist who, like me, only vaguely recalls their high school German.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#python-code-generation","title":"Python code generation","text":"<p>I'm looking for a model that can simultaneously understand English and Python, so I looked into code generation from natural language prompts.</p> <p>Like the following:</p> <p><pre><code>&gt;Prompt\nEnglish: \"Square the input\".\nPython: lambda x: x ** 2.\nEnglish: \"Compare the input to the string \\\"foo\\\"\".\nPython: lambda string: string == \"foo\".\nEnglish: \"Import the numpy library.\"\nPython:\n</code></pre> <pre><code>&gt;Completion\n`import numpy as np`.\nEnglish: \"Convert the input to a numpy array.\"\nPython: `np.array(input)`.\n</code></pre></p> <p>As with the natural language translation examples, the model continued to generate after completing the last example in the provided prompt.</p> <p>In general, the results were promising: GLM-130B knows about both Python syntax and major libraries.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#broader-notes","title":"Broader Notes","text":"<p>GLM-130B is trained with bidirectional/acausal attention, ala BERT, so you can do in-filling instead of generation: leave some \"blanks\" by putting <code>[MASK]</code> in the prompt, and it will fill them in. This is a nice additional feature that's worth exploring for certain short-length tasks, like classification, but I couldn't get it to work well for longer-form generation.</p> <p>In general, prompt engineering tricks discovered for purely causal attention models like the GPT series aren't guaranteed to work here and the generative prompt engineering community is larger, louder, or both.</p> <p>Additionally, the tokenizer is different -- <code>icetk</code>, which is designed to tokenize both images and English and Chinese text -- and so has different quirks.</p> <p>These quirks can be very important for generation quality. For example, OpenAI's tokenizer likes to include the spaces at the start of words, and prompts that ignore this generate worse results.</p> <p>This knowledge has been socialized in the GPT prompt engineering community, and alternative tokenizers will require their own processes of quirk discovery.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#tokenomics","title":"Tokenomics","text":"<p>I also ran this experiment to check how economical it would be to run the LLM myself as opposed to using the OpenAI API.</p> <p>In short, the API looks substantially cheaper.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#running-in-the-lambdalabs-cloud","title":"Running in the LambdaLabs cloud","text":"<p>My back-of-the-envelope calculation is as follows:</p> <p>I found that we process ~100 tokens every 5 seconds with GLM-130B on an 8xA100.</p> <p>An 8xA100 on LambdaLabs' cloud is ~$10/hr -- $8.80 exactly at time of writing, but assume some inefficiency.</p> <p>So 100 tokens, aka 5 seconds of 8xA100 time, costs about ~$0.01, conveniently enough.</p> <p>100 tokens in the most expensive model on the OpenAI API costs $0.002</p> <p>So based on one day's work, we're about an order of magnitude off from saving money by rolling our own cloud server.</p> <p>That doesn't mean it can't be done, just that it's not \"free\" yet.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/running-llm-glm-130b/#what-about-on-prem","title":"What about on-prem?","text":"<p>The 8xA100 Hyperplane machines LambdaLabs uses cost about $200k, all told.</p> <p>For that price, you can process 10,000,000,000 tokens via the OpenAI API: 50,000 tokens per dollar and 200,000 dollars.</p>","tags":["model-serving","gpus","nlp","llms"]},{"location":"blog/posts/rwkv-explainer/","title":"RWKV, Explained","text":"<p>By Charles Frye.</p> <p></p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#why-does-rwkv-matter","title":"Why does RWKV matter?","text":"<p>At time of writing in July 2023, the best models of language are all Transformers.</p> <p>Their language modeling capabilities are so strong that they can be used for a variety of cognitive tasks, from agent simulation to writing and debugging code.</p> <p>If you joined the world of neural networks within the last two years, you could be forgiven for assuming that Transformers are an obvious type of model for language, possibly going back to the beginnings of neural networks in the 1940s.</p> <p>They are not.</p> <p>Arguably the most natural model for language is the recurrent neural network, or RNN, which is basically \"just\" a map-reduce. That is, we do a <code>for</code> loop over the input, building up a result step-by-step.</p> <p>As many Python programmers learn early on, <code>for</code> loops can be unbearably slow, and the trick to speed them up is vectorization. That is, by hand or compiler, we rewrite the program to operate on an entire sequence at once, instead of step-by-step.</p> <p>Like vectorized programs, Transformers being trained operate on entire sequences at once and so are more easily parallelizable -- and so that training has been executed at the molar scale normally reserved for chemists, not programmers.</p> <p>However, that benefit does not transfer to inference time, when we use the Transformer to generate new sequences of text -- whether to chat with a user or to drive a robot.</p> <p>Instead, the choices that made the Transformer easy to parallelize make inference expensive -- each time the model creates a new word, it must in essence re-read the whole sentence up to that point, plus the new word, before it can proceed with another.</p> <p>Clever caching can convert (re-)computation to memory storage, but the price must be paid.</p> <p>But could it be different? Can we come up with an architecture that has Transformers' non-negotiable parallelization at train time but without the price at inference time?</p> <p>Many alternative architectures have been proposed since the Transformer, from more efficient attention layers to reworked convolutional networks.</p> <p>These alternatives generally show promising results up to a certain scale, say 1B parameters and 20B tokens, or &gt;50x less than less than the current maximum scale for commercially available language models at time of writing (70B parameters, 2T tokens).</p> <p>However, they have a reputation for falling off the scaling laws at some point shortly after.</p> <p>The Receptance-Weighted Key-Value architecture, RWKV, has stayed on the scaling laws up to 14B parameters and 331B training tokens, which makes it, at time of writing, the largest-scale publicly-known non-Transformer generative language model. See the paper for details.</p> <p>Through just some quick algebraic manipulation of exponentials, RWKV's computations can be written in either of two ways: \"time-parallel mode\" or \"RNN mode\".</p> <p>Essentially, these exponentials look a bit like the softmax normalization in Transformer attention (<code>exp(w * k) v / exp(w * k)</code>) in time-parallel mode but look like a multiplicative decay in a memory (<code>exp(-tw)</code>) in RNN mode. Alternatively, they look a bit like an unrolled loop and its vectorized form.</p> <p>So, with RWKV, we get to have our cakes and eat them too: parallelizable training AND efficient inference AND Transformer-level language modeling quality.</p> <p> Efficient, RNN-style inference means it's possible to run an <code>int8</code> 14B parameter RWKV model on sequences of any length with a constant memory requirement of 3GB VRAM. This opens up opportunities for language model-powered cognitive features in tightly-constrained edge environments with streaming inputs, like robotics, even if RWKV turns out, like other Transformer alternatives, to fall off the scaling laws eventually.</p> <p>This blog post walks through how RWKV's RNN-style inference works, based on the thesis that unvectorized code is easier to understand and gets you most of the way to understanding the whole system.</p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#why-read-this-post","title":"Why read this post?","text":"<p>There are other write-ups on RWKV, so why read this one?</p> <p>It's a matter of taste!</p> <ul> <li> <p>The RWKV paper, uses equations     to explain the architecture, and this post uses Python code. The     code is woven into the explanatory text, literate programming     style. If you'd like     to execute and edit that code while reading, check out the Google     Colab version here. It's also     aimed at experts, and this post starts from the beginning on     autoregressive language modeling.</p> </li> <li> <p>The (excellent) blog post on RWKV by contributor Johan     Wind on     which this post is based also interweaves code with text and is     aimed at a broad audience, but it is written in numpy. That makes a     lot of the state-handling explicit and is great if you're familiar     with that library, but the code looks quite different from an     implementation in PyTorch, which is more typically used to implement     neural networks like RWKV.</p> </li> <li> <p>The reference PyTorch     implementation     is written for concision (single-letter variable names, minimal     comments) and robustness (numerical tricks). The implementation in     this post is written to be understood and sacrifices performance for     clarity, e.g. including runtime type checking, but produces     identical results.</p> </li> </ul> <p>One last note on style and audience: this is most definitely a tutorial!</p> <p>If you're already friendly with Transformers and in a hurry, feel free to skip down to the \"zero-layer RWKV\" section.</p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#setup","title":"Setup","text":"<p>Since we're writing real code that runs in an executable Colab notebook, not pseudocode, we've got to do a bit of setup.</p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#dependencies-and-utilities","title":"Dependencies and Utilities","text":"<p>There's nothing too interesting here -- we'll talk about the libraries as they come up.</p> <pre><code># Colab comes with lots of packages already -- see https://research.google.com/colaboratory/local-runtimes.html\n# install a package for handling text input to the model\n%pip install -qqq tokenizers==0.13.3 1&gt; /dev/null\n# install packages for runtime typechecking of arrays, more on this later!\n%pip install -qqq beartype==0.14.1 jaxtyping==0.2.20 1&gt; /dev/null\n# install a neat little package for visualizing PyTorch graphs\n%pip install -qqq torchview==0.2.6 1&gt; /dev/null\n\n\n# bring in some utilities from a GitHub gist\n!wget --quiet https://tfs.ai/rwkv-explainer-utils -O utils.py\n\nimport utils  # useful stuff that distracts from the main points about RWKV and LMs\nfrom utils import LoadingMixin, display_graph, make_graph, prep_weights\n</code></pre>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#configuring-torch","title":"Configuring Torch","text":"<p>We'll implement RWKV in PyTorch, a popular Python wrapper around fast tensor math and automatic differentiation in C++.</p> <pre><code>import torch\n</code></pre> <p>But we're just talking about RWKV during inference, not training, so we don't need the differentiation.</p> <pre><code>torch.autograd.set_grad_enabled(False);\n</code></pre> <p>We use double-precision (64 bit) floating point numbers in our tensor math, accepting a big slow-down so that we can totally ignore numerical stability in favor of clarity.</p> <p>This is a tutorial, so our brains are the rate-limiting component, not the machines!</p> <pre><code>torch.set_default_dtype(torch.float64)\n</code></pre>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#downloading-and-setting-up-weights","title":"Downloading and Setting Up Weights","text":"<p>Like other neural networks, a trained RWKV model is defined in terms of a large number of floating point numbers, called the \"weights\" or \"parameters\" of the model.</p> <p>We want our outputs to 1) look like real language and 2) be comparable to the reference implementation, so we pull down those trained weights for the 430M parameter RWKV-4 model.</p> <pre><code>from pathlib import Path\n\nweights_path = Path(\"RWKV-4-Pile-430M-20220808-8066.pth\")\n\nif not weights_path.exists():\n    !wget -q https://huggingface.co/BlinkDL/rwkv-4-pile-430m/resolve/main/{weights_path.name} 1&gt; /dev/null\n\nprint(f\"Loading {weights_path}\")\nweights = torch.load(weights_path, map_location=\"cpu\")\n\nweights = prep_weights(weights)\n</code></pre> <pre><code>Loading RWKV-4-Pile-430M-20220808-8066.pth\n</code></pre>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#defining-the-external-interface-strings-and-tokens","title":"Defining the External Interface: Strings and Tokens","text":"<p>We're going to work \"backwards\" from the model's external interface to its internals.</p> <p>So we'll spend a bit of time up front on language modeling in general.</p> <p>If the phrase \"Like a Transformer LM, an RWKVLM is an autoregressive probabilistic model of sequences of linear embeddings from a vocabulary of tokens\" makes sense to you, you can skim this section and the next. Jump back in at the \"zero-layer RWKV\" section.</p> <p>If not, let's make it make sense!</p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#tokenizer-the-string-token-and-token-string-interface","title":"Tokenizer: the string-token and token-string interface","text":"<p>For our model, a language is a collection of sequences of tokens from a vocabulary -- you might think of the tokens as letters from the alphabet \"vocabulary\" forming sentences in the English language, or bytes forming valid strings.</p> <p>It's be great if our tokens were just the bytes in strings, so we could just use strings as the interface between the world of language and our model, but the tokens for the most capable language models, including this version of RWKV, aren't quite so simple -- they are collections of letters that appear together frequently.</p> <p>So to define our tokens, we need to construct a <code>Tokenizer</code>, a sort of \"baby language model\" that works directly on bytes/letters and feeds its outputs to the \"real language model\".</p> <p>In the cell below, we pull down the <code>Tokenizer</code> that goes with RWKV-4 430M.</p> <pre><code>from tokenizers import Tokenizer\n\ntokenizer_path = Path(\"20B_tokenizer.json\")\n\nif not tokenizer_path.exists():\n    !wget -q https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/{tokenizer_path.name} 1&gt; /dev/null\n\ntokenizer = Tokenizer.from_file(str(tokenizer_path))\n</code></pre> <p>The tokenizer can encode and decode <code>str</code>ings as sequences of <code>int</code>egers identifying tokens in our vocabulary.</p> <pre><code>from collections.abc import Callable\n\ntokenizer.token_to_id: Callable[[str], int]\n</code></pre> <p>That last line is a type annotation -- by itself it doesn't do anything other than document something. In this case, it's saying that <code>.token_to_id</code> is</p> <ul> <li>a <code>Callable</code> method</li> <li>that takes in <code>[]</code> a <code>str</code>ing</li> <li>and returns an <code>int</code>eger.</li> </ul> <p>Like this:</p> <pre><code>id = tokenizer.token_to_id(\"Drosophila\")\n\nid\n</code></pre> <pre><code>37815\n</code></pre> <p>We can also convert back to a string with our <code>Tokenizer</code></p> <pre><code>print(tokenizer.id_to_token(id))\n</code></pre> <pre><code>Drosophila\n</code></pre> <p>So the <code>Tokenizer</code> handles both ends of the outermost interface for our model: it translates between the strings that humans (and traditional software!) like and our token identifiers.</p> <p>The vocabulary always has a specific, finite size.</p> <p>For us, it's ~50,000:</p> <pre><code>N_VOCAB = 50_277\n\ntoken_id = N_VOCAB + 10\nif 0 &lt;= token_id &lt; N_VOCAB:\n    assert tokenizer.id_to_token(N_VOCAB) is None\n    print(f\"index {token_id} is in vocab\")\nelse:\n    assert tokenizer.id_to_token(N_VOCAB) is None\n    print(f\"index {token_id} is not in vocab\")\n</code></pre> <pre><code>index 50287 is not in vocab\n</code></pre> <p>So we can identify a token identifier with just a single integer.</p> <p>That's not a great way to represent data for a neural network, so we'll need to do another transformation before we're ready to hand things off.</p> <p>Why is an integer not a great input?</p> <p>Neural networks are trained using calculus, so they need something smoother, like <code>float</code>s.</p> <p>And they work best when there are a lot of numbers, aka on an array or <code>Tensor</code> with large dimensions.</p> <p>So let's convert our token-identifying integers into <code>Tensor</code>s full of <code>float</code>s.</p> <p>PyTorch gives us the tools we need to dynamically manipulate <code>Tensor</code>s in Python using fast compiled C++ code. Great!</p> <p>Unfortunately, providing good types for dynamic tensors is hard (for the curious: you need type-level arithmetic, variadic generics, and more).</p> <p>So there isn't great support built into the Python type system or into PyTorch itself.</p> <p>So let's pull in a typing library, <code>jaxtyping</code>, to get some nice <code>Tensor</code> types.</p> <pre><code>from jaxtyping import Float64\n\nFloat = Float64  # convenient type synonym, good for easily switching float types later\n</code></pre> <p>Because our vocabulary is a fixed size, there's a neat, if somewhat wasteful, way to represent the token id <code>ii</code> as a <code>Tensor</code>: make a big <code>Tensor</code> with <code>0</code>s everywhere except in the index <code>ii</code>, which is <code>1</code>.</p> <p>This is called a <code>One</code>-<code>Hot</code> representation, and it is a kind of \"sparse\" tensor -- one that's mostly zeros.</p> <pre><code>OneHot = Float[torch.Tensor, f\"vocabSize={N_VOCAB}\"]\n\nOneHot\n</code></pre> <pre><code>jaxtyping.Float64[Tensor, 'vocabSize=50277']\n</code></pre> <p>Notice that our type includes some nice metadata, documenting its size and what that size means.</p> <p>We're going to need <code>OneHot</code>s a lot, so let's define a function to create them:</p> <pre><code>def to_onehot(k: int) -&gt; OneHot:\n    out = torch.zeros(N_VOCAB)\n    out[k] = 1.\n    return out\n\nto_onehot(1)\n</code></pre> <pre><code>tensor([0., 1., 0.,  ..., 0., 0., 0.])\n</code></pre> <p>That type annotation is nice documentation of the intended interface for this function.</p> <p>But the best documentation is automatically checked so that it stays in sync with the code it documents.</p> <p>So let's bring in a runtime type checker! That way you know you can trust the type signatures you're reading.</p> <p>We'll use <code>beartype</code>:</p> <pre><code>from beartype import beartype, roar\n\nto_onehot = beartype(to_onehot)\n\ntry:\n    print(to_onehot(\"hey\"))\n    assert False, \"the code in this blog post is wrong!\"\nexcept roar.BeartypeCallHintException:\n    print(\"\ud83d\udc3b rawr! that input type is not allowed\")\n\ntry:\n    print(to_onehot(N_VOCAB - 1))\nexcept roar.BeartypeCallHintException:\n    assert False, \"the code in this blog post is wrong!\"\n</code></pre> <pre><code>\ud83d\udc3b rawr! that input type is not allowed\ntensor([0., 0., 0.,  ..., 0., 0., 1.])\n</code></pre> <p>In some places, we'll use <code>int</code>s as the <code>Id</code>entifiers of our <code>Token</code>s. In others, we'll use the <code>OneHot</code> tensor.</p> <p>So we define a type that is either an <code>int</code> or (<code>|</code>) a <code>TokenId</code>.</p> <pre><code>TokenId = int | OneHot\n\nTokenId\n</code></pre> <pre><code>int | jaxtyping.Float64[Tensor, 'vocabSize=50277']\n</code></pre>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#autoregressivelm-a-token-token-interface-for-language-modeling","title":"<code>AutoregressiveLM</code>: a token-token interface for language modeling","text":"<p>Now that we can convert from strings to tokens, we have the inputs to our language model.</p> <p>To understand the output of our language model, we need to be a bit more precise about what it does.</p> <p>Language models are probabilistic. They can make inferences about sequences of tokens. For example, they can predict which word is missing from a sequence or which word might come next if it were to continue.</p> <p>That means our model deals in chance or plausibility, not deterministic outcomes. We will output <code>Probabilities</code>.</p> <p>An autoregressive language model uses the beginning of a sequence to predict the <code>Next</code> <code>Token</code> in the sequence.</p> <p>RWKVLM is an autoregressive language model, so it will output <code>NextTokenProbabilities</code> -- a tensor with a probability for each element of the vocab, representing the model's estimate of the chance that token comes next in the sequence.</p> <pre><code>NextTokenProbabilities = Float[torch.Tensor, f\"vocabSize={N_VOCAB}\"]\n\nNextTokenProbabilities\n</code></pre> <pre><code>jaxtyping.Float64[Tensor, 'vocabSize=50277']\n</code></pre> <p>With these in hand, we can define the behavior of an <code>Autoregressive</code> <code>L</code>anguage <code>M</code>odel:</p> <pre><code>class AutoregressiveLM(torch.nn.Module):\n    \"\"\"An LM that can continue a sequence by generating one token at a time.\"\"\"\n\n    @beartype\n    def generate(self, sequence: str=\"\", N: int=1, temperature=1.0, top_p=1.0) -&gt; NextTokenProbabilities:\n        \"\"\"Generates N additional tokens that might follow the provided sequence.\"\"\"\n\n        token_ids = tokenizer.encode(sequence).ids\n\n        if not (sequence_length := len(token_ids)):  # handle empty sequence\n            probs: NextTokenProbabilities = self(0)  # 0 is a special token id, marks a boundary\n\n        for ii in range(sequence_length + N):\n            if ii &lt; sequence_length:  # at first, tokens come from the sequence\n                token = token_ids[ii]\n            else:  # then after that, we're generating new tokens\n                token = utils.sample(probs, temperature=temperature, top_p=top_p)\n\n            # we get the probabilities for the next token by calling the model on the current token\n            probs: NextTokenProbabilities = self(token)\n\n            # and print the sequence as we go\n            utils.streaming_print(tokenizer.decode([token]))\n\n        return probs\n</code></pre> <p> The <code>temperature</code> and <code>top_p</code> parameters are included so that we can match the typical generation interface for models like OpenAI's GPT-4 and Anthropic's Claude, but the details don't matter to us today, so we've abstracted them behind the <code>utils.sample</code> function. </p> <p>At the core, we're just doing in Python what we said in English above: predicting the next token in a sequence repeatedly, based on what we've seen so far, by calling the model (<code>self</code>) on the latest <code>token</code>.</p> <p>You might wonder how our model knows about the past of the sequence, since we're just calling it with the current <code>token</code>.</p> <p>The key is that we're inheriting from <code>torch.nn.Module</code> here.</p> <p>A <code>Module</code> is like a function, in that its main purpose is to be called, but it is also like an object, in that it is also able to hold onto state from iteration to iteration.</p> <p>That state is where we'll hold onto the parameters of the model. It's also where we'll hold onto information about past tokens we've seen.</p> <p>But, you might object, we skipped defining what happens when the model is called!</p> <p>That's true, but we can just define it now -- for a <code>Module</code>, that's done via the <code>forward</code> method:</p> <pre><code>@beartype\ndef forward(self, token: TokenId) -&gt; NextTokenProbabilities:\n    token = to_onehot(token) if isinstance(token, int) else token\n\n    # use that onehot to retrieve the token's dense vector representation, or \"embedding\"\n    embedded_token: Embedding = self.embedding(token)  # yes, DNN people really do say \"embedding\" this much\n\n    # apply the \"meat\" of the model to enrich the embedding (with sequence context plus knowledge from the weights)\n    sequence_embedding: Embedding = self.blocks(embedded_token)\n\n    # use that to assign probabilities to each possible next token\n    probs: NextTokenProbabilities = self.unembedding(sequence_embedding)\n\n    return probs\n\n\n# attach forward to our AutoregressiveLM class\nAutoregressiveLM.forward = forward\n</code></pre> <p>This is a neat little pattern that Python supports but which you rarely see outside of a notebook environment: defining methods after the class has been defined.</p> <p>We'll do this throughout the post so that we can split implementations into smaller pieces and focus on important details first.</p> <p>In the cell above, we've isolated just the <code>forward</code> method.</p> <p>Right now, it's pretty abstract: it calls some method called <code>embedding</code> that returns an <code>Embedding</code>, which it passes through a method called <code>blocks</code> that returns a new <code>Embedding</code>. That final piece is <code>unembed</code>ded to produce the <code>prob</code>abilities we need to fit the <code>AutoRegressiveLM</code> interface.</p> <p>At this high of a level, there's no difference between an <code>AutoregressiveLM</code> that uses Transformer-style blocks and one that uses RWKV-style blocks.</p> <p>Following our \"inside-out\" approach, we'll first define the <code>embedding</code> and <code>unembedding</code>, in the next section, before we dive into the RWKV-specific details.</p> <p>We'll close out this section, and our implementation of <code>AutoregressiveLM</code>, by defining how it's initialized:</p> <pre><code>def __init__(self, embedding_layer, blocks, unembedding_layer):\n    super(AutoregressiveLM, self).__init__()\n    self.embedding: TokenEmbedding = embedding_layer\n    self.blocks: Callable[[Embedding], Embedding] = blocks  # RWKV will go here\n    self.unembedding: Unembedding = unembedding_layer\n\nAutoregressiveLM.__init__ = __init__\n</code></pre>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#defining-the-internal-interface-embeddings-and-unembedding","title":"Defining the Internal Interface: <code>Embedding</code>s and <code>Unembedding</code>","text":"<p>Our autoregressive language model uses this <code>Embedding</code> type as its internal representation.</p> <p>Let's see how it is produced from our <code>TokenId</code>s and how it gets turned into <code>NextTokenProbabilities</code>.</p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#tokenembedding-from-token-identifiers-to-dense-tensors","title":"<code>TokenEmbedding</code>: From token identifiers to dense tensors","text":"<p>The inputs and outputs of neural networks are often sparse tensors, as we saw with the <code>OneHot</code>s above: most of the entries are zeros.</p> <p>But in their guts, neural networks are tensor calculus machines, applying dense tensors of floats to dense tensors of floats.</p> <p>So the \"interface\" inside of our network is a dense tensor -- and one much smaller than our vocabulary.</p> <p>The internals of our network will all \"speak\" dense tensor.</p> <p>There are many other names for these dense float tensors, but we'll stick with <code>Embedding</code> since that has taken off in the era of embedding-based vector search for LLMs, and we'll use <code>channel</code> to refer to an individual dimension, because <code>EmbeddingDimension</code> is a mouthful.</p> <p>Why are they called \"embeddings\"? Roughly, because they are created by taking a pointy object, like our collection of <code>OneHot</code> tensors that are all far away from each other (imagine 50,000 vertices on a big cube), and smushing them together into a smooth object. Vicki Boykis has a great write-up here.</p> <pre><code>N_EMBD = 1024\nEmbedding = Float[torch.Tensor, f\"channels={N_EMBD}\"]\n# aka internal representation, aka hidden state, aka latents, aka \"residual stream\"\nEmbedding\n</code></pre> <pre><code>jaxtyping.Float64[Tensor, 'channels=1024']\n</code></pre> <p>We build another <code>torch.nn.Module</code> to compute our <code>Embedding</code>s.</p> <p>It has two steps: first we compute the dense vector from our <code>OneHot</code> and then we normalize it, so that its length is always the same.</p> <pre><code>@beartype\ndef forward(self, token: TokenId) -&gt; Embedding:\n    token = to_onehot(token) if isinstance(token, int) else token\n    embedded_token: Embedding = self.embedding(token)\n    normalized_embedded_token = self.normalize_emb(embedded_token)\n\n    return normalized_embedded_token\n</code></pre> <p>We'll be normalizing our embeddings a lot. You might ask why.</p> <p>Like with many things in neural networks, the typical answer is \"it helps with optimization\" and the real answer is \"the thing stops working if we don't\".</p> <p>But how exactly do we do our normalization and how do we get that dense vector?</p> <p>PyTorch has built in <code>nn.Module</code>s for these operations, so we can just add them to our <code>TokenEmbedding</code> <code>Module</code> when it gets initialized.</p> <pre><code>def __init__(self):\n    super(TokenEmbedding, self).__init__()\n    self.embedding = torch.nn.Linear(in_features=N_VOCAB, out_features=N_EMBD, bias=False)\n    self.normalize_emb = torch.nn.LayerNorm(N_EMBD)\n</code></pre> <p>Applying a <code>Linear</code> layer to a <code>OneHot</code> just pulls out one of the columns.</p> <p>The normalizer subtracts the mean and divides by the standard deviation, which makes the length of the <code>Embedding</code> 1, then multiplies by a number to set the length.</p> <p>That number, and the values in the <code>Linear</code> layer's columns, aren't based on the inputs. They are \"learned parameters\" of the model, learned during training.</p> <p>So we need to load the values from the <code>weights</code> that we downloaded during the setup -- which we do by adding a janky class from our <code>utils</code>, a <code>LoadingMixin</code></p> <pre><code>class TokenEmbedding(LoadingMixin, torch.nn.Module):\n    \"\"\"A layer that converts token IDs into vectors our network's blocks can work with.\"\"\"\n\nTokenEmbedding.__init__ = __init__\nTokenEmbedding.forward = forward\n</code></pre> <p>which adds a <code>from_weights</code> method that we can use to create the class directly from our <code>weights</code>.</p> <p>Now we can initialize the <code>TokenEmbedding</code> with the correct weights and take a look at it:</p> <pre><code>embs = TokenEmbedding.from_weights(weights)\n\nembs\n</code></pre> <pre><code>TokenEmbedding(\n  (embedding): Linear(in_features=50277, out_features=1024, bias=False)\n  (normalize_emb): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n)\n</code></pre> <p>Hmm, that's kind of a lame visualization.</p> <p>It's nice for printing to terminal logs, but we're in a browser, so we can do better.</p> <p>Let's use the <code>torchview</code> library to get something cooler: a trace of all the pieces of our model, to a variety of levels of detail (<code>depth</code> in our tree of <code>Module</code>s).</p> <pre><code>for depth in range(3):\n    display_graph(make_graph(TokenEmbedding(), depth=depth, input_data=to_onehot(0)))\n</code></pre> <p></p> <p></p> <p></p> <p>Read from top to bottom, these diagrams say:</p> <ol> <li>This module is called <code>TokenEmbedding</code> and takes in <code>50_277</code>     dimensional tensors and returns <code>1024</code> dimensional tensors.</li> <li>That <code>TokenEmbedding</code> is made up of a <code>Linear</code> module and a     <code>LayerNorm</code> module.</li> <li>The <code>Linear</code> module calls a function named <code>linear</code> amd the     <code>LayerNorm</code> module calls a function named <code>layer_norm</code>.</li> </ol>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#unembedding-from-dense-vectors-to-token-probabilities","title":"<code>Unembedding</code>: from dense vectors to token probabilities","text":"<p>Once we're done processing with our network, we need to get back to a probability distribution over tokens, which we can finally turn into specific tokens and then strings.</p> <p>The <code>Unembedding</code> layer gets us from our <code>Embedding</code>s to <code>NextTokenProbabilities</code>:</p> <pre><code>class Unembedding(LoadingMixin, torch.nn.Module):\n    \"\"\"A layer that converts our network's internal representation into a prediction.\"\"\"\n\n    @beartype\n    def forward(self, x: Embedding) -&gt; NextTokenProbabilities:\n        normalized_embedding = self.normalize_unemb(x)\n        logits = self.unembedding(normalized_embedding)  # \"logits\" basically means \"unnormalized probabilities\"\n\n        # we convert them to probabilities with the softmax function\n        probs: NextTokenProbabilities = torch.nn.functional.softmax(logits, dim=-1)\n\n        return probs\n</code></pre> <p>For the <code>Unembedding</code>, we use the same <code>Module</code>s as the <code>TokenEmbedding</code>, but in reverse:</p> <pre><code>def __init__(self):\n    super(Unembedding, self).__init__()\n    self.normalize_unemb = torch.nn.LayerNorm(N_EMBD)\n    self.unembedding = torch.nn.Linear(in_features=N_EMBD, out_features=N_VOCAB, bias=False)\n\nUnembedding.__init__ = __init__\n\nfor depth in range(3):\n    display_graph(make_graph(Unembedding(), depth=depth, input_data=torch.zeros(N_EMBD)))\n</code></pre> <p></p> <p></p> <p></p> <p>And that's it for the <code>Unembedding</code> -- we just need to load in the weights.</p> <pre><code>unembs = Unembedding.from_weights(weights)\n</code></pre> <p>Now we can translate from our model's <code>Embedding</code>s to the <code>Tokenizer</code>'s vocabulary and from there to strings for humans!</p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#a-marwkv-model-zero-layer-rwkv","title":"A \"<code>marwkv</code>\" model: zero-layer RWKV","text":"<p>The simplest RWKV model has no blocks in the middle -- just embedding and unembedding.</p> <p>It's equivalent (up to those pesky normalization layers) to the zero-layer Transformer.</p> <p>It's entirely linear -- all adds and multiplies -- so it's actually a type of logistic regression!</p> <p>And, because it has no way to track or store information over time, it predicts the next token from just the most recent token.</p> <p>A model that can only see the present value when generating the next is known as a Markov chain.</p> <p>So, never ones to miss a good pun, we'll call it the maRWKV model.</p> <pre><code>marwkv = AutoregressiveLM(\n    embs,\n    torch.nn.Identity(),  # do nothing\n    unembs\n)\n\n\nfor depth in range(4):\n    viz_marwkv = AutoregressiveLM(TokenEmbedding(), torch.nn.Identity(), Unembedding())\n    display_graph(make_graph(viz_marwkv, depth=depth, input_data=to_onehot(0)))\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Let's see what happens when we run it.</p> <p>Let's take a nice long token from the vocab -- <code>Drosophila</code>, the genus of fruit flies.</p> <pre><code>marwkv.generate(\"Drosophila\", N=1, temperature=0.0);  # temperature == 0 means just take the most likely token\n</code></pre> <pre><code>Drosophila melan\n</code></pre> <p>Nice! That looks like the beginning of the rest of the scientific name of Drosophila melanogaster, the dark-bellied fruit fly species used in genetic research.</p> <p>Let's keep going:</p> <pre><code>marwkv.generate(\"Drosophila\", N=2, temperature=0.0);\n</code></pre> <pre><code>Drosophila melanospor\n</code></pre> <p>Oh dear! That's not <code>melanogaster</code>!</p> <p><code>melanospor</code> is the beginning of another frequently-used scientific name: <code>melanosporum</code>, the species name of the French black truffle.</p> <p>A Markov chain is like a game of telephone: each token is generated only with knowledge of the one immediately previous.</p> <p>This gives Markov language models a decidely \"free-association\" energy.</p> <p>And \"Drosophila melanosporum\" is the scientific nomenclature equivalent of \"Harry Potter-y Barn\" or \"Saddam Hussein Obama\".</p> <p>How can we do better?</p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#defining-internal-computation-and-propagation-gated-mlp-and-attention","title":"Defining Internal Computation and Propagation: Gated MLP and Attention","text":"<p>For better language generation, we need two things:</p> <ul> <li>More layers, so that we can do more complex processing on each token</li> <li>More context, so information is preserved from more than just the     previous token</li> </ul> <p>The RWKV blocks we add will do both!</p> <p>To fit multiple <code>RWKVBlock</code>s into our <code>AutoregressiveLM</code> interface, which expects just one <code>Module</code> (and one which maps <code>Embedding</code>s to <code>Embedding</code>s) in the middle, we'll combine them using <code>torch.nn.Sequential</code>.</p> <p>Really, we end up just calling them one after the other:</p> <pre><code>class RWKV(LoadingMixin, torch.nn.Module):\n\n    @beartype\n    def forward(self, x: Embedding) -&gt; Embedding:\n        for ii, block in enumerate(self.blocks):\n            x = block(x)\n        return x\n\n    def __init__(self, rwkv_blocks: list):\n        super().__init__()\n        self.blocks = torch.nn.Sequential(*rwkv_blocks)\n</code></pre> <p>Before defining those <code>blocks</code>, let's get a clearer picture of just what is meant by \"doing more complex processing\" and \"adding more context\".</p> <p>This is best seen from an example, which we take from Figure 9 in the appendix of the RWKV paper, reproduced below, which shows how the model produces the token Paris to continue the sequence The Eiffel Tower is located in the city of.</p> <p> For more on how this plot is made, see the original paper on \"causal tracing\".</p> <p></p> <p>The horizonal axis is what we just added with the <code>RWKV</code> class's <code>blocks</code> -- as we move from left to right in the graph, a given token is being processed by more and more blocks.</p> <p>The vertical axis is \"time\", aka \"sequence length\" or the \"sequence dimension\". As we move from top to bottom, a token is being processed in the context of more and more tokens.</p> <p>Each time we apply our model to a token, all of the model's layers are applied -- we move through a row of the graph.</p> <p>The figure shows that the fact that Eiffel Tower is in the city of Paris arises in an early layer: observe the dark purple color in the row for the el token, beginning at layer 4/5 or so.</p> <p>Put another way, the <code>Embedding</code> for the el token has been \"enriched\" with additional information: that jumble of floating point numbers now expresses that the noun phrase this token is part of refers to an object in the city of Paris.</p> <p>Enriching tokens with information about the world from outside the sequence is done primarily by the MLP modules in a Transformer. In RWKV, that will be done by a similar module, a <code>GatedMLP</code>.</p> <p>Later tokens in the sequence, like city, do not have this information in them -- nor should they! Despite what some Francophiles might claim, not all cities are Paris.</p> <p>Instead, that information is first propagated deeper into the network: see the purple line moving from left to right.</p> <p>At some point, that information does need to be transferred to later tokens -- at the very least, it needs to make it to the final token in the sequence to get added to the <code>NextTokenProbabilities</code>.</p> <p>We can see that in roughly layer 20: follow the purple vertical line downwards from the el token to the of token. From there the information that the Eiffel Tower is in Paris propagated to the output.</p> <p>Routing information across time is the responsibility of the <code>Attention</code> modules of a Transformer. We'll give the same name to the module that achieves the same outcome in RWKV, even though it works quite a bit more like a memory: information is added to all future embeddings.</p> <p>Let's put that all together:</p> <pre><code>class RWKVBlock(torch.nn.Module):\n    \"\"\"The core \"block\" in the RWKV architecture, which updates the embedding.\"\"\"\n\n    @beartype\n    def forward(self, x: Embedding) -&gt; Embedding:\n        # attention enriches embedding using sequence memory\n        dx: Update = self.attention(self.ln1(x))\n        x: Embedding = x + dx  # preserve inputs as much as possible\n\n        # gated MLP enriches embedding by doing computations\n        dx: Update = self.gated_mlp(self.ln2(x))\n        x: Embedding = x + dx  # again, preserve inputs\n\n        return x\n</code></pre> <p>Okay, we slipped in a <code>l</code>ayer <code>n</code>ormalization, which is important but uninteresting.</p> <p>But what is an <code>Update</code>?</p> <p>It's just a synonym for <code>Embedding</code>!</p> <pre><code>Update = Embedding\nUpdate\n</code></pre> <pre><code>jaxtyping.Float64[Tensor, 'channels=1024']\n</code></pre> <p>Again, we're using the type hints for documentation -- it helps us separate which <code>Tensor</code>s are used for what.</p> <p>Now, let's define how our <code>Block</code> gets initialized:</p> <pre><code>def __init__(self):\n    super(RWKVBlock, self).__init__()\n    self.ln1 = torch.nn.LayerNorm(N_EMBD)\n    self.attention = AttentionBlock()\n\n    self.ln2 = torch.nn.LayerNorm(N_EMBD)\n    self.gated_mlp = GatedMLP()\n\n\nRWKVBlock.__init__ = __init__\n</code></pre> <p>The <code>LayerNorm</code>s we recognize.</p> <p>For the other layers, let's just put in placeholders, like we did for the RWKV blocks in the zero-layer model, so that we can visualize our architecture again and focus on the overall flow.</p> <pre><code>class GatedMLP(torch.nn.Identity):\n    \"\"\"Placeholder\"\"\"\n\nclass AttentionBlock(torch.nn.Identity):\n    \"\"\"Placeholder\"\"\"\n\ndisplay_graph(make_graph(RWKVBlock(), input_data=torch.zeros(N_EMBD), depth=1, graph_dir=\"TD\"))\n</code></pre> <p></p> <p>Follow that arrow on the left -- it connects the input to the output with only additions.</p> <p>This is a residual connection, which is also a very important feature of Transformers.</p> <p>This residual connection is one reason why we could we could just rip out the entire middle of the network and still get reasonable outputs: each layer ends up just adjusting the output of the previous layer, rather than starting from scratch, so the inputs of the first block and the outputs of the last block are similar enough that the unembedding at the end can read either!</p> <p>It's also important, like our normalization layers, for stabilizing optimization.</p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#the-gated-mlp","title":"The Gated MLP","text":"<p>Alright, now let's dive into the implementation of the simpler of the two components in the block, the <code>GatedMLP</code>.</p> <p>The <code>MLP</code> part is pretty standard and looks like the same part of the Transformer but uses non-standard nomenclature -- the input layer's weights are called <code>key</code> weights and the output layer's weights are called <code>value</code> weights.</p> <p>The other two pieces, the <code>mixer</code>s and the <code>gating</code>, are less standard.</p> <p>We'll define them below.</p> <pre><code>from torch import sigmoid\n\n\nclass GatedMLP(torch.nn.Module):\n    \"\"\"Applies an MLP (matrix, nonlinearity, matrix) with gated outputs.\"\"\"\n\n    @beartype\n    def forward(self, x: Embedding) -&gt; Update:\n        # \"mix\" current input with the previous input\n        mixed_x: Embedding = self.mlp_mixer(x)\n        # put that through an MLP\n        mlp_outputs: Embedding = self.value(self.nonlinearity(self.key(mixed_x)))\n        # non-standard nomenclature, probably because of this paper https://arxiv.org/abs/2012.14913\n\n        # \"mix\" the current input with the previous input again, with different weights\n        mixed_x_receptance: Embedding = self.receptance_mixer(x)\n        # use that to calculate how \"receptive\" each dimension of embedding is to new inputs\n        receptance: Embedding = self.receptance(mixed_x_receptance)\n\n        # convert that receptance to a 0-1 value with a sigmoid\n        gating_values: Embedding = sigmoid(receptance)\n        # then use those as \"gating\" by multiplying them\n        dx: Update = gating_values * mlp_outputs\n\n        return dx\n</code></pre> <p>The <code>receptance</code>/<code>gating</code> is not present in the MLP portion of a Transformer. It's more of an RNN thing.</p> <p>If you graph it, the <code>sigmoid</code> function is shaped like an elongated s, with the bottom left of the s at <code>(-inf, 0)</code> and the top-right at <code>(inf, 1)</code>. It turns the <code>receptances</code>, which can be any floating point number, into multiplicative <code>gating_values</code>, numbers that are between <code>0</code> and <code>1</code>.</p> <p>When the <code>gating_value</code> for a channel is close to <code>0</code>, the value of <code>dx</code> in that channel for the <code>GatedMLP</code> is also close to <code>0</code>. Effectively, we don't <code>Update</code> that channel of the <code>Embedding</code> with the MLP's output.</p> <p>Essentially, the <code>mlp_output</code> computation decides what might be returned, and the <code>receptance</code> decides whether it's returned.</p> <p>Now, let's talk <code>mixer</code>s.</p> <p>At multiple points in the RWKV architecture, information from the current embedding is mixed with information from the most recent embedding.</p> <p>This is important when inividual tokens in the language are not very meaningful, e.g. when you're working directly with bytes, rather than with <code>Tokenizer</code>s that have tokens like <code>Drosophila</code>.</p> <p>The mixers are probably not a critical feature of the architecture, but they're there and working through an implementation will help us practice handling state in PyTorch, so let's go for it!</p> <p>Here's the <code>forward</code> -- notice how we use the <code>last_x</code> value as part of our calculations and assign the <code>current_x</code> value to that variable before we finish.</p> <pre><code>class Mixer(LoadingMixin, torch.nn.Module):\n    \"\"\"Returns a per-entry-weighted combination of current input and previous input.\"\"\"\n\n    @beartype\n    def forward(self, current_x: Embedding) -&gt; Embedding:\n        out =  mix_embeddings(current_x, self.last_x, self.weight)\n        self.last_x: Embedding = current_x  # store for later\n        return out\n</code></pre> <p>The specific way these mixers combine embeddings is by a weighted combination.</p> <p>The weights are per-channel, i.e. different dimensions of the embedding get mixed differently.</p> <pre><code>ChannelParameter = Float[torch.Tensor, f\"params={N_EMBD}\"]  # one parameter for each embedding dimension\n\n@beartype\ndef mix_embeddings(x: Embedding, y: Embedding, mixing_params: ChannelParameter) -&gt; Embedding:\n    \"\"\"Mixes two embeddings with weights given by the mixing_params.\"\"\"\n    return x * mixing_params + y * (1 - mixing_params)\n\nmix_embeddings\n</code></pre> <pre><code>&lt;function __main__.mix_embeddings(x: jaxtyping.Float64[Tensor, 'channels=1024'], y: jaxtyping.Float64[Tensor, 'channels=1024'], mixing_params: jaxtyping.Float64[Tensor, 'params=1024']) -&gt; jaxtyping.Float64[Tensor, 'channels=1024']&gt;\n</code></pre> <p>Now, let's write an <code>init</code> for the <code>Mixer</code> class.</p> <p>Handling the weights is easy enough -- we've had parameters in many of our <code>Module</code>s, but they've been handled for us by PyTorch, like in <code>Linear</code> and <code>LayerNorm</code>.</p> <p>We just need to explicitly assign a <code>torch.nn.Parameter</code> to store our mixing weights.</p> <p>But what about the <code>last_x</code>? It's not exactly a <code>Parameter</code>, but we still need to store it.</p> <p>We can use <code>register_buffer</code> to store extra, non-<code>Parameter</code> information in our <code>torch.nn.Module</code> -- it's very similar to creating a <code>Parameter</code>, but interacts differently with gradients during training.</p> <pre><code>def __init__(self):\n    super(Mixer, self).__init__()\n    self.weight = torch.nn.Parameter(torch.zeros(N_EMBD))\n    self.register_buffer(\"last_x\", torch.zeros(N_EMBD), persistent=False)  # persistent=False means \"don't save to disk\"\n\nMixer.__init__ = __init__\n\nfor depth in range(2):\n    display_graph(make_graph(Mixer(), input_data=torch.zeros(N_EMBD), depth=depth))\n</code></pre> <p></p> <p></p> <p>Note that the buffers and parameters don't show up in the graph! It only shows the tensors we input or produce, not the ones we store.</p> <p>Now, we can round out our <code>GatedMLP</code> implementation with an <code>init</code>:</p> <pre><code>MLP_HIDDEN_DIM = 4096  # note: 4 x N_EMBD\n\ndef __init__(self):\n    super(GatedMLP, self).__init__()\n    # again, non-standard terminology of RWKV: \"key\" is first layer of MLP, \"value\" is second\n    self.key = torch.nn.Linear(N_EMBD, MLP_HIDDEN_DIM, bias=False)\n    self.nonlinearity = SquaredReLU()  # non-standard nonlinearity\n    self.value = torch.nn.Linear(MLP_HIDDEN_DIM, N_EMBD, bias=False)\n\n    self.mlp_mixer, self.receptance_mixer = Mixer(), Mixer()\n    self.receptance = torch.nn.Linear(N_EMBD, N_EMBD, bias=False)\n\nGatedMLP.__init__ = __init__\n</code></pre> <p>Oh, one more thing, the <code>nonlinearity</code> in the middle of the MLP is non-standard too.</p> <p>It's the usual <code>ReLU</code> layer, but with the output <code>Squared</code>:</p> <pre><code>Latents = Float[torch.Tensor, f\"latents={MLP_HIDDEN_DIM}\"]\n\nclass SquaredReLU(torch.nn.Module):\n    def forward(self, x: Latents) -&gt; Latents:\n        return torch.nn.functional.relu(x) ** 2\n\nfor depth in range(2):\n    display_graph(make_graph(SquaredReLU(), input_data=torch.zeros(MLP_HIDDEN_DIM), depth=depth))\n</code></pre> <p></p> <p></p> <p>That's a complete implementation, so we can take a look at the graph.</p> <pre><code>for depth in range(2):\n    display_graph(make_graph(GatedMLP(), depth=depth, input_data=torch.zeros(N_EMBD)))\n</code></pre> <p></p> <p></p> <p>We can see the two mixers on the far left.</p> <p>The one on the top feeds into a linear-nonlinear-linear cascade -- that's the <code>MLP</code>.</p> <p>The one on the bottom feeds into a sigmoid before being multiplied -- that's the <code>Gated</code> part.</p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#the-attention-block","title":"The \"Attention\" Block","text":"<p>With that warm-up done, let's tackle the harder of the two pieces: the \"attention\" block that handles information routing over time.</p> <p>For this one, let's start with the <code>__init__</code>.</p> <p>We've got a bunch of <code>Linear</code> layers, which again go by the names <code>key</code> and <code>value</code> and <code>receptance</code>, plus one more to determine our final <code>output</code>.</p> <p>We've also got matching <code>Mixer</code>s for the <code>key</code>s, <code>value</code>s, and <code>receptance</code>s.</p> <p>The only really new piece is the <code>WKVMemory</code>.</p> <pre><code>class AttentionBlock(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # linear operations\n        self.key = torch.nn.Linear(N_EMBD, N_EMBD, bias=False)\n        self.value = torch.nn.Linear(N_EMBD, N_EMBD, bias=False)\n        self.receptance = torch.nn.Linear(N_EMBD, N_EMBD, bias=False)\n        self.output = torch.nn.Linear(N_EMBD, N_EMBD, bias=False)\n\n        # mixers\n        self.key_mixer, self.value_mixer = Mixer(), Mixer()\n        self.receptance_mixer = Mixer()\n\n        # memory\n        self.memory: torch.nn.Module = WKVMemory()\n</code></pre>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#the-secret-sauce-wkvmemory","title":"The secret sauce: <code>WKVMemory</code>","text":"<p>But it's a big piece!</p> <p>The memory is the heart of the architecture, and it's both very different from any component of a Transformer and fairly complicated.</p> <p>But, as a wise model once said, \"let's work this out in a step-by-step way to be sure we have the right answer\".</p> <p>We'll start with the components, which we define in the <code>init</code>.</p> <p>As with the mixers, we have some parameters that operate on channels and we have some persistent state to track.</p> <pre><code>class WKVMemory(torch.nn.Module):\n    \"\"\"A memory module whose contents exponentially decay over time, at a different rate per channel.\"\"\"\n    def __init__(self):\n        super().__init__()\n\n        # learned memory parameters -- one value for each dimension in the embeddings\n        self.log_gain: ChannelParameter = torch.nn.Parameter(torch.zeros(N_EMBD))\n        self.log_decay: ChannelParameter = torch.nn.Parameter(torch.zeros(N_EMBD))\n\n        # state buffers to track information across a sequence\n        contents, normalizer = torch.zeros(N_EMBD), torch.zeros(N_EMBD)\n        self.register_buffer(\"contents\", contents, persistent=False)\n        self.register_buffer(\"normalizer\", normalizer, persistent=False)\n</code></pre> <p>The names for these parameters, <code>gain</code> and <code>decay</code>, come from signal processing.</p> <p>A <code>gain</code> is used to attenuate or amplify a signal. We'll use it only on the current embedding, so our memory can treat it specially, relative to the stored information.</p> <p>A <code>decay</code> parameter determines the rate at which a signal attenuates over time. We'll use it on the information stored in the memory so that it goes away over time -- fading towards <code>0</code>.</p> <p>The memory has two pieces of state to track:</p> <ul> <li> <p>the <code>contents</code> track the information observed so far, accumulating     over time</p> </li> <li> <p>they're unnormalized, so we also track a <code>normalizer</code> for those     <code>contents</code>.</p> </li> </ul> <p>The final \"state of\" or \"information in\" the memory is their ratio, <code>contents / normalizer</code>.</p> <p>As part of a forwards pass, we update both, so our \"memory\" is some kind of average across time of what we've seen so far.</p> <p>Here's what that looks like:</p> <pre><code>from typing import Tuple\n\nScalingWeight = Float[torch.Tensor, f\"positiveEntries={N_EMBD}\"]  # positive number, one per channel\n\n@beartype\ndef update(self, importances: ScalingWeight, values: Embedding) -&gt; Tuple[Update, Update]:\n    \"\"\"Updates the memory by incrementing time and mixing in the weighted input values.\"\"\"\n    # decay the information currently in memory by one step\n    self.step()\n\n    # compute new information to add to the memory\n    contents_update: Update = importances * values  # scale each value by the matching importance weight\n    normalizer_update: Update = importances  # keep track of the weights so we can normalize across steps\n\n    # and then add the new information to the memory\n    self.contents += contents_update\n    self.normalizer += normalizer_update # -- including updating the normalizer!\n\n    # and return it\n    return contents_update, normalizer_update\n\n\nWKVMemory.update = update\n</code></pre> <p>Without the decay step, the ratio of <code>contents</code> and <code>normalizer</code> would be just a weighted average of past values.</p> <p>That is, for each channel, we're accumulating (<code>+=</code>) the weighted <code>values</code> into the <code>content</code> and the <code>weights</code> into <code>normalizer</code>, and <code>contents/normalizer</code> is their ratio: the weighted average.</p> <p>But once we include the decay <code>step</code>, each channel in the memory becomes an exponential moving weighted average:</p> <pre><code>from torch import exp\n\n\ndef step(self):\n    \"\"\"Pushes the information currently in the memory towards zero.\"\"\"\n    decay_rate: ScalingWeight = exp(self.log_decay)  # exp ensures that decay rate is positive\n    self.contents *= exp(-decay_rate)  # decay_rate &gt; 0, so exp(-decay_rate) &lt; 1\n    self.normalizer *= exp(-decay_rate)  # so each .step shrinks the contents and normalizer towards 0\n\n\nWKVMemory.step = step\n</code></pre> <p>That is, we repeatedly multiply the <code>contents</code> (and their <code>normalizer</code>!) with a number between 0 and 1, determined by our <code>decay_rate</code>.</p> <p>If a channel had an infinitely large <code>decay_rate</code>, its state would just be the most recent <code>value</code> in that channel.</p> <p>Channels with very large decay rates are common early in the network.</p> <p>If it had a <code>decay_rate</code> of 0, the channel would go back to being a weighted average.</p> <p>That allows for longer-term integration of information, and channels with very low decay rates are common later in the network.</p> <p>Now let's look at the full <code>forward</code> pass for the memory.</p> <p>It's almost as simple as</p> <ul> <li>update the memory</li> <li>return the memory's state, aka <code>contents / normalizer</code></li> </ul> <p>but there's one small complication -- the <code>gain</code>, which gets applied to just the most recent value.</p> <p>The <code>gain</code> ensures that the most recent value is treated differently than all past values.</p> <p>Here's what that looks like:</p> <pre><code>@beartype\ndef forward(self, values: Embedding, importances: ScalingWeight) -&gt; Update:\n    \"\"\"Applies the RWKV \"time-mixing block\" forward pass, in the \"RNN Cell\" style.\n\n    For details, see https://arxiv.org/abs/2305.13048, Appendix B, Eqn. 19-22 and Fig. 7.\"\"\"\n    # first, we update the memory and return what we just added\n    latest_contents, latest_normalizer = self.update(importances, values)\n\n    # then, we adjust the representation of the latest information\n    latest_contents, latest_normalizer = self.apply_gain(latest_contents, latest_normalizer)\n\n    # before adding it in and dividing, to get the final thing we report as output\n    out: Update = (self.contents + latest_contents) /           \\\n                  (self.normalizer + latest_normalizer)\n\n    return  out\n\n\nWKVMemory.forward = forward\n</code></pre> <p>By the way, this is where we hit the numerical instability that requires us to use <code>float64</code> in this implementation. We are taking exponents (dangerous) and dividing them (doubly dangerous).</p> <p>The official implementation uses several tricks to remove this instability and allow the use of lower precision floats, but they add a lot of complexity to code that's already pretty tough to follow.</p> <p>To finish out our implementation of <code>WKVMemory</code>, let's add the <code>gain</code>:</p> <pre><code>def apply_gain(self, latest_contents, latest_normalizer):\n    \"\"\"Applies the channelwise gain to the latest contents and normalizer.\"\"\"\n    gain = exp(self.log_gain) - 1  # -1 &lt; gain &lt; inf\n\n    boosted_contents = gain * latest_contents\n    boosted_normalizer = gain * latest_normalizer\n\n    return boosted_contents, boosted_normalizer\n\n\nWKVMemory.apply_gain = apply_gain\n</code></pre> <p>When the gain parameter for a channel is at its lowest value, <code>-1</code>, applying it removes the update we added. That channel is always \"one step behind\" and its output only reflects the past -- useful for spreading information across tokens.</p> <p>This way of writing it is another source of numerical instability in this implementation: we add and then subtract, which is unfortunately not quite the same as doing nothing when floats are involved.</p> <p>When the gain for a channel is very large, the output of the memory is always the same as the input value in that channel -- much like having a very large <code>decay_rate</code>.</p> <p>When the gain for the channel is close to <code>0</code>, the current value is treated the same as past values.</p> <p>The graph representation isn't particularly helpful for the <code>WKVMemory</code>, because this <code>Module</code> doesn't have any sub-modules.</p> <p>But if you look closely, you can see the memory updates. They're the <code>add_</code> operations -- <code>_</code> means \"in-place\" in PyTorch.</p> <pre><code>for depth in range(2):\n    display_graph(make_graph(WKVMemory(), depth=depth, input_data=(torch.zeros(N_EMBD), torch.zeros(N_EMBD))))\n</code></pre> <p></p> <p></p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#the-rest-of-the-attentionblock","title":"The rest of the <code>AttentionBlock</code>","text":"<p>Let's see how the memory gets incorporated into the <code>AttentionBlock</code>.</p> <p>In short, we</p> <ul> <li>calculate the <code>keys</code> and <code>values</code>, after running the <code>Mixer</code>s,</li> <li>use the <code>exp</code>onentiated <code>keys</code> as weights to store the <code>values</code> in     the <code>memory</code>,</li> <li>calculate <code>gating</code> for our memory's output based on <code>receptance</code>s,     and finally</li> <li>use one more <code>Linear</code> layer to calculate our final <code>Update</code>.</li> </ul> <p>Which looks like this:</p> <pre><code>@beartype\ndef forward(self, x: Embedding) -&gt; Update:\n    # as with the MLP, do mixers before anything else\n    mixed_keys = self.key_mixer(x)\n    keys: Embedding = self.key(mixed_keys)\n\n    mixed_values = self.value_mixer(x)\n    values: Embedding = self.value(mixed_values)\n\n    # wkv: apply \"w\"eighted decay to merge\n    #      current info (\"k\"eys and \"v\"alues) with past\n    wkv: Embedding = self.memory(values, exp(keys))\n\n    # decide how \"r\"eceptive each channel is to inputs\n    mixed_receptances = self.receptance_mixer(x)\n    receptances: Embedding = self.receptance(mixed_receptances)\n    gating_values = sigmoid(receptances)\n\n    # rwkv: use the \"r\"eceptances to gate the output of the \"wkv\" memory\n    rwkv: Embedding = gating_values * wkv\n\n    # and then do one final linear transform before returning it\n    dx: Update = self.output(rwkv)\n\n    return dx\n\nAttentionBlock.forward = forward\n</code></pre> <p>The graph view, below, is a helpful summary of the flow in this block.</p> <p>The three <code>Mixer</code>s-with-<code>Linear</code>-transformations appear first.</p> <p>One is used via <code>sigmoid</code>-then-<code>mul</code> to gate the rest -- that'd be the <code>receptances</code>.</p> <p>The other two are used in the <code>WKVMemory</code> -- but the <code>keys</code> are first <code>exp</code>onentiated into importance weights.</p> <pre><code>display_graph(make_graph(AttentionBlock(), depth=1, input_data=torch.zeros(N_EMBD), graph_dir=\"TD\"))\n</code></pre> <p></p> <p>Notice that there are no \"queries\" -- there's nothing we compare to the <code>keys</code> to decide which <code>values</code> are important.</p> <p>That makes this very different from Transformer attention, which looks a lot more like a lookup from a key-value store -- so much so that you can implement it in Redis, a popular key-value database.</p> <p>Instead, determining what is relevant as we proceed through the sequence, aka deciding what's worth our attention, is split into deciding what to store in memory right now and what from our memory is relevant right now.</p> <ol> <li>We decide what to store in our memory by calculating <code>values</code> and     assigning them importances via the (<code>exp</code>onentiated) <code>keys</code>.</li> <li>We decide what's relevant right now by using the <code>receptances</code> to     filter the <code>wkv</code> memory.</li> </ol> <p>Hence <code>rwkv</code>.</p> <p>This is the core of what makes RWKV's inference easier on the RAM than Transformer inference: we explicitly store information from the past, rather than looking the information up from the past every time we need it!</p> <p>It also makes the memory a bottleneck, which is one reason why you might suspect that a model like RWKV might not be as capable as a Transformer of the same size.</p> <p>That hasn't been the case up to 14B parameters, but scale can reveal hidden issues!</p> <p>That said, 14B parameters is big enough to get some pretty useful behavior out of a language model, so with RWKV, efficient-inference LMs have already secured a spot in the language modeling tech stack!</p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#putting-it-all-together","title":"Putting it all together","text":"<p>Let's see what the graph view looks like for an entire <code>RWKVBlock</code>.</p> <pre><code>display_graph(make_graph(RWKVBlock(), depth=2, input_data=torch.zeros(N_EMBD), graph_dir=\"TD\"))\n</code></pre> <p></p> <p>What could be simpler?</p> <p>But in all seriousness: these graphs can be very helpful adjuncts to the code!</p> <p>To really grok this architecture, I recommend pulling one of the graphs up in a separate window and mapping it onto the matching module's code.</p> <p>Now, let's run a few layers and see if the added ability to store information about past tokens solves the <code>Drosophila melanosporum</code> problem.</p> <pre><code>k = 2\nshort_rwkv = RWKV.from_weights(weights, [RWKVBlock() for _ in range(k)])\nshort_rwkvlm = AutoregressiveLM(embs, short_rwkv, unembs)\nshort_rwkvlm = short_rwkvlm.eval()\n\nout = short_rwkvlm.generate(sequence=\"Drosophila\", N=2, temperature=0.0)\n</code></pre> <pre><code>Drosophila melanog\n</code></pre> <p>Success! We're starting to get the rest of \"melanogaster\", the expected following token.</p> <p>But we've got one more thing to handle: we've written how to add state to the memory and the mixers, but we haven't written any way to remove information, so our model will just accumulate information forever, and we'd need to reinitialize it if we wanted to start \"fresh\" on a new sequence.</p> <p>Let's add a quick helper to clear out state:</p> <pre><code>def clear_buffers(module, verbose=False):\n    for name, buffer in module.named_buffers():\n        if verbose:\n            print(f\"clearing buffer {name}\")\n        buffer.zero_()\n\n\nAutoregressiveLM.clear_buffers = clear_buffers\nRWKV.clear_buffers = clear_buffers\nRWKVBlock.clear_buffers = clear_buffers\n</code></pre>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#running-a-real-model-rwkv-4-430m","title":"Running a \"real\" model: RWKV-4 430M","text":"<p>Okay, so we can run a toy model with a few layers and get three tokens in a row to make sense.</p> <p>That's cool, but what about the entire RWKV-4 430M model whose weights we've been using?</p> <p>Let's close our examination of RWKV inference with that!</p> <p>First we initialize it:</p> <pre><code>N_LAYER = 24\nrwkv_blocks = [RWKVBlock() for _ in range(N_LAYER)]\n\nrwkv = RWKV.from_weights(weights, rwkv_blocks)\n\nrwkv4 = AutoregressiveLM(embs, rwkv, unembs)\nrwkv4 = rwkv4.eval()\n</code></pre> <p>And then we run it:</p> <pre><code>rwkv4.clear_buffers()\n\nout = rwkv4.generate(sequence=\"Drosophila\", N=8, temperature=0.0)\n</code></pre> <pre><code>Drosophila* and *Drosophila melanogaster*\n</code></pre> <p>Interestingly, it starts adding some Markdown formatting -- scientific names are usually written Like this, which is formatted in Markdown <code>*Like this*</code>.</p> <p>Lastly, let's confirm that the model can generate reasonable text.</p> <p>More than that, let's check that it outputs the same text as the official reference!</p> <p>This is the main test I used to check that my implementation was really equivalent.</p> <pre><code>rwkv4.clear_buffers()\n\nsequence = \"\"\"\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. \"\"\" + \\\n\"Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n\nrwkv4.generate(sequence, N=20, temperature=0.0);\n</code></pre> <pre><code>In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n\nThe dragons were discovered by a team of researchers from the University of California, Berkeley, who\n</code></pre> <p>Go Bears.</p>","tags":["llms","rwkv","code","notebook"]},{"location":"blog/posts/rwkv-explainer/#acknowledgements","title":"Acknowledgements","text":"<p>Thanks to Ryan Zarcone for several long {white,chalk}boarding sessions on RWKV and to Igor Vasiljevic, Faris Hijazi, Rog\u00e9rio Chaves, and Ben Field for helpful comments on drafts.</p> <p>Also, many thanks to the RWKV team, in particular Johan Wind, whose blog post implementing RWKV in numpy was an invaluable resource and provided the initial scaffolding for the code in this post.</p>","tags":["llms","rwkv","code","notebook"]},{"location":"cloud-gpus/","title":"Cloud GPUs","text":"<p>By Sergey Karayev and Charles Frye. Updated October 30, 2023.</p> <p>Discussion of this page on Hacker News, May 21, 2023.</p> <p>Training and running neural networks often requires hardware acceleration, and the most popular hardware accelerator is the venerable graphics processing unit, or GPU.</p> <p>We have assembled cloud GPU vendor pricing all into tables, sortable and filterable to your liking!</p> <p>We have split the vendor offerings into two classes:</p> <ul> <li>GPU Cloud Servers, which are long-running (but possibly pre-emptible) machines, and</li> <li>Severless GPUs, which are machines that scale-to-zero in the absence of traffic (like an AWS Lambda or Google Cloud Function)</li> </ul> <p>We welcome your help in adding more cloud GPU providers and keeping the pricing info current.</p> <p>Please file an issue or make a pull request to this repo, editing this file to update the text on this page or one of the CSV files to update the data: <code>cloud-gpus.csv</code> for servers and <code>serverless-gpus.csv</code> for serverless options.</p>"},{"location":"cloud-gpus/#gpu-cloud-server-comparison","title":"GPU Cloud Server Comparison","text":""},{"location":"cloud-gpus/#notes","title":"Notes","text":"<p>The table below does not include all possible configurations for all providers, as providers differ in their configuration strategy.</p> <ul> <li>Most providers, including AWS, Azure, and Lambda, provide instances with pre-set configurations.</li> <li>On GCP, any suitable machine can be connected to a configuration of GPUs.</li> <li>On other providers, like Oblivus Cloud, Cudo Compute, and RunPod, users have precise control over the resources they request. Note that RunPod's Community Cloud, Oblivus, and Cudo are all \"open clouds\", meaning compute is provided by third parties.</li> <li>For providers without pre-set instance configurations, we have selected configurations that are roughly equivalent to AWS's options. Generally, these configurations are good for workloads that require heavy inter-GPU communication.</li> <li>Where possible, regions were set to be the west or central parts of the United States. GPU availability depends on the region.</li> <li>Raw data can be found in a csv on GitHub.</li> <li>Costs can be substantially reduced via preemption recovery and failover across clouds. If you don't want to roll your own, consider a tool like SkyPilot. See discussion of their launch on Hacker News, December 13, 2022.</li> </ul> *All prices are in $/hr.*"},{"location":"cloud-gpus/#serverless-gpus","title":"Serverless GPUs","text":""},{"location":"cloud-gpus/#notes_1","title":"Notes","text":"<p>We use the classic definition of \"serverless\", courtesy of the original AWS announcement on serverless computing: no server management, flexible scaling, high availability, and no idle capacity. We only include services that fit this criterion in our options below.</p> <p>Furthermore, we only include services that provide serverless GPUs, which can be used to run custom workloads, not just inference in particular models as a service.</p> <ul> <li>Direct price comparisons are trickier for serverless offerings: cold boot time and autoscaling logic can substantially impact cost-of-traffic.</li> <li>Some of the providers allow configuration of CPU and RAM resources. We have selected reasonable defaults, generally comparable to the fixed offerings of other providers.</li> <li>You can find pricing pages for the providers here: Banana, Baseten, Beam, Covalent, Modal, OVHcloud, Replicate, RunPod</li> <li>Serverless GPUs are a newer technology, so the details change quickly and you can expect bugs/growing pains. Stay frosty!</li> <li>Raw data can be found in a csv on GitHub.</li> </ul> *All prices are in $/hr.*"},{"location":"cloud-gpus/#how-do-i-choose-a-gpu","title":"How do I choose a GPU?","text":"<p>This page is intended to track and make explorable the current state of pricing and hardware for cloud GPUs.</p> <p>If you want advice on which machines and cards are best for your use case, we recommend Tim Dettmer's blog post on GPUs for deep learning.</p> <p>The whole post is a tutorial and FAQ on GPUS for DNNs, but if you just want the resulting heuristics for decision-making, see the \"GPU Recommendations\" section, which is the source of the chart below.</p> <p> </p> <p>Flowchart for quickly selecting an appropriate GPU for your needs, by Tim Dettmers</p>"},{"location":"cloud-gpus/#gpu-raw-performance-numbers-and-datasheets","title":"GPU Raw Performance Numbers and Datasheets","text":"<p>Below are the raw TFLOPs of the different GPUs available from cloud providers.</p> Model Arch FP32 Mixed-precision FP16 Source A100 Ampere 19.5 156 312 Datasheet A10G Ampere 35 35 70 Datasheet A6000 Ampere 38 ? ? Datasheet V100 Volta 14 112 28 Datasheet T4 Turing 8.1 65 ? Datasheet P4 Pascal 5.5 N/A N/A Datasheet P100 Pascal 9.3 N/A 18.7 Datasheet K80 Kepler 8.73 N/A N/A Datasheet A40 Ampere 37 150 150 Datasheet"},{"location":"cloud-gpus/#gpu-performance-benchmarks","title":"GPU Performance Benchmarks","text":"<p>Below are some basic benchmarks for GPUs on common deep learning tasks.</p> <p> </p> <p>Benchmark of different GPUs on a single ImageNet epoch, by AIME</p> <p> </p> <p>Benchmark of different GPUs on a mix of tasks, by Lambda Labs</p>"},{"location":"conference/","title":"FSDL 2023 Conference","text":"<p>Products are built by people, and people build best when they build together.</p> <p>So we're bringing together some of the best builders of ML-powered products to share their hard-won knowledge from the trenches, make professional and social connections, and celebrate all the amazing technologies of the last year and years to come.</p> <p>We invite you to join us, virtually or in-person in San Francisco, for an all-day conference on April 22, 2023.</p>"},{"location":"conference/#we-are-excited-to-announce-the-fsdl-2023-conference","title":"\ud83d\ude80  We are excited to announce the FSDL 2023 Conference \ud83d\ude80","text":"<p>Register by April 15th!</p>"},{"location":"conference/#confirmed-speakers","title":"Confirmed Speakers","text":"Charles Frye teaches people on the internet. He worked in education and growth at Weights &amp; Biases after getting a PhD in Neuroscience at UC Berkeley. He now works as a consultant, including for Gantry.           Sergey Karayev is Co-founder of Volition. He co-founded Gradescope after getting a PhD in Computer Vision at UC Berkeley.         Josh Tobin is Co-founder and CEO of Gantry. He worked as a Research Scientist at OpenAI and received a PhD in AI at UC Berkeley."},{"location":"conference/#where","title":"Where","text":"<p>The in-person event will be at the South San Francisco Conference Center.</p> <p>Links and information required to join the event virtually will be delivered via e-mail.</p>"},{"location":"conference/#when","title":"When","text":"<p>The event will run all day on Saturday, April 22, 2023.</p>"},{"location":"conference/#register","title":"Register","text":"Online        $0.00      <ul> <li> \ud83d\udcfa \u00a0 Watch the talks live online       <li> \ud83d\udcf9 \u00a0 Review recordings afterwards      RSVP In-Person $350 <ul> <li> \ud83d\udde3 \u00a0 Watch the talks live in real life       <li> \ud83e\udd1d \u00a0 Network with builders and vendors in ML       <li> \ud83d\udc55 \u00a0 Get some conference swag       <li> \ud83e\udd5e \u00a0 Eat food       <li> \ud83d\udd1c \u00a0 Register by April 15th!      Register here"},{"location":"conference/#sponsors","title":"Sponsors","text":"<p>We're currently looking for organizations that are interested in sponsoring this event!</p> <p>See this page for details and contact <code>sponsorships@fullstackdeeplearning.com</code> with inquiries.</p>"},{"location":"conference/sponsors/","title":"Info for Sponsors","text":"<p>We offer three tiers of sponsorship for the FSDL 2023 Conference:</p> <ol> <li> <p>Vector Tier sponsors receive the following benefits: logo displayed on website and during virtual conference, verbal acknowledgement , and one registration. Vector Tier sponsorships are available for $500.</p> </li> <li> <p>Matrix Tier sponsors receive all the benefits of the Vector Tier, plus: logo displayed in between talks, 8' by 10' area for booth setup, and an additional registration (for a total of two). Matrix Tier sponsorships are available for $1500.</p> </li> <li> <p>Tensor Tier sponsors receive all the benefits of the Matrix Tier, plus: logo displayed on a banner in the registration area, access to an opt-in database of attendees who are interested in tooling or recruitment, and two additional registrations (for a toral of four). Tensor Tier sponsorships are available for $2500.</p> </li> </ol> <p>Contact <code>sponsorships@fullstackdeeplearning.com</code> if you're interested in sponsoring the conference!</p>"},{"location":"course/","title":"Full Stack Deep Learning Courses","text":"<p>The Full Stack Deep Learning course started in 2018, as a three-day bootcamp hosted on Berkeley campus. Since then, we've hosted several in-person bootcamps, online courses, and official university courses.</p> <p>Looking for the most recent FSDL materials?</p> <p>You can find them here.</p>"},{"location":"course/#testimonials","title":"Testimonials","text":""},{"location":"course/#past-iterations","title":"Past Iterations","text":"<ul> <li>FSDL 2022 (Online): A fully online course, taught via YouTube, Crowdcast, and Discord.</li> <li>FSDL 2021 (Online): Contemporaneous with the Berkeley course, we taught an online cohort course.</li> <li>FSDL 2021 (Berkeley): Taught as a UC Berkeley undergrad course CS194-080 in Spring 2021</li> <li>FSDL 2020 (UW): Taught as University of Washington Professional Master's Program course CSEP 590C in Spring 2020</li> <li>FSDL 2019 (Online): Materials from the November 2019 bootcamp held on Berkeley campus organized in a nice online format.</li> <li>FSDL 2019 (Bootcamp): Raw materials from the March 2019 bootcamp, held on Berkeley campus.</li> <li>FSDL 2018 (Bootcamp): Our first bootcamp, held on Berkeley campus in August 2018</li> </ul>"},{"location":"course/2022/","title":"Full Stack Deep Learning - 2022 Course","text":"<p>Course Completed</p> <p>All the lecture and lab material is free forever. Just check out the links below.</p> <p>To be among the first to hear about future iterations of the course, simply enter your email below, follow us on  Twitter, or subscribe to our  YouTube channel.</p> <p> </p>"},{"location":"course/2022/#schedule","title":"Schedule","text":"<p>We released lecture videos on Mondays at 6pm Pacific and lab videos on Wednesdays at 6pm Pacific on  YouTube.</p> Week Lecture Lab Project 2022.07.25 Pre-Labs 1-3: CNNs, Transformers, and PyTorch+Lightning - 2022.08.08 Lecture 1: Course Vision and When to Use ML Lab Overview - 2022.08.15 Lecture 2: Development Infrastructure &amp; Tooling Lab 4: Experiment Management - 2022.08.22 Lecture 3: Troubleshooting &amp; Testing Lab 5: Troubleshooting &amp; Testing - 2022.08.29 Lecture 4: Data Management Lab 6: Data Annotation Start forming groups 2022.09.05 Lecture 5: Deployment Lab 7: Web Deployment Group proposals due 2022.09.12 Lecture 6: Continual Learning Lab 8: Model Monitoring Work on project 2022.09.19 Lecture 7: Foundation Models Work on project 2022.09.26 Lecture 8: ML Teams and Project Management Work on project 2022.10.03 Lecture 9: Ethics Work on project 2022.10.10 Project Presentations Project due"},{"location":"course/2022/#detailed-contents","title":"Detailed Contents","text":""},{"location":"course/2022/#pre-labs-1-3-cnns-transformers-pytorch-lightning","title":"Pre-Labs 1-3: CNNs, Transformers, PyTorch Lightning","text":"<p>We review some prerequisites -- the DNN architectures we'll be using and basic model training with PyTorch -- and introduce PyTorch Lightning. Published August 10, 2022.</p>"},{"location":"course/2022/#lecture-1-course-vision-and-when-to-use-ml","title":"Lecture 1: Course Vision and When to Use ML","text":"<p>We review the purpose of the course and consider when it's a good (or bad!) idea to use ML. Published August 8, 2022.</p>"},{"location":"course/2022/#lab-overview","title":"Lab Overview","text":"<p>We walk through the entire architecture of the application we will be building, from soup to nuts. Published July 25, 2022.</p>"},{"location":"course/2022/#lecture-2-development-infrastructure-tooling","title":"Lecture 2: Development Infrastructure &amp; Tooling","text":"<p>We tour the landscape of infrastructure and tooling for developing deep learning models. Published August 15, 2022.</p>"},{"location":"course/2022/#lab-4-experiment-management","title":"Lab 4: Experiment Management","text":"<p>We run, track, and manage model development experiments with Weights &amp; Biases. Published August 17, 2022.</p>"},{"location":"course/2022/#lecture-3-troubleshooting-testing","title":"Lecture 3: Troubleshooting &amp; Testing","text":"<p>We look at tools and practices for testing software in general and ML models in particular. Published August 22, 2022.</p>"},{"location":"course/2022/#lab-5-troubleshooting-testing","title":"Lab 5: Troubleshooting &amp; Testing","text":"<p>We try out some Python testing tools and dissect a PyTorch trace to learn performance troubleshooting techniques. Published August 24, 2022.</p>"},{"location":"course/2022/#lecture-4-data-management","title":"Lecture 4: Data Management","text":"<p>We look at sourcing, storing, exploring, processing, labeling, and versioning data for deep learning. Published August 29, 2022.</p>"},{"location":"course/2022/#lab-6-data-annotation","title":"Lab 6: Data Annotation","text":"<p>We spin up a data annotation server and learn just how messy data really is. Published August 31, 2022.</p>"},{"location":"course/2022/#lecture-5-data-management","title":"Lecture 5: Data Management","text":"<p>We do a lightning tour of all the ways models are deployed and do a deep dive on running models as web services. Published September 5, 2022.</p>"},{"location":"course/2022/#lab-7-web-deployment","title":"Lab 7: Web Deployment","text":"<p>We create and deploy our ML-powered text recognition application with a simple web UI and a serverless model service. Published September 7, 2022.</p>"},{"location":"course/2022/#lecture-6-continual-learning","title":"Lecture 6: Continual Learning","text":"<p>We consider what it takes to build a continual learning system around an ML-powered application. Published September 12, 2022.</p>"},{"location":"course/2022/#lab-8-model-monitoring","title":"Lab 8: Model Monitoring","text":"<p>We add user feedback to our ML application and review data logged by actual users of the FSDL Text Recognizer. Published September 14, 2022.</p>"},{"location":"course/2022/#lecture-7-foundation-models","title":"Lecture 7: Foundation Models","text":"<p>We look at how to build on GPT-3, CLIP, StableDiffusion, and other large models. Published September 19, 2022.</p>"},{"location":"course/2022/#lecture-8-ml-teams-and-project-management","title":"Lecture 8: ML Teams and Project Management","text":"<p>We look at the structure of ML teams and projects, including how to hire or get hired on an ML team and how to build an ML-first organization. Published September 26, 2022.</p>"},{"location":"course/2022/#lecture-9-ethics","title":"Lecture 9: Ethics","text":"<p>We consider ethical concerns around buiding technlogy, building with machine learning, and building artificial intelligence. Published October 3, 2022.</p>"},{"location":"course/2022/#teaching-assistants","title":"Teaching Assistants","text":"<p>This course was only possible with the support of our amazing TAs (in alphabetical order):</p> <ul> <li>Andrew Mendez is a Deep Learning Solutions Engineer at DeterminedAI, working on computer vision and NLP solutions for defense and autonomous vehicle companies. Previously Andrew worked as an ML Engineer at Clarifai and CACI.</li> <li>Daniel Hen is a Senior Data Scientist at Digital Turbine, working on Ad Tech and mobile solutions, as well as Big Data problems. Working with Spark, ML algorithms such as XGBoost, Computer Vision, and constantly learning new technology.</li> <li>James Le runs Data Relations and Partnerships at Superb AI, a data management platform for computer vision use cases. Outside work, he writes data-centric blog posts, hosts a data-focused podcast, and organizes in-person events for the data community.</li> <li>Saurabh Bipin Chandra is a Senior ML Scientist at Turnitin.</li> <li>Sayak Paul is as a Machine Learning Engineer at Carted on NLP and representation learning from HTML webpages. Besides work, he contributes to various open-source projects.</li> <li>Vishnu Rachakonda is a Data Scientist at firsthand.</li> </ul>"},{"location":"course/2022/announcement/","title":"Full Stack Deep Learning 2022 Course Announcement","text":"<p>Info</p> <p>Looking for the latest edition of the course? Click here.</p> <p>Want to be among the first to hear about future iterations of the course? Simply enter your email below, follow us on  Twitter, or subscribe to our  YouTube channel.</p> <p> Email Address  </p>"},{"location":"course/2022/announcement/#what-you-will-learn","title":"What you will learn","text":"<p>Our course incorporates feedback from thousands of learners over 5 years of teaching production machine learning.</p> <p>We've updated all materials with best practices as of 2022 and added some brand new topics:</p> <ul> <li>Formulating the problem and estimating project cost</li> <li>Sourcing, cleaning, processing, labeling, synthesizing, and augmenting data</li> <li>Picking the right framework and compute infrastructure</li> <li>Troubleshooting training and ensuring reproducibility</li> <li>Deploying the model at scale</li> <li>\u2728 Monitoring and continually improving the deployed model \u2728</li> <li>\u2728 How ML teams work and how to manage ML projects \u2728</li> <li>\u2728 Building on Large Language Models and other Foundation Models \u2728</li> </ul> <p> </p>"},{"location":"course/2022/announcement/#labs","title":"Labs","text":"<p>Through the first few weeks of the course, you will construct an end-to-end ML system, with best practices as of 2022.</p> <ul> <li>Applying CI/CD principles for clean, fast-moving ML</li> <li>Hypermodern training: PyTorch + Lightning + W&amp;B</li> <li>Hypermodern deployment: Docker + AWS Lambda + Gradio</li> </ul>"},{"location":"course/2022/announcement/#project","title":"Project","text":"<p>Once the labs are done, you'll begin working on your own portfolio project. You'll create a working a working ML-powered application of your choice and share it with your fellow learners, getting feedback from course staff.</p> <p>Selected projects will get the opportunity to share their work with the broader FSDL community.</p>"},{"location":"course/2022/announcement/#who-is-this-for","title":"Who is this for","text":"<p>ML Researchers and Engineers, MS students, software engineers looking to get into ML, data scientists looking to up their software engineering game, and PMs on ML teams will all benefit from materials in our course.</p> <p>You will get the most out of this course if you have:</p> <ul> <li>At least one year of experience programming in Python.</li> <li>At least one deep learning course (at a university or online).</li> <li>Experience with code versioning, Unix environments, and software engineering.</li> <li>At least four hours a week to commit to learning, split across lectures, Q&amp;A, labs, reading, and project work.</li> </ul> <p>We review the fundamentals of deep learning (backprop, MLPs, CNNs, Transformers) in supplementary lectures released before the start of the course \u2014 but you should not expect to learn this material for the first time from these.</p>"},{"location":"course/2022/announcement/#testimonials","title":"Testimonials","text":""},{"location":"course/2022/announcement/#instructors","title":"Instructors","text":"<p>Charles Frye teaches people on the internet. He worked in education and growth at Weights &amp; Biases after getting a PhD in Neuroscience at UC Berkeley. He now works as a consultant, including for Gantry.   </p> <p> </p> <p>Sergey Karayev is Co-founder of Volition. He co-founded Gradescope after getting a PhD in Computer Vision at UC Berkeley.   </p> <p> </p> <p>Josh Tobin is Co-founder and CEO of Gantry. He worked as a Research Scientist at OpenAI and received a PhD in AI at UC Berkeley.   </p>"},{"location":"course/2022/announcement/#registration","title":"Registration","text":"On-Your-Own        $0.00      50% discount for students <ul> <li> \ud83d\udcf9 \u00a0 Lecture and lab videos      Sign Up for Mailing List Cohort-Based $495 50% discount for students and academics <ul> <li> \ud83d\udcf9 \u00a0 Lecture and lab videos       <li> \ud83d\udde3 \u00a0 Private Discord with instructors       <li> \u2753 \u00a0 Weekly live online Q&amp;A       <li> \ud83d\udcb2 \u00a0 Compute credits       <li> \ud83c\udfa8 \u00a0 Team-based portfolio project with feedback       <li> \ud83c\udfc6 \u00a0 Certificate of completion       <li> \ud83d\udd1c \u00a0 Starts August 8th, 2022      Registration Closed <p>Bummed that you missed registration for the 2022 course?</p> <p>To make sure you hear about the next iteration, enter your email below to sign up for our mailing list.</p> <p> </p>"},{"location":"course/2022/announcement/#cohort-based-course-faqs","title":"Cohort-Based Course FAQs","text":"<p>Read more about the 2022 synchronous cohort here.</p>"},{"location":"course/2022/announcement/#how-long-is-the-course-how-much-time-do-i-need-to-devote-to-it","title":"How long is the course? How much time do I need to devote to it?","text":"<p>The course is 10 weeks long, from August 8, 2022 to October 14, 2022. During that time, you should expect to spend about four hours a week on the course. In the last four weeks, during which you'll be working on the project, devoting more time will result in a final project that you can be even more proud to put in your portfolio.</p>"},{"location":"course/2022/announcement/#which-components-of-the-course-are-synchronous-what-days-and-times-will-synchronous-events-occur","title":"Which components of the course are synchronous? What days and times will synchronous events occur?","text":"<p>The Q&amp;A sessions are synchronous. Their timing will be set based on student availability.</p> <p>Lectures are pre-recorded, with the Q&amp;A session serving as a live \"flipped classroom\" for discussing the lecture content.</p> <p>Labs are available online, with a pre-recorded walkthrough video. We encourage students to form study groups, via the Discord server, for working on the lab material.</p>"},{"location":"course/2022/announcement/#how-do-the-projects-work","title":"How do the projects work?","text":"<p>Teams of five will work for four weeks to build an ML-powered application of their choosing: a chatbot, an image editor, a natural language speech interface, or anything else you can imagine building with ML.</p> <p>At the end, they will present their work to the rest of the class.</p> <p>Project proposals and final projects will be reviewed by course staff. We'll provide detailed feedback, but grading will be done based on completion. The best projects will be featured by FSDL on YouTube, Twitter, and at our summits.</p> <p>Questions about registration or the class that aren't answered here?</p> <p>Tweet at us @full_stack_dl or email <code>admin@fullstackdeeplearning.com</code>.</p>"},{"location":"course/2022/cohort/","title":"Synchronous Cohort Option","text":"<p>We organized the synchronous cohort for the 2022 course via Discord.</p>"},{"location":"course/2022/cohort/#its-project-time-how-do-i-access-my-free-gpus","title":"It's project time! How do I access my free GPUs?","text":"<p>Check out the instructions here.</p>"},{"location":"course/2022/cohort/#how-do-i-know-if-im-fully-registered","title":"How do I know if I'm fully registered?","text":"<p>If you completed your registration for the course, you should be able to access the <code>#course-announcements</code> channel here.</p> <p>Please email us at <code>registration@fullstackdeeplearning.com</code> if you registered for the course but cannot access the Discord.</p>"},{"location":"course/2022/cohort/#ive-registered-and-im-in-the-discord-how-do-i-follow-along","title":"I've registered and I'm in the Discord. How do I follow along?","text":"<p>For a quick list of the most important links for cohort members, see this Discord post. You can add the private course calendar linked there to your own so you never miss an event.</p> <p>For a tour of the Discord, plus some tips and tricks, check out the video below:</p>"},{"location":"course/2022/cohort/#how-do-i-control-my-notifications-in-discord","title":"How do I control my notifications in Discord?","text":"<p>To stay apprised of updates without getting overwhelmed, we suggest these notification settings: </p> <p>You can find them here in the Discord web client: </p>"},{"location":"course/2022/cohort/#only-course-staff-can-use-the-remaining-notification-stream-mentions","title":"Only course staff can use the remaining notification stream, @mentions.","text":"<p>If you turn off <code>@role</code>/<code>@everyone</code>/<code>@here</code>, you won't see announcements about live events and changes of schedule.</p> <p>If you want to further reduce distractions, turn off the notification dot.</p> <p>You can also mute individual channels, leaving only <code>#course-announcements</code> off mute.</p>"},{"location":"course/2022/cohort/#only-instructors-can-post-in-the-course-announcements-channel","title":"Only instructors can post in the <code>#course-announcements</code> channel.","text":"<p>If you furthermore hide muted channels, the Discord interface will only show that channel, but you'll miss out on the chance to discover discussions in other channels.</p>"},{"location":"course/2022/lab-0-overview/","title":"Lab Overview","text":"<p>By Charles Frye. Published July 25, 2022.</p>"},{"location":"course/2022/lab-0-overview/#what-are-these-labs-for","title":"What are these labs for?","text":"<p>In the lab portion of Full Stack Deep Learning 2022, we will incrementally develop a complete codebase to train a deep neural network to recognize characters in hand-written paragraphs and deploy it inside a simple web application.</p> <p>These labs act as an opportunity to work through the nitty-gritty details that come up when implementing some of the recommendations given in the lectures in a concrete system. It's also a chance for you to gain familiarity with some of the tools we recommend in the lectures.</p> <p>This lab reviews the overall architecture of the system.</p>"},{"location":"course/2022/lab-0-overview/#architecture-of-the-text-recognizer","title":"Architecture of the Text Recognizer","text":"<p>Software architectures are inherently about trade-offs: decisions that make for better scaling might make for worse security or tools that encourage faster iteration might reduce transparency.</p> <p>We design our architecture with agility and simplicity as the prime directives. We choose simplicity in order to empower individuals to understand the \"full stack\" of the application, from GPUs crunching tensors in model development up to serverless cloud functions acting on requests in production. And we choose agility so that individual is able to quickly iterate on the application, especially in response to user feedback.</p>"},{"location":"course/2022/lab-0-overview/#architecture-diagram","title":"Architecture Diagram","text":"<p>We put together a handy architecture diagram summarizing the application here:</p> <p>For a guided tour of this architecture, watch the video at the top of the page or click the badge below to open an interactive Jupyter notebook on Google Colab:</p>"},{"location":"course/2022/lab-0-overview/#running-the-labs","title":"Running the labs","text":""},{"location":"course/2022/lab-0-overview/#one-click-setup-on-colab","title":"One-click setup on Colab","text":"<p>To make it as easy as possible to run the labs, we've made them compatible with Google Colab.</p> <p>Wherever you see an \"Open in Colab\" badge, like the one below, just click on it and you'll be dropped into a hosted notebook environment for the lab, complete with free GPU. The badge below opens the first main-track lab, Lab 4 on experiment management.</p> <p>You can read more here.</p>"},{"location":"course/2022/lab-0-overview/#setup-on-your-own-linux-machine","title":"Setup on your own Linux machine","text":"<p>If you have a Linux machine with an NVIDIA GPU and drivers, either locally or in the cloud, you can also run the labs there. The video above and text instructions here should be enough to get you going.</p> <p>Don't get stuck on setup!</p> <p>Remember that Google Colab is always there as an option if you run into issues while setting up.</p> <p>Rather than getting frustrated with some obnoxious library linking or driver issue that's irrelevant to the material you are really trying to learn and getting stuck in an installation quagmire, just run the labs on Colab so you can get back to learning about machine learning!</p>"},{"location":"course/2022/lab-4-experiment-management/","title":"Lab 4: Experiment Management","text":"<p>By Charles Frye. Published August 17, 2022.</p> <p>In this lab, we'll work through an entire experiment management workflow for model development, using a tool called Weights &amp; Biases.</p>"},{"location":"course/2022/lab-4-experiment-management/#outline","title":"Outline","text":"<ul> <li>00:00 Why do we need experiment management?</li> <li>02:24 Tracking experiments with TensorBoard</li> <li>04:16 Experiment management with Weights &amp; Biases</li> <li>06:48 A guided tour of the W&amp;B run interface</li> <li>12:12 Exploratory data analysis with W&amp;B Tables</li> <li>14:00 Project management with W&amp;B</li> <li>16:27 Artifact versioning with W&amp;B</li> <li>18:52 Programmatic API access to W&amp;B</li> <li>20:14 Collaboration tools in W&amp;B</li> <li>25:00 Hyperparameter sweeps in W&amp;B</li> <li>28:15 Overview of exercises</li> </ul> <p>Wait, what happened to labs 1 through 3?</p> <p>The first three labs review some pre-requisites for the course -- DNN architectures and the basics of model training.</p> <p>You can find them here.</p> <p>If you're already basically familiar with training neural networks in any framework, you really only need to review Lab 02a, on using PyTorch Lightning.</p>"},{"location":"course/2022/lab-5-troubleshooting-and-testing/","title":"Lab 5: Troubleshooting &amp; Testing","text":"<p>By Charles Frye. Published August 24, 2022.</p> <p>In this lab, we'll check out the basic tools required to write clean Python code and see how to write memorization tests for training code in PyTorch Lightning. Then we'll take a deep dive into the trace of a PyTorch training step and use it to debug performance issues in GPU-accelerated code.</p>"},{"location":"course/2022/lab-5-troubleshooting-and-testing/#outline","title":"Outline","text":"<ul> <li>00:00 Overview</li> <li>00:51 Linting: pre-commit, black, flake8</li> <li>05:42 Testing: pytest, doctest, memorization testing</li> <li>11:15 Troubleshooting PyTorch performance</li> <li>16:13 A guided tour of a PyTorch trace</li> </ul>"},{"location":"course/2022/lab-6-data-annotation/","title":"Lab 6: Data Annotation","text":"<p>By Charles Frye. Published August 31, 2022.</p> <p>In this lab, we'll see how raw data becomes useful data via data annotation and how structured data stored on disk becomes neural network-ready with preprocessing and PyTorch <code>Dataset</code>s.</p> <p>We'll also spin up a data annotation server using Label Studio.</p>"},{"location":"course/2022/lab-6-data-annotation/#outline","title":"Outline","text":"<ul> <li>00:00 Overview</li> <li>00:36 Loading annotated data and synthesizing data</li> <li>02:39 Setting up a data annotation server with Label Studio</li> <li>06:54 Uploading data to Label Studio</li> <li>09:15 Building and using an annotation interface in Label Studio</li> <li>13:17 Exercises</li> </ul>"},{"location":"course/2022/lab-7-web-deployment/","title":"Lab 7: Web Deployment","text":"<p>By Charles Frye. Published September 7, 2022.</p> <p>In this lab, we'll take the leap from ML model to ML-powered application by packaging our text recognizer into a portable TorchSript binary, wrapping that binary up into a serverless cloud function, and building a simple UI in Python with gradio.</p>"},{"location":"course/2022/lab-7-web-deployment/#outline","title":"Outline","text":"<ul> <li>00:00 Overview</li> <li>01:06 Compiling the model to TorchScript</li> <li>06:00 Why not deploy on GPUs?</li> <li>08:58 Building a GUI with gradio</li> <li>15:34 Spinning up a model service</li> <li>21:11 Creating a public URL with ngrok</li> <li>24:52 Writing a Dockerfile for our server</li> <li>30:06 Recap</li> </ul>"},{"location":"course/2022/lab-8-model-monitoring/","title":"Lab 8: Model Monitoring","text":"<p>By Charles Frye. Published September 14, 2022.</p> <p>In this lab, we'll add flagging to our ML-powered application so that users can give us feedback.</p> <p>Then, we'll explore some data logged based on feedback from actual users of the FSDL Text Recognizer to the model monitoring and continual learning platform Gantry.</p>"},{"location":"course/2022/lab-8-model-monitoring/#outline","title":"Outline","text":"<ul> <li>00:00 Basic user feedback with gradio</li> <li>04:51 Logging feedback to Gantry</li> <li>08:34 Checking for model toxicity with Gantry projections</li> <li>14:23 Detecting model bugs in the Gantry UI with distributions and filters</li> <li>19:01 Discovering surprising user data in the Gantry UI</li> <li>29:53 Outro</li> </ul>"},{"location":"course/2022/labs-1-3-cnns-transformers-pytorch-lightning/","title":"Pre-Labs 1-3: CNNs, Transformers, PyTorch Lightning","text":"<p>By Charles Frye. Published August 10, 2022.</p> <p>This first set of \"review\" labs covers deep learning fundamentals and introduces two of the core libraries we will use for model training: PyTorch and PyTorch Lightning.</p> <p>These labs are optional -- it's possible to get most of the value out of the main set of labs without detailed knowledge of the material here.</p> <p>But if you find yourself getting tripped up on PyTorch or Lightning details or on the model architecture, come back and review these labs.</p> <p>The videos above give a quick high-level overview of each lab.</p> <p>If you need a refresher or a deeper dive on any of the topics or libraries, then work through the notebooks as well. We recommend only completing the exercises for the labs that are of most interest to you.</p>"},{"location":"course/2022/labs-1-3-cnns-transformers-pytorch-lightning/#click-the-badges-below-to-access-individual-lab-notebooks-on-colab-and-videos-on-youtube","title":"Click the badges below to access individual lab notebooks on Colab and videos on YouTube","text":"Lab Colab Video Lab 01: Deep Neural Networks in PyTorch Lab 02a: PyTorch Lightning Lab 02b: Training a CNN on Synthetic Handwriting Data Lab 03: Transformers and Paragraphs"},{"location":"course/2022/labs-1-3-cnns-transformers-pytorch-lightning/#running-the-labs","title":"Running the labs","text":""},{"location":"course/2022/labs-1-3-cnns-transformers-pytorch-lightning/#one-click-setup-on-colab","title":"One-click setup on Colab","text":"<p>To make it as easy as possible to run the labs, we've made them compatible with Google Colab.</p> <p>Wherever you see an \"Open in Colab\" badge, like the one below, just click on it and you'll be dropped into a hosted notebook environment for the lab, complete with free GPU. The badge below opens Lab 01, on PyTorch.</p> <p>You can read more here.</p>"},{"location":"course/2022/labs-1-3-cnns-transformers-pytorch-lightning/#setup-on-your-own-linux-machine","title":"Setup on your own Linux machine","text":"<p>If you have a Linux machine with an NVIDIA GPU and drivers, either locally or in the cloud, you can also run the labs there. The video above and text instructions here should be enough to get you going.</p> <p>Don't get stuck on setup!</p> <p>Remember that Google Colab is always there as an option if you run into issues while setting up.</p> <p>Rather than getting frustrated with some obnoxious library linking or driver issue that's irrelevant to the material you are really trying to learn and getting stuck in an installation quagmire, just run the labs on Colab so you can get back to learning about machine learning!</p>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/","title":"Lecture 1: Course Vision and When to Use ML","text":"<p>Lecture by Josh Tobin. Notes by James Le and Vishnu Rachakonda. Published August 8, 2022. Download slides.</p>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#1-course-vision","title":"1 - Course Vision","text":""},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#history-of-fsdl","title":"History of FSDL","text":"<p>Full Stack Deep Learning (FSDL) is the course and community for people who are building products that are powered by machine learning (ML). It's an exciting time to talk about ML-powered products because ML is rapidly becoming a mainstream technology - as you can see in startup funding, job postings, and continued investments of large companies.</p> <p>FSDL was originally started in 2018 when the most exciting ML-powered products were built by the biggest companies. However, the broader narrative in the field was that very few companies could get value out of this technology.</p> <p>Now in 2022, there's a proliferation of powerful products that are powered by ML. The narrative has shifted as well: There's standardization that has emerged around the tech stack - with transformers and NLP starting to seep their way into more use cases, as well as practices around how to apply ML technologies in the world. One of the biggest changes in the field in the past four years has been the emergence of the term MLOps.</p> <p>In addition to the field being more mature and research continuing to progress, a big reason for this rapid change is that the training of models is starting to become commoditized.</p> <ul> <li> <p>With tools like HuggingFace, you can deploy a state-of-the-art NLP or CV model in one or two lines of code.</p> </li> <li> <p>AutoML is starting to work for a lot of applications.</p> </li> <li> <p>Companies like OpenAI are starting to provide models as a service where you don't even have to download open-source packages to use them. You can make a network call to get predictions from a state-of-the-art model.</p> </li> <li> <p>Many libraries are starting to standardize around frameworks like Keras and PyTorch Lightning.</p> </li> </ul>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#ai-progress","title":"AI Progress","text":"<p>The history of ML is characterized by stratospheric rises and meteoric falls of the public perception of the technology. These were driven by a few different AI winters that happened over the history of the field - where the technology didn't live up to its hype. If you project forward a few years, what will happen to ML?</p> <p></p> <p>Source: 5 Things You Should Know About AI (Cambridge Consultants, May 2017)</p> <p>Here are the major categories of possible outcomes and our guess about their likelihoods:</p> <ol> <li> <p>A true AI winter, where people become skeptical about AI as a technology. We think this is less likely.</p> </li> <li> <p>A slightly more likely outcome is that the overall luster of the technology starts to wear off, but specific applications are getting a ton of value out of it.</p> </li> <li> <p>The upside outcome for the field is that AI continues to accelerate rapidly and becomes pervasive and incredibly effective.</p> </li> </ol> <p>Our conjecture is that: The way we, as a field, avoid an AI winter is by translating research progress into real-world products. That's how we avoid repeating what has happened in the past.</p>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#ml-powered-products-require-a-different-process","title":"ML-Powered Products Require a Different Process","text":"<p>Building ML-powered products requires a fundamentally different process in many ways than developing ML models in an academic setting.</p> <p></p> <p>In academia, you build \"flat-earth\" ML - selecting a problem, collecting data, cleaning and labeling the data, iterating on model development until you have a model that performs well on the dataset collected, evaluating that model, and writing a report at the end.</p> <p></p> <p>But ML-powered products require an outer loop where after you deploy the model into production, you measure how that model performs when it interacts with real users. Then, you use real-world data to improve your model, setting up a data flywheel that enables continual improvement.</p>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#this-course","title":"This Course","text":"<p>This class is about the unique aspects you need to know beyond training models to build great ML-powered products. Here are some concrete goals for us:</p> <ol> <li> <p>Teaching you generalist skills and an understanding of the components of ML-powered products (and ML projects more generally).</p> </li> <li> <p>Teaching you enough MLOps to get things done.</p> </li> <li> <p>Sharing best practices and explaining the motivation behind them.</p> </li> <li> <p>Learning things that might help you with job interviews for ML engineering roles.</p> </li> <li> <p>Forming a community to learn together and from each other.</p> </li> </ol> <p>We do NOT try to:</p> <ol> <li> <p>Teach you ML or software engineering from scratch.</p> </li> <li> <p>Cover the whole breadth of deep learning techniques.</p> </li> <li> <p>Make you an expert in any single aspect of ML.</p> </li> <li> <p>Do research in deep learning.</p> </li> <li> <p>Cover the full spectrum of MLOps.</p> </li> </ol> <p>If you feel rusty on your pre-requisites but want to get started with FSDL, here are our recommendations to get up to speed with the fundamentals:</p> <ul> <li> <p>Andrew Ng's Machine Learning Coursera course</p> </li> <li> <p>Google's crash course on Machine Learning</p> </li> <li> <p>MIT's The Missing Semester on software engineering</p> </li> </ul>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#ml-powered-products-vs-mlops","title":"ML-Powered Products vs MLOps","text":"<p>MLOps, as a discipline, has emerged in just the last few years. It is about practices for deploying, maintaining, and operating ML systems that generate ML models in production. A lot of MLOps is about:</p> <ul> <li> <p>How do we put together an infrastructure that allows us to build models in a repeatable and governable way?</p> </li> <li> <p>How can we run ML systems in a potentially high-scale production setting?</p> </li> <li> <p>How can we collaborate on these systems as a team?</p> </li> </ul> <p></p> <p>ML-powered product building is a distinct but overlapping discipline. A lot of what it takes to build a great ML-powered product goes beyond the infrastructure side of ML systems. It focuses on how to fit ML into the context of the product or the application that you're building.</p> <p>Other topics in the scope of the ML product discipline include:</p> <ul> <li> <p>How do you understand how your users are interacting with your model?</p> </li> <li> <p>How do you build a team or an organization that can work together effectively on ML systems?</p> </li> <li> <p>How do you do product management in the context of ML?</p> </li> <li> <p>What are the best practices for designing products that use ML as part of them?</p> </li> </ul> <p>This class focuses on teaching you end-to-end what it takes to get a product out in the world that uses ML and will cover aspects of MLOps that are most critical in order to do that.</p>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#chapter-summary","title":"Chapter Summary","text":"<ol> <li> <p>ML-powered products are going mainstream thanks to the democratization of modeling.</p> </li> <li> <p>However, building great ML-powered products requires a different process from building models.</p> </li> <li> <p>Full-Stack Deep Learning is here to help!</p> </li> </ol>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#2-when-to-use-machine-learning","title":"2 - When To Use Machine Learning","text":""},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#when-to-use-ml-at-all","title":"When to Use ML At All","text":"<p>ML projects have a higher failure rate than software projects in general. One reason that's worth acknowledging is that for many applications, ML is fundamentally still research. Therefore, we shouldn't aim for 100% success.</p> <p>Additionally, many ML projects are doomed to fail even before they are undertaken due to a variety of reasons:</p> <ol> <li> <p>They are technically infeasible or poorly scoped.</p> </li> <li> <p>They never make the leap to a production environment.</p> </li> <li> <p>The broader organization is not all on the same page about what would be considered success criteria for them.</p> </li> <li> <p>They solve the problem that you set out to solve but do not solve a big enough problem to be worth their complexity.</p> </li> </ol> <p>The bar for your ML projects should be that their value must outweigh not just the cost of developing them but also the additional complexity that these ML systems introduce to your software (as introduced in the classic paper \"The High-Interest Credit Card of Technical Debt\").</p> <p>In brief, ML systems erode the boundaries between other systems, rely on expensive data dependencies, are commonly plagued by system design anti-patterns, and are subject to the instability of the external world.</p> <p>Before starting an ML project, ask yourself:</p> <ol> <li> <p>Are you ready to use ML? More specifically, do you have a product? Are you collecting data and storing it in a sane way? Do you have the right people?</p> </li> <li> <p>Do you really need ML to solve this problem? More specifically, do you need to solve the problem at all? Have you tried using rules or simple statistics to solve the problem?</p> </li> <li> <p>Is it ethical to use ML to solve this problem? We have a whole lecture about ethics!</p> </li> </ol>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#how-to-pick-problems-to-solve-with-ml","title":"How to Pick Problems to Solve with ML","text":"<p>Just like any other project prioritization, you want to look for use cases that have high impact and low cost:</p> <ol> <li> <p>High-impact problems are likely to be those that address friction in your product, complex parts of your pipeline, places where cheap prediction is valuable, and generally what other people in your industry are doing.</p> </li> <li> <p>Low-cost projects are those with available data, where bad predictions are not too harmful.</p> </li> </ol> <p></p>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#high-impact-projects","title":"High-Impact Projects","text":"<p>Here are some heuristics that you can use to find high-impact ML projects:</p> <ol> <li> <p>Find problems that ML takes from economically infeasible to feasible. A good resource here is the book \"Prediction Machines: The Simple Economics of AI.\" The book's central thesis is that AI reduces the cost of prediction, which is central to decision-making. Therefore, look for projects where making prediction cheaper will have a huge impact.</p> </li> <li> <p>Think about what your product needs. This article from the ML team at Spotify talks about the three principles for designing Discover Weekly, one of Spotify's most powerful and popular ML-powered features.</p> </li> <li> <p>Think about the types of problems that ML is particularly good at. One common class of problem that is overlooked is \"Software 2.0\", as coined by Andrej Kaparthy. Essentially, if you have a part of your system that is complex and manually defined, then that's potentially a good candidate to be automated with ML.</p> </li> <li> <p>Look at what other people in the industry are doing. Generally, you can read papers and blog posts from both Big Tech and top earlier-stage companies.</p> </li> </ol>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#low-cost-projects","title":"Low-Cost Projects","text":"<p>There are three main drivers for how much a project will cost:</p> <ol> <li> <p>Data availability: How hard is it to acquire data? How expensive is data labeling? How much data will be needed? How stable is the data? What data security requirements do you have?</p> </li> <li> <p>Accuracy requirement: How costly are wrong predictions? How frequently does the system need to be right to be useful? What are the ethical implications of your model making wrong predictions? It is noteworthy that ML project costs tend to scale super-linearly in the accuracy requirement.</p> </li> <li> <p>Problem difficulty: Is the problem well-defined enough to be solved with ML? Is there good published work on similar problems? How much compute does it take to solve the problem? Generally, it's hard to reason about what's feasible in ML.</p> </li> </ol>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#whats-hard-in-ml","title":"What's Hard in ML?","text":"<p>Here are the three types of hard problems:</p> <ol> <li> <p>Output is complex: The model predictions are ambiguous or in a high-dimensional structure.</p> </li> <li> <p>Reliability is required: ML systems tend to fail in unexpected ways, so anywhere you need high precision or high robustness is going to be more difficult to solve with ML.</p> </li> <li> <p>Generalization is required: These problems tend to be more in the research domain. They can deal with out-of-distribution data or do tasks such as reasoning, planning, or understanding causality.</p> </li> </ol>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#ml-feasibility-assessment","title":"ML Feasibility Assessment","text":"<p>This is a quick checklist you can use to assess the feasibility of your ML projects:</p> <ol> <li> <p>Make sure that you actually need ML.</p> </li> <li> <p>Put in the work upfront to define success criteria with all of the stakeholders.</p> </li> <li> <p>Consider the ethics of using ML.</p> </li> <li> <p>Do a literature review.</p> </li> <li> <p>Try to rapidly build a labeled benchmark dataset.</p> </li> <li> <p>Build a \"minimum\" viable model using manual rules or simple heuristics.</p> </li> <li> <p>Answer this question again: \"Are you sure that you need ML at all?\"</p> </li> </ol>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#not-all-ml-projects-should-be-planned-the-same-way","title":"Not All ML Projects Should Be Planned The Same Way","text":"<p>Not all ML projects have the same characteristics; therefore, they shouldn't be planned the same way. Understanding different archetypes of ML projects can help select the right approach.</p>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#ml-product-archetypes","title":"ML Product Archetypes","text":"<p>The three archetypes offered here are defined by how they interact with real-world use cases:</p> <ol> <li> <p>Software 2.0 use cases: Broadly speaking, this means taking something that software or a product does in an automated fashion today and augmenting its automation with machine learning. An example of this would be improving code completion in the IDE (like Github Copilot).</p> </li> <li> <p>Human-in-the-loop systems: Machine learning can be applied for tasks where automation is not currently deployed - but where humans could have their judgment or efficiency augmented. Simply put, helping humans do their jobs better by complementing them with ML-based tools. An example of this would be turning sketches into slides, a process will usually involve humans approving the output of a machine learning model that made the slides.</p> </li> <li> <p>Autonomous systems: Systems that apply machine learning to augment existing or implement new processes without human input. An example of this would be full self-driving, where there is no opportunity for a driver to intervene in the functioning of the car.</p> </li> </ol> <p>For each archetype, some key considerations inform how you should go about planning projects.</p> <p></p> <ol> <li> <p>In the case of Software 2.0 projects, you should focus more on understanding how impactful the performance of the new model is. Is the model truly much better? How can the performance continue to increase across iterations?</p> </li> <li> <p>In the case of human-in-the-loop systems, consider more the context of the human user and what their needs might be. How good does the system actually have to be to improve the life of a human reviewing its output? In some cases, a model that does even 10% better with accuracy (nominally a small increase) might have outsize impacts on human users in the loop.</p> </li> <li> <p>For autonomous systems, focus heavily on the failure rate and its consequences. When there is no opportunity for human intervention, as is the case with autonomous systems, failures need to be carefully monitored to ensure outsize harm doesn't occur. Self-driving cars are an excellent example of an autonomous system where failure rates are carefully monitored.</p> </li> </ol>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#data-flywheels","title":"Data Flywheels","text":"<p>As you build a software 2.0 project, strongly consider the concept of the data flywheel. For certain ML projects, as you improve your model, your product will get better and more users will engage with the product, thereby generating more data for the model to get even better. It's a classic virtuous cycle and truly the gold standard for ML projects.</p> <p></p> <p>As you consider implementing data flywheels, remember to know the answer to these three questions:</p> <ol> <li> <p>Do you have a data loop? To build a data flywheel, you crucially need to be able to get labeled data from users in a scalable fashion. This helps increase access to high-quality data and define a data loop.</p> </li> <li> <p>Can you turn more data into a better model? This somewhat falls onto you as the modeling expert, but it may also not be the case that more data leads to significantly better performance. Make sure you can actually translate data scale into better model performance.</p> </li> <li> <p>Does better model performance lead to better product use? You need to verify that improvements with models are actually tied to users enjoying the product more and benefiting from it!</p> </li> </ol>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#impact-and-feasibility-of-ml-product-archetypes","title":"Impact and Feasibility of ML Product Archetypes","text":"<p>Let's visit our impact vs. feasibility matrix. Our three product archetypes differ across the spectrum.</p> <p></p> <p>This is a pretty intuitive evaluation you can apply to all your ML projects: If it's harder to build (like autonomous systems), it's likely to have a greater impact! There are ways, however, to change this matrix in the context of specific projects.</p> <ol> <li> <p>For Software 2.0, data flywheels can magnify impact by allowing models to get much better and increase customer delight over time.</p> </li> <li> <p>For human-in-the-loop systems, you can increase feasibility by leveraging good product design. Thoughtful design can help reduce expectations and accuracy requirements. Alternatively, a \"good enough\" mindset that prioritizes incremental delivery over time can make such systems more feasible.</p> </li> <li> <p>For autonomous systems, leveraging humans in the loop can make development more feasible by adding guardrails and reducing the potential impact of failures.</p> </li> </ol>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#just-get-started","title":"Just Get Started!","text":"<p>With all this discussion about archetypes and impact matrices, don't forget the most important component of engineering: actually building! Dive in and get started. Start solving problems and iterate on solutions.</p> <p>One common area practitioners trip up in is tool fetishization. As MLOps and production ML have flourished, so too has the number of tools and platforms that address various aspects of the ML process. You don't need to be perfect with your tooling before driving value from machine learning. Just because Google and Uber are doing things in a very structured, at-scale way doesn't mean you need to as well!</p> <p>In this course, we will primarily focus on how to set things up the right way to do machine learning in production without overcomplicating it. This is an ML products-focused class, not an MLOps class! Check out this talk by Jacopo Tagliabue describing MLOps at Reasonable Scale for a great exposition of this mindset.</p>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#chapter-summary_1","title":"Chapter Summary","text":"<ol> <li> <p>ML adds complexity. Consider whether you really need it.</p> </li> <li> <p>Make sure what you're working on is high impact, or else it might get killed.</p> </li> </ol>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#3-lifecycle","title":"3 - Lifecycle","text":"<p>ML adds complexity to projects and isn't always a value driver. Once you know, however, that it's the right addition to your project, what does the actual lifecycle look like? What steps do we embark upon as we execute?</p> <p>In this course, the common running example we use is of a pose estimation problem. We'll use this as a case study to demonstrate the lifecycle and illustrate various points about ML-powered products.</p> <p></p> <p>Here's a graphic that visualizes the lifecycle of ML projects:</p> <p></p> <p>It provides a very helpful structure. Watch from 48:00 to 54:00 to dive deeper into how this lifecycle occurs in the context of a real machine learning problem around pose estimation that Josh worked on at OpenAI.</p> <p>Let's comment on some specific nuances:</p> <ul> <li> <p>Machine learning projects tend to be very iterative. Each of these phases can feed back into any of the phases that go before it, as you learn more about the problem that you're working on.</p> <ul> <li> <p>For example, you might realize that \"Actually, it's way too hard for us to get data in order to solve this problem!\" or \"It's really difficult for us to label the pose of these objects in 3D space\".</p> </li> <li> <p>A solution might actually be to go back a step in the lifecycle and set up the problem differently. For example, what if it were cheaper to annotate per pixel?</p> </li> <li> <p>This could repeat itself multiple times as you progress through a project. It's a normal and expected part of the machine learning product development process.</p> </li> </ul> </li> <li> <p>In addition to iteration during execution, there's also cross-project \"platform\" work that matters! Hiring and infrastructure development are crucial to the long-term health of your project.</p> </li> <li> <p>Going through this lifecycle and winning each step is what we'll cover in this class!</p> </li> </ul>"},{"location":"course/2022/lecture-1-course-vision-and-when-to-use-ml/#lecture-summary","title":"Lecture Summary","text":"<p>In summary, here's what we covered in this lecture:</p> <ol> <li> <p>ML is NOT a cure-all. It's a complex technology that needs to be used thoughtfully.</p> </li> <li> <p>You DON'T need a perfect setup to get going. Start building and iterate!</p> </li> <li> <p>The lifecycle of machine learning is purposefully iterative and circuitous. We'll learn how to master this process together!</p> </li> </ol>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/","title":"Lecture 2: Development Infrastructure &amp; Tooling","text":"<p>Lecture by Sergey Karayev. Notes by James Le and Vishnu Rachakonda. Published August 15, 2022. Download slides.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#1-introduction","title":"1 - Introduction","text":"<p>The dream of ML development is that given a project spec and some sample data, you get a continually improving prediction system deployed at scale.</p> <p>The reality is starkly different:</p> <ul> <li> <p>You have to collect, aggregate, process, clean, label, and version the data.</p> </li> <li> <p>You have to find the model architecture and their pre-trained weights and then write and debug the model code.</p> </li> <li> <p>You run training experiments and review the results, which will be fed back into the process of trying out new architectures and debugging more code.</p> </li> <li> <p>You can now deploy the model.</p> </li> <li> <p>After model deployment, you have to monitor model predictions and close the data flywheel loop. Basically, your users generate fresh data for you, which needs to be added to the training set.</p> </li> </ul> <p></p> <p>This reality has roughly three components: data, development, and deployment. The tooling infrastructure landscape for them is large, so we'll have three lectures to cover it all. This lecture focuses on the development component.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#2-software-engineering","title":"2 - Software Engineering","text":""},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#language","title":"Language","text":"<p>For your choice of programming language, Python is the clear winner in scientific and data computing because of all the libraries that have been developed. There have been some contenders like Julia and C/C++, but Python has really won out.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#editors","title":"Editors","text":"<p>To write Python code, you need an editor. You have many options, such as Vim, Emacs, Jupyter Notebook/Lab, VS Code, PyCharm, etc.</p> <ul> <li> <p>We recommend VS Code because of its nice features such as built-in git version control, documentation peeking, remote projects opening, linters and type hints to catch bugs, etc.</p> </li> <li> <p>Many practitioners develop in Jupyter Notebooks, which is great as the \"first draft\" of a data science project. You have to put in little thought before you start coding and seeing the immediate output. However, notebooks have a variety of problems: primitive editor, out-of-order execution artifacts, and challenges to version and test them. A counterpoint to these problems is the nbdev package that lets you write and test code all in one notebook environment.</p> </li> <li> <p>We recommend you use VS Code with built-in support for notebooks - where you can write code in modules imported into notebooks. It also enables awesome debugging.</p> </li> </ul> <p>If you want to build something more interactive, Streamlit is an excellent choice. It lets you decorate Python code, get interactive applets, and publish them on the web to share with the world.</p> <p></p> <p>For setting up the Python environment, we recommend you see how we did it in the lab.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#3-deep-learning-frameworks","title":"3 - Deep Learning Frameworks","text":"<p>Deep learning is not a lot of code with a matrix math library like Numpy. But when you have to deploy your code onto CUDA for GPU-powered deep learning, you want to consider deep learning frameworks as you might be writing weird layer types, optimizers, data interfaces, etc.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#frameworks","title":"Frameworks","text":"<p>There are various frameworks, such as PyTorch, TensorFlow, and Jax. They are all similar in that you first define your model by running Python code and then collect an optimized execution graph for different deployment patterns (CPU, GPU, TPU, mobile).</p> <ol> <li> <p>We prefer PyTorch because it is absolutely dominant by measures such as the number of models, the number of papers, and the number of competition winners. For instance, about 77% of 2021 ML competition winners used PyTorch.</p> </li> <li> <p>With TensorFlow, you have TensorFlow.js (that lets you run deep learning models in your browser) and Keras (an unmatched developer experience for easy model development).</p> </li> <li> <p>Jax is a meta-framework for deep learning.</p> </li> </ol> <p></p> <p>PyTorch has excellent developer experience and is production-ready and even faster with TorchScript. There is a great distributed training ecosystem. There are libraries for vision, audio, etc. There are also mobile deployment targets.</p> <p>PyTorch Lightning provides a nice structure for organizing your training code, optimizer code, evaluation code, data loaders, etc. With that structure, you can run your code on any hardware. There are nice features such as performance and bottleneck profiler, model checkpointing, 16-bit precision, and distributed training libraries.</p> <p>Another possibility is FastAI software, which is developed alongside the fast.ai course. It provides many advanced tricks such as data augmentations, better initializations, learning rate schedulers, etc. It has a modular structure with low-level API, mid-level API, high-level API, and specific applications. The main problem with FastAI is that its code style is quite different from mainstream Python.</p> <p>At FSDL, we prefer PyTorch because of its strong ecosystem, but TensorFlow is still perfectly good. If you have a specific reason to prefer it, you are still going to have a good time.</p> <p>Jax is a more recent project from Google that is not specific to deep learning. It provides general vectorization, auto-differentiation, and compilation to GPU/TPU code. For deep learning, there are separate frameworks like Flax and Haiku. You should only use Jax for a specific need.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#meta-frameworks-and-model-zoos","title":"Meta-Frameworks and Model Zoos","text":"<p>Most of the time, you will start with at least a model architecture that someone has developed or published. You will use a specific architecture (trained on specific data with pre-trained weights) on a model hub.</p> <ul> <li> <p>ONNX is an open standard for saving deep learning models and lets you convert from one type of format to another. It can work well but can also run into some edge cases.</p> </li> <li> <p>HuggingFace has become an absolutely stellar repository of models. It started with NLP tasks but has then expanded into all kinds of tasks (audio classification, image classification, object detection, etc.). There are 60,000 pre-trained models for all these tasks. There is a Transformers library that works with PyTorch, TensorFlow, and Jax. There are 7,500 datasets uploaded by people. There's also a community aspect to it with a Q&amp;A forum.</p> </li> <li> <p>TIMM is a collection of state-of-the-art computer vision models and related code that looks cool.</p> </li> </ul>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#4-distributed-training","title":"4 - Distributed Training","text":"<p>Let's say we have multiple machines represented by little squares above (with multiple GPUs in each machine). You are sending batches of data to be processed by a model with parameters. The data batch can fit on a single GPU or not. The model parameters can fit on a single GPU or not.</p> <p>The best case is that both your data batch and model parameters fit on a single GPU. That's called trivial parallelism. You can either launch more independent experiments on other GPUs/machines or increase the batch size until it no longer fits on one GPU.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#data-parallelism","title":"Data Parallelism","text":"<p>If your model still fits on a single GPU, but your data no longer does, you have to try out data parallelism - which lets you distribute a single batch of data across GPUs and average gradients that are computed by the model across GPUs. A lot of model development work is cross-GPU, so you want to ensure that GPUs have fast interconnects.</p> <p>If you are using a server card, expect a linear speedup in training time. If you are using a consumer card, expect a sublinear speedup instead.</p> <p>Data parallelism is implemented in PyTorch with the robust DistributedDataParallel library. Horovod is another 3rd-party library option. PyTorch Lightning makes it dead simple to use either of these two libraries - where speedup seems to be the same.</p> <p>A more advanced scenario is that you can't even fit your model on a single GPU. You have to spread the model over multiple GPUs. There are three solutions to this.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#sharded-data-parallelism","title":"Sharded Data-Parallelism","text":"<p>Sharded data parallelism starts with the question: What exactly takes up GPU memory?</p> <ul> <li> <p>The model parameters include the floats that make up our model layers.</p> </li> <li> <p>The gradients are needed to do back-propagation.</p> </li> <li> <p>The optimizer states include statistics about the gradients</p> </li> <li> <p>Finally, you have to send a batch of data for model development.</p> </li> </ul> <p></p> <p>Sharding is a concept from databases where if you have one source of data, you actually break it into shards of data that live across your distributed system. Microsoft implemented an approach called ZeRO that shards the optimizer states, the gradients, and the model parameters. This results in an insane order of magnitude reduction in memory use, which means your batch size can be 10x bigger. You should watch the video in this article to see how model parameters are passed around GPUs as computation proceeds.</p> <p>Sharded data-parallelism is implemented by Microsoft's DeepSpeed library and Facebook's FairScale library, as well as natively by PyTorch. In PyTorch, it's called Fully-Sharded DataParallel. With PyTorch Lightning, you can try it for a massive memory reduction without changing the model code.</p> <p>This same ZeRO principle can also be applied to a single GPU. You can train a 13B-parameter model on a single V100 (32GB) GPU. Fairscale implements this (called CPU-offloading).</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#pipelined-model-parallelism","title":"Pipelined Model-Parallelism","text":"<p>Model parallelism means that you can put each layer of your model on each GPU. It is trivial to implement natively but results in only one GPU being active at a time. Libraries like DeepSpeed and FairScale make it better by pipelining computation so that the GPUs are fully utilized. You need to tune the amount of pipelining on the batch size to the exact degree of how you will split up the model on the GPU.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#tensor-parallelism","title":"Tensor-Parallelism","text":"<p>Tensor parallelism is another approach, which observes that there is nothing special about matrix multiplication that requires the whole matrix to be on one GPU. You can distribute the matrix over multiple GPUs. NVIDIA published the Megatron-LM repo, which does this for the Transformer model.</p> <p>You can actually use all of the three techniques mentioned above if you really want to scale a huge model (like a GPT-3 sized language model). Read this article on the technology behind BLOOM training for a taste.</p> <p></p> <p>In conclusion:</p> <ul> <li> <p>If your model and data fit on one GPU, that's awesome.</p> </li> <li> <p>If they do not, and you want to speed up training, try DistributedDataParallel.</p> </li> <li> <p>If the model still doesn't fit, try ZeRO-3 or Full-Sharded Data Parallel.</p> </li> </ul> <p>For more resources to speed up model training, look at this list compiled by DeepSpeed, MosaicML, and FFCV.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#5-compute","title":"5 - Compute","text":"<p>Compute is the next essential ingredient to developing machine learning models and products.</p> <p>The compute-intensiveness of models has grown tremendously over the last ten years, as the below charts from OpenAI and HuggingFace show.</p> <p></p> <p>Recent developments, including models like GPT-3, have accelerated this trend. These models are extremely large and require a large number of petaflops to train.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#gpus","title":"GPUs","text":"<p>To effectively train deep learning models, GPUs are required. NVIDIA has been the superior choice for GPU vendors, though Google has introduced TPUs (Tensor Processing Units) that are effective but are only available via Google Cloud. There are three primary considerations when choosing GPUs:</p> <ol> <li> <p>How much data fits on the GPU?</p> </li> <li> <p>How fast can the GPU crunch through data? To evaluate this, is your data 16-bit or 32-bit? The latter is more resource intensive.</p> </li> <li> <p>How fast can you communicate between the CPU and the GPU and between GPUs?</p> </li> </ol> <p>Looking at recent NVIDIA GPUs, it becomes clear that a new high-performing architecture is introduced every few years. There's a difference between these chips, which are licensed for personal use as opposed to corporate use; businesses should only use server cards.</p> <p></p> <p>Two key factors in evaluating GPUs are RAM and Tensor TFlops. The more RAM, the better the GPU contains large models and datasets. Tensor TFlops are special tensor cores that NVIDIA includes specifically for deep learning operations and can handle more intensive mixed-precision operations. A tip: leveraging 16-bit training can effectively double your RAM capacity!</p> <p>While these theoretical benchmarks are useful, how do GPUs perform practically? Lambda Labs offers the best benchmarks here. Their results show that the most recent server-grade NVIDIA GPU (A100) is more than 2.5 times faster than the classic V100 GPU. RTX chips also outperform the V100. AIME is also another source of GPU benchmarks.</p> <p>Cloud services such as Microsoft Azure, Google Cloud Platform, and Amazon Web Services are the default place to buy access to GPUs. Startup cloud providers like Paperspace, CoreWeave, and Lambda Labs also offer such services.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#tpus","title":"TPUs","text":"<p>Let's briefly discuss TPUs. There are four generations of TPUs, and the most recent v4 is the fastest possible accelerator for deep learning. V4 TPUs are not generally available yet, but TPUs generally excel at scaling to larger and model sizes. The below charts compare TPUs to the fastest A100 NVIDIA chip.</p> <p></p> <p>It can be overwhelming to compare the cost of cloud access to GPUs, so we made a tool that solves this problem! Feel free to contribute to our repository of Cloud GPU cost metrics. The tool has all kinds of nifty features like enabling filters for only the most recent chip models, etc.</p> <p>If we combine the cost metrics with performance metrics, we find that the most expensive per hour chips are not the most expensive per experiment! Case in point: running the same Transformers experiment on 4 V100s costs $1750 over 72 hours, whereas the same experiment on 4 A100s costs $250 over only 8 hours. Think carefully about cost and performance based on the model you're trying to train.</p> <p>Some helpful heuristics here are:</p> <ol> <li> <p>Use the most expensive per-hour GPU in the least expensive cloud.</p> </li> <li> <p>Startups (e.g., Paperspace) tend to be cheaper than major cloud providers.</p> </li> </ol>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#on-prem-vs-cloud","title":"On-Prem vs. Cloud","text":"<p>For on-prem use cases, you can build your own pretty easily or opt for a pre-built computer from a company like NVIDIA. You can build a good, quiet PC with 128 GB RAM and 2 RTX 3090s for about $7000 and set it up in a day. Going beyond this can start to get far more expensive and complicated. Lambda Labs offers a $60,000 machine with 8 A100s (super fast!). Tim Dettmers offers a great (slightly outdated) perspective on building a machine here.</p> <p>Some tips on on-prem vs. cloud use:</p> <ul> <li> <p>It can be useful to have your own GPU machine to shift your mindset from minimizing cost to maximizing utility.</p> </li> <li> <p>To truly scale-out experiments, you should probably just use the most expensive machines in the least expensive cloud.</p> </li> <li> <p>TPUs are worth experimenting with for large-scale training, given their performance.</p> </li> <li> <p>Lambda Labs is a sponsor, and we highly encourage looking at them for on-prem and cloud GPU use!</p> </li> </ul>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#6-resource-management","title":"6 - Resource Management","text":"<p>Now that we've talked about raw compute, let's talk about options for how to manage our compute resources. Let's say we want to manage a set of experiments. Broadly speaking, we'll need hardware in the form of GPUs, software requirements (e.g., PyTorch version), and data to train on.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#solutions","title":"Solutions","text":"<p>Leveraging best practices for specifying dependencies (e.g., Poetry, conda, pip-tools) makes the process of spinning up such experiments quick and easy on a single machine.</p> <p>If, however, you have a cluster of machines to run experiments on, SLURM is the tried and true solution for workload management that is still widely used.</p> <p>For more portability, Docker is a way to package up an entire dependency stack into a lighter-than-a-VM package. Kubernetes is the most popular way to run many Docker containers on top of a cluster. The OSS Kubeflow project helps manage ML projects that rely on Kubernetes.</p> <p>These projects are useful, but they may not be the easiest or best choice. They're great if you already have a cluster up and running, but how do you actually set up a cluster or compute platform?</p> <p>Before proceeding, FSDL prefers open source and/or transparently priced products. We discuss tools that fall into these categories, not SaaS with opaque pricing.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#tools","title":"Tools","text":"<p>For practitioners all in on AWS, AWS Sagemaker offers a convenient end-to-end solution for building machine learning models, from labeling data to deploying models. Sagemaker has a ton of AWS-specific configuration, which can be a turnoff, but it brings a lot of easy-to-use old school algorithms for training and allows you to BYO algorithms as well. They're also increasing support for PyTorch, though the markup for PyTorch is about 15-20% more expensive.</p> <p>Anyscale is a company created by the makers of the Berkeley OSS project Ray. Anyscale recently launched Ray Train, which they claim is faster than Sagemaker with a similar value proposition. Anyscale makes it really easy to provision a compute cluster, but it's considerably more expensive than alternatives.</p> <p>Grid.ai is created by the PyTorch Lightning creators. Grid allows you to specify what compute parameters to use easily with \"grid run\" followed by the types of compute and options you want. You can use their instances or AWS under the hood. Grid has an uncertain future, as its future compatibility with Lightning (given their rebrand) has not been clarified.</p> <p>There are several non-ML options for spinning up compute too! Writing your own scripts, using various libraries, or even Kubernetes are all options. This route is harder.</p> <p>Determined.AI is an OSS solution for managing on-prem and cloud clusters. They offer cluster management, distributed training, and more. It's pretty easy to use and is in active development.</p> <p>With all this said, there is still room to improve the ease of experience for launching training on many cloud providers.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#7-experiment-and-model-management","title":"7 - Experiment and Model Management","text":"<p>In contrast to compute, experiment management is quite close to being solved. Experiment management refers to tools and processes that help us keep track of code, model parameters, and data sets that are iterated on during the model development lifecycle. Such tools are essential to effective model development. There are several solutions here:</p> <ul> <li> <p>TensorBoard: A non-exclusive Google solution effective at one-off experiment tracking. It is difficult to manage many experiments.</p> </li> <li> <p>MLflow: A non-exclusive Databricks project that includes model packaging and more, in addition to experiment management. It must be self-hosted.</p> </li> <li> <p>Weights and Biases: An easy-to-use solution that is free for personal and academic projects! Logging starts simply with an \"experiment config\" command.</p> </li> <li> <p>Other options include Neptune AI, Comet ML, and Determined AI, all of which have solid experiment tracking options.</p> </li> </ul> <p>Many of these platforms also offer intelligent hyperparameter optimization, which allows us to control the cost of searching for the right parameters for a model. For example, Weights and Biases has a product called Sweeps that helps with hyperparameter optimization. It's best to have it as part of your regular ML training tool; there's no need for a dedicated tool.</p>"},{"location":"course/2022/lecture-2-development-infrastructure-and-tooling/#8-all-in-one","title":"8 - \"All-In-One\"","text":"<p>There are machine learning infrastructure solutions that offer everything--training, experiment tracking, scaling out, deployment, etc. These \"all-in-one\" platforms simplify things but don't come cheap! Examples include Gradient by Paperspace, Domino Data Lab, AWS Sagemaker, etc.</p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/","title":"Lecture 3: Troubleshooting &amp; Testing","text":"<p>Lecture by Charles Frye. Notes by James Le and Vishnu Rachakonda. Published August 22, 2022. Download slides.</p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#1-testing-software","title":"1 - Testing Software","text":"<ol> <li> <p>The general approach is that tests will help us ship faster with fewer bugs, but they won't catch all of our bugs.</p> </li> <li> <p>That means we will use testing tools but won't try to achieve 100% coverage.</p> </li> <li> <p>Similarly, we will use linting tools to improve the development experience but leave escape valves rather than pedantically following our style guides.</p> </li> <li> <p>Finally, we'll discuss tools for automating these workflows.</p> </li> </ol>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#11-tests-help-us-ship-faster-they-dont-catch-all-bugs","title":"1.1 - Tests Help Us Ship Faster. They Don't Catch All Bugs","text":"<p>Tests are code we write that are designed to fail intelligibly when our other code has bugs. These tests can help catch some bugs before they are merged into the main product, but they can't catch all bugs. The main reason is that test suites are not certificates of correctness. In some formal systems, tests can be proof of code correctness. But we are writing in Python (a loosely goosey language), so all bets are off in terms of code correctness.</p> <p>Nelson Elhage framed test suites more like classifiers. The classification problem is: does this commit have a bug, or is it okay? The classifier output is whether the tests pass or fail. We can then treat test suites as a \"prediction\" of whether there is a bug, which suggests a different way of designing our test suites.</p> <p>When designing classifiers, we need to trade off detection and false alarms. If we try to catch all possible bugs, we can inadvertently introduce false alarms. The classic signature of a false alarm is a failed test - followed by a commit that fixes the test rather than the code.</p> <p>To avoid introducing too many false alarms, it's useful to ask yourself two questions before adding a test:</p> <ol> <li> <p>Which real bugs will this test catch?</p> </li> <li> <p>Which false alarms will this test raise?</p> </li> </ol> <p>If you can think of more examples for the second question than the first one, maybe you should reconsider whether you need this test.</p> <p>One caveat is that: in some settings, correctness is important. Examples include medical diagnostics/intervention, self-driving vehicles, and banking/finance. A pattern immediately arises here: If you are operating in a high-stakes situation where errors have consequences for people's lives and livelihoods, even if it's not regulated yet, it might be regulated soon. These are examples of low-feasibility, high-impact ML projects discussed in the first lecture.</p> <p></p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#12-use-testing-tools-but-dont-chase-coverage","title":"1.2 - Use Testing Tools, But Don't Chase Coverage","text":"<ul> <li> <p>Pytest is the standard tool for testing Python code. It has a Pythonic implementation and powerful features such as creating separate suites, sharing resources across tests, and running parametrized variations of tests.</p> </li> <li> <p>Pure text docs can't be checked for correctness automatically, so they are hard to maintain or trust. Python has a nice module, [doctests], for checking code in the documentation and preventing rot.</p> </li> <li> <p>Notebooks help connect rich media (charts, images, and web pages) with code execution. A cheap and dirty solution to test notebooks is adding some asserts and using nbformat to run the notebooks.</p> </li> </ul> <p></p> <p>Once you start adding different types of tests and your codebase grows, you will want coverage tools for recording which code is checked or \"covered\" by tests. Typically, this is done in lines of code, but some tools can be more fine-grained. We recommend Codecov, which generates nice visualizations you can use to drill down and get a high-level overview of the current state of your testing. Codecov helps you understand your tests and can be incorporated into your testing. You can say you want to reject commits not only where tests fail, but also where test coverage goes down below a certain threshold.</p> <p>However, we recommend against that. Personal experience, interviews, and published research suggest that only a small fraction of the tests you write will generate most of your value. The right tactic, engineering-wise, is to expand the limited engineering effort we have on the highest-impact tests and ensure that those are super high quality. If you set a coverage target, you will instead write tests in order to meet that coverage target (regardless of their quality). You end up spending more effort to write tests and deal with their low quality.</p> <p></p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#13-use-linting-tools-but-leave-escape-valves","title":"1.3 - Use Linting Tools, But Leave Escape Valves","text":"<p>Clean code is of uniform and standard style.</p> <ol> <li> <p>Uniform style helps avoid spending engineering time on arguments over style in pull requests and code review. It also helps improve the utility of our version control by cutting down on noisy components of diffs and reducing their size. Both benefits make it easier for humans to visually parse the diffs in our version control system and make it easier to build automation around them.</p> </li> <li> <p>Standard style makes it easier to accept contributions for an open-source repository and onboard new team members for a closed-source system.</p> </li> </ol> <p></p> <p>One aspect of consistent style is consistent code formatting (with things like whitespace). The standard tool for that in Python is [the] [black] Python formatter. It's a very opinionated tool with a fairly narrow scope in terms of style. It focuses on things that can be fully automated and can be nicely integrated into your editor and automated workflows.</p> <p>For non-automatable aspects of style (like missing docstrings), we recommend [flake8]. It comes with many extensions and plugins such as docstring completeness, type hinting, security, and common bugs.</p> <p>ML codebases often have both Python code and shell scripts in them. Shell scripts are powerful, but they also have a lot of sharp edges. shellcheck knows all the weird behaviors of bash that often cause errors and issues that aren't immediately obvious. It also provides explanations for why it's raising a warning or an error. It's very fast to run and can be easily incorporated into your editor.</p> <p></p> <p>One caveat to this is: pedantic enforcement of style is obnoxious. To avoid frustration with code style and linting, we recommend:</p> <ol> <li> <p>Filtering rules down to the minimal style that achieves the goals we set out (sticking with standards, avoiding arguments, keeping version control history clean, etc.)</p> </li> <li> <p>Having an \"opt-in\" application of rules and gradually growing coverage over time - which is especially important for existing codebases (which may have thousands of lines of code that we need to be fixed).</p> </li> </ol>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#14-always-be-automating","title":"1.4 - Always Be Automating","text":"<p>To make the best use of testing and linting practices, you want to automate these tasks and connect to your cloud version control system (VCS). Connecting to the VCS state reduces friction when trying to reproduce or understand errors. Furthermore, running things outside of developer environments means that you can run tests automatically in parallel to other development work.</p> <p>Popular, open-source repositories are the best place to learn about automation best practices. For instance, the PyTorch Github library has tons of automated workflows built into the repo - such as workflows that automatically run on every push and pull.</p> <p></p> <p>The tool that PyTorch uses (and that we recommend) is GitHub Actions, which ties automation directly to VCS. It is powerful, flexible, performant, and easy to use. It gets great documentation, can be used with a YAML file, and is embraced by the open-source community. There are other options such as pre-commit.ci, CircleCI, and Jenkins; but GitHub Actions seems to have won the hearts and minds in the open-source community in the last few years.</p> <p>To keep your version control history as clean as possible, you want to be able to run tests and linters locally before committing. We recommend pre-commit to enforce hygiene checks. You can use it to run formatting, linting, etc. on every commit and keep the total runtime to a few seconds. pre-commit is easy to run locally and easy to automate with GitHub Actions.</p> <p>Automation to ensure the quality and integrity of our software is a productivity enhancer. That's broader than just CI/CD. Automation helps you avoid context switching, surfaces issues early, is a force multiplier for small teams, and is better documented by default.</p> <p>One caveat is that: automation requires really knowing your tools. Knowing Docker well enough to use it is not the same as knowing Docker well enough to automate it. Bad automation, like bad tests, takes more time than it saves. Organizationally, that makes automation a good task for senior engineers who have knowledge of these tools, have ownership over code, and can make these decisions around automation.</p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#summary","title":"Summary","text":"<ol> <li> <p>Automate tasks with GitHub Actions to reduce friction.</p> </li> <li> <p>Use the standard Python toolkit for testing and cleaning your projects.</p> </li> <li> <p>Choose testing and linting practices with the 80/20 principle, shipping velocity, and usability/developer experience in mind.</p> </li> </ol>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#2-testing-ml-systems","title":"2 - Testing ML Systems","text":"<ol> <li> <p>Testing ML is hard, but not impossible.</p> </li> <li> <p>We should stick with the low-hanging fruit to start.</p> </li> <li> <p>Test your code in production, but don't release bad code.</p> </li> </ol>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#21-testing-ml-is-hard-but-not-impossible","title":"2.1 - Testing ML Is Hard, But Not Impossible","text":"<p>Software engineering is where many testing practices have been developed. In software engineering, we compile source code into programs. In machine learning, training compiles data into a model. These components are harder to test:</p> <ol> <li> <p>Data is heavier and more inscrutable than source code.</p> </li> <li> <p>Training is more complex and less well-defined.</p> </li> <li> <p>Models have worse tools for debugging and inspection than compiled programs.</p> </li> </ol> <p>In this section, we will focus primarily on \"smoke\" tests. These tests are easy to implement and still effective. They are among the 20% of tests that get us 80% of the value.</p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#22-use-expectation-testing-on-data","title":"2.2 - Use Expectation Testing on Data","text":"<p>We test our data by checking basic properties. We express our expectations about the data, which might be things like there are no nulls in this column or the completion date is after the start date. With expectation testing, you will start small with only a few properties and grow them slowly. You only want to test things that are worth raising alarms and sending notifications to others.</p> <p></p> <p>We recommend [great_expectations] for data testing. It automatically generates documentation and quality reports for your data, in addition to built-in logging and alerting designed for expectation testing. To get started, check out this MadeWithML tutorial on great_expectations.</p> <p></p> <p>To move forward, you want to stay as close to the data as possible:</p> <ol> <li> <p>A common pattern is that there's a benchmark dataset with annotations (in academia) or an external annotation team (in the industry). A lot of the detailed information about that data can be extracted by simply looking at it.</p> </li> <li> <p>One way for data to get internalized into the organization is that at the start of the project, model developers annotate data ad-hoc (especially if you don't have the budget for an external annotation team).</p> </li> <li> <p>However, if the model developers at the start of the project move on and more developers get onboarded, that knowledge is diluted. A better solution is an internal annotation team that has a regular information flow with the model developers is a better solution.</p> </li> <li> <p>The best practice (recommended by Shreya Shankar) is to have a regular on-call rotation where model developers annotate data themselves. Ideally, these are fresh data so that all members of the team who are developing models know about the data and build intuition/expertise in the data.</p> </li> </ol>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#23-use-memorization-testing-on-training","title":"2.3 - Use Memorization Testing on Training","text":"<p>Memorization is the simplest form of learning. Deep neural networks are very good at memorizing data, so checking whether your model can memorize a very small fraction of the full data set is a great smoke test for training. If a model can't memorize, then something is clearly very wrong!</p> <p>Only really gross issues with training will show up with this test. For example, your gradients may not be calculated correctly, you have a numerical issue, or your labels have been shuffled; serious issues like these. Subtle bugs in your model or your data are not going to show up. A way to catch smaller bugs is to include the length of run time in your test coverage. It's a good way to detect if smaller issues are making it harder for your model to learn. If the number of epochs it takes to reach an expected performance suddenly goes up, it may be due to a training bug. PyTorch Lightning has an \"overfit_batches\" feature that can help with this.</p> <p>Make sure to tune memorization tests to run quickly, so you can regularly run them. If they are under 10 minutes or some short threshold, they can be run every PR or code change to better catch breaking changes. A couple of ideas for speeding up these tests are below:</p> <p></p> <p>Overall, these ideas lead to memorization tests that implement model training on different time scale and allow you to mock out scenarios.</p> <p>A solid, if expensive idea for testing training is to rerun old training jobs with new code. It's not something that can be run frequently, but doing so can yield lessons about what unexpected changes might have happened in your training pipeline. The main drawback is the potential expense of running these tests. CI platforms like CircleCI charge a great deal for GPUs, while others like Github Actions don't offer access to the relevant machines easily.</p> <p>The best option for testing training is to regularly run training with new data that's coming in from production. This is still expensive, but it is directly related to improvements in model development, not just testing for breakages. Setting this up requires a data flywheel similar to what we talked about in Lecture 1. Further tooling needed to achieve will be discussed down the line.</p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#24-adapt-regression-testing-for-models","title":"2.4 - Adapt Regression Testing for Models","text":"<p>Models are effectively functions. They have inputs and produce outputs like any other function in code. So, why not test them like functions with regression testing? For specific inputs, we can check to see whether the model consistently returns the same outputs. This is best done with simpler models like classification models. It's harder to maintain such tests with more complex models. However, even in a more complex model scenario, regression testing can be useful for comparing changes from training to production.</p> <p></p> <p>A more sophisticated approach to testing for ML models is to use loss values and model metrics to build documented test suites out of your data. Consider this similar to the test-driven development (TDD) code writing paradigm. The test that is written before your code in TDD is akin to your model's loss performance; both represent the gap between where your code needs to be and where it is. Over time, as we improve the loss metric, our model is getting closer to passing \"the test\" we've imposed on it. The gradient descent we use to improve the model can be considered a TDD approach to machine learning models!</p> <p></p> <p>While gradient descent is somewhat like TDD, it's not exactly the same because simply reviewing metrics doesn't tell us how to resolve model failures (the way traditional software tests do).</p> <p>To fill in this gap, start by looking at the data points that have the highest loss. Flag them for a test suite composed of \"hard\" examples. Doing this provides two advantages: it helps find where the model can be improved, and it can also help find errors in the data itself (i.e. poor labels).</p> <p>As you examine these failures, you can aggregate types of failures into named suites. For example in a self-driving car use case, you could have a \"night time\" suite and a \"reflection\" suite. Building these test suites can be considered the machine learning version of regression testing, where you take bugs that you've observed in production and add them to your test suite to make sure that they don't come up again.</p> <p></p> <p>The method can be quite manual, but there are some options for speeding it up. Partnering with the annotation team at your company can help make developing these tests a lot faster. Another approach is to use a method called Domino that uses foundation models to find errors. Additionally, for testing NLP models, use the CheckList approach.</p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#25-test-in-production-but-dont-yolo","title":"2.5 - Test in Production, But Don't YOLO","text":"<p>It's crucial to test in true production settings. This is especially true for machine learning models, because data is an important component of both the production and the development environments. It's difficult to ensure that both are very close to one another.</p> <p>The best way to solve the training and production difference is to test in production.</p> <p>Testing in production isn't sufficient on its own. Rather, testing in production allows us to develop tooling and infrastructure that allows us to resolve production errors quickly (which are often quite expensive). It reduces pressure on other kinds of testing, but does not replace them.</p> <p></p> <p>We will cover in detail the tooling needed for production monitoring and continual learning of ML systems in a future lecture.</p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#26-ml-test-score","title":"2.6 - ML Test Score","text":"<p>So far, we have discussed writing \"smoke\" tests for ML: expectation tests for data, memorization tests for training, and regression tests for models.</p> <p>As your code base and team mature, adopt a more full-fledged approach to testing ML systems like the approach identified in the ML Test Score paper. The ML Test Score is a rubric that evolved out of machine learning efforts at Google. It's a strict rubric for ML test quality that covers data, models, training, infrastructure, and production monitoring. It overlaps with, but goes beyond some of the recommendations we've offered.</p> <p></p> <p>It's rather expensive, but worth it for high stakes use cases that need to be really well-engineered! To be really clear, this rubric is really strict. Even our Text Recognizer system we've designed so far misses a few categories. Use the ML Test Score as inspiration to develop the right testing approach that works for your team's resources and needs.</p> <p></p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#3-troubleshooting-models","title":"3 - Troubleshooting Models","text":"<p>Tests help us figure out something is wrong, but troubleshooting is required to actually fix broken ML systems. Models often require the most troubleshooting, and in this section we'll cover a three step approach to troubleshooting them.</p> <ol> <li> <p>\"Make it run\" by avoiding common errors.</p> </li> <li> <p>\"Make it fast\" by profiling and removing bottlenecks.</p> </li> <li> <p>\"Make it right\" by scaling model/data and sticking with proven architectures.</p> </li> </ol>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#31-make-it-run","title":"3.1 - Make It Run","text":"<p>This is the easiest step for models; only a small portion of bugs cause the kind of loud failures that prevent a model from running at all. Watch out for these bugs in advance and save yourself the trouble of models that don't run.</p> <p>The first type of bugs that prevent models from running at all are shape errors. When the shape of the tensors don't match for the operations run on them, models can't be trained or run. Prevent these errors by keeping notes on the expected size of tensors, annotate the sizes in the code, and even step through your model code with a debugger to check tensor size as you go.</p> <p></p> <p>The second type of bugs is out of memory errors. This occurs when you try to push a tensor to a GPU that is too large to fit. PyTorch Lightning has good tools to prevent this. Make sure you're using the lowest precision your training can tolerate; a good default is 16 bit precision. Another common reason for this is trying to run a model on too much data or too large a batch size. Use the autoscale batch size feature in PyTorch Lightning to pick the right size batch. You can use gradient accumulation if these batch sizes get too small. If neither of these options work, you can look into manual techniques like tensor parallelism and gradient checkpoints.</p> <p>Numerical errors also cause machine learning failures. This is when NaNs or infinite values show up in tensors. These issues most commonly appear first in the gradient and then cascade through the model. PyTorch Lightning has a good tool for tracking and logging gradient norms. A good tip to check whether these issues are caused by precision issues is to switch to Python 64 bit floats and see if that causes these issues to go away. Normalization layers tend to cause these issues, generally speaking. So watch out for how you do normalization!</p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#32-make-it-fast","title":"3.2 - Make It Fast","text":"<p>Once you can run a model, you'll want it to run fast. This can be tricky because the performance of DNN training code is very counterintuitive. For example, transformers can actually spend more time in the MLP layer than the attention layer. Similarly, trivial components like loading data can soak up performance.</p> <p>To solve these issues, the primary solution is to roll up your sleeves and profile your code. You can often find pretty easy Python changes that yield big results. Read these two tutorials by Charles and Horace for more details.</p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#33-make-it-right","title":"3.3 - Make It Right","text":"<p>After you make it run fast, make the model right. Unlike traditional software, machine learning models never are truly perfect. Production performance is never perfect. As such, it might be more appropriate to say \"make it as right as needed\".</p> <p>Knowing this, making the model run and run fast allows us to make the model right through applying scale. To achieve performance benefits, scaling a model or its data are generally fruitful and achievable routes. It's a lot easier to scale a fast model. Research from OpenAI and other institutions is showing that benefits from scale can be rigorously measured and predicted across compute budget, dataset size, and parameter count.</p> <p></p> <p>If you can't afford to scale yourself, consider finetuning a model trained at scale for your task.</p> <p>So far, all of the advice given has been model and task-agnostic. Anything more detailed has to be specific to the model and the relevant task. Stick close to working architectures and hyperparameters from places like HuggingFace, and try not to reinvent the wheel!</p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#4-resources","title":"4 - Resources","text":"<p>Here are some helpful resources that discuss this topic.</p>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#tweeters","title":"Tweeters","text":"<ol> <li> <p>Julia Evans</p> </li> <li> <p>Charity Majors</p> </li> <li> <p>Nelson Elhage</p> </li> <li> <p>kipply</p> </li> <li> <p>Horace He</p> </li> <li> <p>Andrej Karpathy</p> </li> <li> <p>Chip Huyen</p> </li> <li> <p>Jeremy Howard</p> </li> <li> <p>Ross Wightman</p> </li> </ol>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#templates","title":"Templates","text":"<ol> <li> <p>Lightning Hydra Template</p> </li> <li> <p>NN Template</p> </li> <li> <p>Generic Deep Learning Project Template</p> </li> </ol>"},{"location":"course/2022/lecture-3-troubleshooting-and-testing/#texts","title":"Texts","text":"<ol> <li> <p>Reliable ML Systems talk</p> </li> <li> <p>\"ML Test Score\" paper</p> </li> <li> <p>\"Attack of the Cosmic Rays!\"</p> </li> <li> <p>\"Computers can be understood\"</p> </li> <li> <p>\"Systems that defy detailed understanding\"</p> </li> <li> <p>Testing section from MadeWithML course on MLOps</p> </li> </ol>"},{"location":"course/2022/lecture-4-data-management/","title":"Lecture 4: Data Management","text":"<p>Lecture by Sergey Karayev. Notes by James Le and Vishnu Rachakonda. Published August 29, 2022. Download slides.</p>"},{"location":"course/2022/lecture-4-data-management/#1-introduction","title":"1 - Introduction","text":"<p>One thing people don't quite get as they enter the field of ML is how much of it deals with data - putting together datasets, exploring the data, wrangling the data, etc. The key points of this lecture are:</p> <ol> <li> <p>Spend 10x as much time exploring the data as you would like to.</p> </li> <li> <p>Fixing, adding, and augmenting the data is usually the best way to improve performance.</p> </li> <li> <p>Keep it all simple!</p> </li> </ol>"},{"location":"course/2022/lecture-4-data-management/#2-data-sources","title":"2 - Data Sources","text":"<p>There are many possibilities for the sources of data. You might have images, text files, logs, or database records. In deep learning, you need to get that data into a local filesystem disk next to a GPU. How you send data from the sources to training is different for each project.</p> <ul> <li> <p>With images, you can simply download them from S3.</p> </li> <li> <p>With text files, you need to process them in some distributed way, analyze the data, select a subset, and put that on a local machine.</p> </li> <li> <p>With logs and database records, you can use a data lake to aggregate and process the data.</p> </li> </ul> <p></p> <p>The basics will be the same - a filesystem, object storage, and databases.</p>"},{"location":"course/2022/lecture-4-data-management/#filesystem","title":"Filesystem","text":"<p>The filesystem is a fundamental abstraction. Its fundamental unit is a file - which can be text or binary, is not versioned, and is easily overwritten. The filesystem is usually on a disk connected to your machine - physically connected on-prem, attached in the cloud, or even distributed.</p> <p>The first thing to know about discs is that their speed and bandwidth range - from hard discs to solid-state discs. There are two orders of magnitude differences between the slowest (SATA SSD) and the fastest (NVMe SSD) discs. Below are some latency numbers you should know, with the human-scale numbers in parentheses:</p> <p></p> <p>What formats should the data be stored on the local disc?</p> <ul> <li> <p>If you work with binary data like images and audio, just use the standard formats like JPEG or MP3 that it comes in.</p> </li> <li> <p>If you work with metadata (like labels), tabular data, or text data, then compressed JSON or text files are just fine. Alternatively, Parquet is a table format that is fast, compact, and widely used.</p> </li> </ul>"},{"location":"course/2022/lecture-4-data-management/#object-storage","title":"Object Storage","text":"<p>The object storage is an API over the filesystem. Its fundamental unit is an object, usually in a binary format (an image, a sound file, a text file, etc.). We can build versioning or redundancy into the object storage service. It is not as fast as the local filesystem, but it can be fast enough within the cloud.</p>"},{"location":"course/2022/lecture-4-data-management/#databases","title":"Databases","text":"<p>Databases are persistent, fast, and scalable storage and retrieval of structured data systems. A helpful mental model for this is: all the data that the databases hold is actually in the computer\\'s RAM, but the database software ensures that if the computer gets turned off, everything is safely persisted to disk. If too much data is in the RAM, it scales out to disk in a performant way.</p> <p>You should not store binary data in the database but the object-store URLs instead. Postgres is the right choice most of the time. It is an open-source database that supports unstructured JSON and queries over that JSON. SQLite is also perfectly good for small projects.</p> <p>Most coding projects that deal with collections of objects that reference each other will eventually implement a crappy database. Using a database from the beginning with likely save you time. In fact, most MLOps tools are databases at their core (e.g., W&amp;B is a database of experiments, HuggingFace Hub is a database of models, and Label Studio is a database of labels).</p> <p></p> <p>Data warehouses are stores for online analytical processing (OLAP), as opposed to databases being the data stores for online transaction processing (OLTP). You get data into the data warehouse through a process called ETL (Extract-Transform-Load): Given a number of data sources, you extract the data, transform it into a uniform schema, and load it into the data warehouse. From the warehouse, you can run business intelligence queries. The difference between OLAP and OLTP is that: OLAPs are column-oriented, while OLTPs are row-oriented.</p> <p></p> <p>Data lakes are unstructured aggregations of data from multiple sources. The main difference between them and data warehouses is that data lakes use ELT (Extract-Load-Transform) process: dumping all the data in and transforming them for specific needs later.</p> <p>The big trend is unifying both data lake and data warehouse, so that structured data and unstructured data can live together. The two big platforms for this are Snowflake and Databricks. If you are really into this stuff, \"Designing Data-Intensive Applications\" is a great book that walks through it from first principles.</p>"},{"location":"course/2022/lecture-4-data-management/#3-data-exploration","title":"3 - Data Exploration","text":"<p>To explore the data, you must speak its language, mostly SQL and, increasingly, DataFrame. SQL is the standard interface for structured data, which has existed for decades. Pandas is the main DataFrame in the Python ecosystem that lets you do SQL-like things. Our advice is to become fluent in both to interact with both transactional databases and analytical warehouses and lakes.</p> <p>Pandas is the workhorse of Python data science. You can try DASK DataFrame to parallelize Pandas operations over cores and RAPIDS to do Pandas operations on GPUs.</p>"},{"location":"course/2022/lecture-4-data-management/#4-data-processing","title":"4 - Data Processing","text":"<p>Talking about data processing, it's useful to have a motivational example. Let's say we have to train a photo popularity predictor every night. For each photo, the training data must include:</p> <ol> <li> <p>Metadata (such as posting time, title, and location) that sits in the database.</p> </li> <li> <p>Some features of the user (such as how many times they logged in today) that are needed to be computed from logs.</p> </li> <li> <p>Outputs of photo classifiers (such as content and style) that are needed to run the classifiers.</p> </li> </ol> <p>Our ultimate task is to train the photo predictor model, but we need to output data from the database, compute the logs, and run classifiers to output their predictions. As a result, we have task dependencies. Some tasks can't start until others are finished, so finishing a task should kick off its dependencies.</p> <p>Ideally, dependencies are not always files but also programs and databases. We should be able to spread this work over many machines and execute many dependency graphs all at once.</p> <p></p> <ul> <li> <p>Airflow is a standard scheduler for Python, where it's possible to specify the DAG (directed acyclic graph) of tasks using Python code. The operator in that graph can be SQL operations or Python functions.</p> </li> <li> <p>To distribute these jobs, the workflow manager has a queue for the tasks and manages the workers that pull from them. It will restart jobs if they fail and ping you when the jobs are finished.</p> </li> <li> <p>Prefect and Dagster are contenders to improve and replace Airflow in the long run.</p> </li> </ul> <p>The primary advice here is not to over-engineer things. You can get machines with many CPU cores and a lot of RAM nowadays. For example, UNIX has powerful parallelism, streaming, and highly optimized tools.</p>"},{"location":"course/2022/lecture-4-data-management/#5-feature-store","title":"5 - Feature Store","text":"<p>Let's say your data processing generates artifacts you need for training. How do you make sure that, in production, the trained model sees the same processing taking place (which happened during training)? How do you avoid recomputation during retraining?</p> <p>Feature stores are a solution to this (that you may not need!).</p> <ul> <li> <p>The first mention of feature stores came from this Uber blog post describing their ML platform, Michelangelo. They had an offline training process and an online prediction process, so they built an internal feature store for both processes to be in sync.</p> </li> <li> <p>Tecton is the leading SaaS solution to feature store.</p> </li> <li> <p>Feast is a common open-source option.</p> </li> <li> <p>Featureform is a relatively new option.</p> </li> </ul>"},{"location":"course/2022/lecture-4-data-management/#6-datasets","title":"6 - Datasets","text":"<p>What about datasets specifically made for machine learning?</p> <p>HuggingFace Datasets is a great source of machine learning-ready data. There are 8000+ datasets covering a wide variety of tasks, like computer vision, NLP, etc. The Github-Code dataset on HuggingFace is a good example of how these datasets are well-suited for ML applications. Github-Code can be streamed, is in the modern Apache Parquet format, and doesn't require you to download 1TB+ of data in order to properly work with it. Another sample dataset is RedCaps, which consists of 12M image-text pairs from Reddit.</p> <p></p> <p>Another interesting dataset solution for machine learning is Activeloop. This tool is particularly well equipped to work with data and explore samples without needing to download it.</p>"},{"location":"course/2022/lecture-4-data-management/#7-data-labeling","title":"7 - Data Labeling","text":""},{"location":"course/2022/lecture-4-data-management/#no-labeling-required","title":"No Labeling Required","text":"<p>The first thing to talk about when it comes to labeling data is...maybe we don\\'t have to label data? There are a couple of options here we will cover.</p> <p>Self-supervised learning is a very important idea that allows you to avoid painstakingly labeling all of your data. You can use parts of your data to label other parts of your data. This is very common in NLP right now. This is further covered in the foundation model lecture. The long and short of it is that models can have elements of their data masked (e.g., the end of a sentence can be omitted), and models can use earlier parts of the data to predict the masked parts (e.g., I can learn from the beginning of the sentence and predict the end). This can even be used across modalities (e.g., computer vision and text), as OpenAI CLIP demonstrates.</p> <p></p> <p>Image data augmentation is an almost compulsory technique to adopt, especially for vision tasks. Frameworks like torchvision help with this. In data augmentation, samples are modified (e.g., brightened) without actually changing their core \"meaning.\" Interestingly, augmentation can actually replace labels. SimCLR is a model that demonstrates this - where its learning objective is to maximize agreement between augmented views of the same image and minimize agreement between different images.</p> <p>For other forms of data, there are a couple of augmentation tricks that can be applied. You can delete some cells in tabular data to simulate missing data. In text, there aren't established techniques, but ideas include changing the order of words or deleting words. In speech, you could change the speed, insert pauses, etc.</p> <p>Synthetic data is an underrated idea. You can synthesize data based on your knowledge of the label. For example, you can create receipts if your need is to learn how to recognize receipts from images. This can get very sophisticated and deep, so tread carefully.</p> <p>You can also get creative and ask your users to label data for you. Google Photos, as any user of the app knows, regularly gets users to label images about where people in photos are the same or different.</p> <p></p> <p>This is an example of the data flywheel. Improving the data allows the user to improve the model, which in turn makes their product experience better.</p>"},{"location":"course/2022/lecture-4-data-management/#labeling-solutions","title":"Labeling Solutions","text":"<p>These are all great options for avoiding labeling data. However, you'll usually have to label some data to get started.</p> <p>Labeling has standard annotation features, like bounding boxes, that help capture information properly. Training annotators properly is more important than the particular kind of annotation. Standardizing how annotators approach a complex, opinable task is crucial. Labeling guidelines can help capture the exact right label from an annotator. Quality assurance is key to ensuring annotation and labeling are happening properly.</p> <p>There are a few options for sourcing labor for annotations:</p> <ol> <li> <p>Full-service data labeling vendors offer end-to-end labeling solutions.</p> </li> <li> <p>You can hire and train annotators yourself.</p> </li> <li> <p>You can crowdsource annotation on a platform like Mechanical Turk.</p> </li> </ol> <p>Full-service companies offer a great solution that abstracts the need to build software, manage labor, and perform quality checks. It makes sense to use one. Before settling on one, make sure to dedicate time to vet several. Additionally, label some gold standard data yourself to understand the data yourself and to evaluate contenders. Take calls with several contenders, ask for work samples on your data, and compare them to your own labeling performance.</p> <ul> <li> <p>Scale AI is the dominant data labeling solution. It offers an API that allows you to spin up tasks.</p> </li> <li> <p>Additional contenders include Labelbox and Supervisely.</p> </li> <li> <p>LabelStudio is an open-source solution for performing annotation yourself, with a companion enterprise version. It has a great set of features that allow you to design your interface and even plug-in models for active learning!</p> </li> <li> <p>Diffgram is a competitor to Label Studio.</p> </li> <li> <p>Recent offerings, like Aquarium and Scale Nucleus, have started to help concentrate labeling efforts on parts of the dataset that are most troublesome for models.</p> </li> <li> <p>Snorkel is a dataset management and labeling platform that uses weak supervision, which is a similar concept. You can leverage composable rules (e.g., all sentences that have the term \"amazing\" are positive sentiments) that allow you to quickly label data faster than if you were to treat every data point the same.</p> </li> </ul> <p>In conclusion, try to avoid labeling using techniques like self-supervised learning. If you can't, use labeling software and eventually outsource the work to the right vendor. If you can't afford vendors, consider hiring part-time work rather than crowdsourcing the work to ensure quality.</p>"},{"location":"course/2022/lecture-4-data-management/#8-data-versioning","title":"8 - Data Versioning","text":"<p>Data versioning comes with a spectrum of approaches:</p> <ol> <li> <p>Level 0 is bad. In this case, data just lives on some file system. In these cases, the issue arises because the models are unversioned since their data is unversioned. Models are part code, part data. This will lead to the consequence of being unable to get back to a previous level of performance if need be.</p> </li> <li> <p>You can prevent this event with Level 1, where you snapshot your data each time you train. This somewhat works but is far from ideal.</p> </li> <li> <p>In Level 2, data is versioned like code, as a commingled asset with versioned code. You can use a system like git-lfs that allows you to store large data assets alongside code. This works really well!</p> </li> <li> <p>Level 3 involves specialized solutions for working with large data files, but this may not be needed unless you have a very specific need (i.e., uniquely large or compliance-heavy files).</p> </li> </ol> <p></p> <p>DVC is a great tool for this. DVC helps upload your data asset to a remote storage location every time you commit changes to the data file or trigger a commit; it functions like a fancier git-lfs. It adds features like lineage for data and model artifacts, allowing you to recreate pipelines.</p> <p>Several techniques are associated with privacy-controlled data, like federated learning, differential privacy, and learning on encrypted data. These techniques are still in research, so they aren't quite ready for an FSDL recommendation.</p>"},{"location":"course/2022/lecture-5-deployment/","title":"Lecture 5: Deployment","text":"<p>Lecture by Josh Tobin. Notes by James Le and Vishnu Rachakonda. Published September 5, 2022. Download slides.</p>"},{"location":"course/2022/lecture-5-deployment/#introduction","title":"Introduction","text":"<p>Deploying models is a critical part of making your models good, to begin with. When you only evaluate the model offline, it's easy to miss the more subtle flaws that the model has, where it doesn't actually solve the problem that your users need it to solve. Oftentimes, when we deploy a model for the first time, only then do we really see whether that model is actually doing a good job or not. Unfortunately, for many data scientists and ML engineers, model deployment is an afterthought relative to other techniques we have covered.</p> <p>Much like other parts of the ML lifecycle, we'll focus on deploying a minimum viable model as early as possible, which entails keeping it simple and adding complexity later. Here is the process that this lecture covers:</p> <ul> <li> <p>Build a prototype</p> </li> <li> <p>Separate your model and UI</p> </li> <li> <p>Learn the tricks to scale</p> </li> <li> <p>Consider moving your model to the edge when you really need to go fast</p> </li> </ul>"},{"location":"course/2022/lecture-5-deployment/#1-build-a-prototype-to-interact-with","title":"1 - Build a Prototype To Interact With","text":"<p>There are many great tools for building model prototypes. HuggingFace has some tools built into its playground. They have also recently acquired a startup called Gradio, which makes it easy to wrap a small UI around the model. Streamlit is another good option with a bit more flexibility.</p> <p></p> <p>Here are some best practices for prototype deployment:</p> <ol> <li> <p>Have a basic UI: The goal at this stage is to play around with the model and collect feedback from other folks. Gradio and Streamlit are your friends here - often as easy as adding a couple of lines of code to create a simple interface for the model.</p> </li> <li> <p>Put it behind a web URL: An URL is easier to share. Furthermore, you will start thinking about the tradeoffs you'll be making when dealing with more complex deployment schemes. There are cloud versions of Streamlit and HuggingFace for this.</p> </li> <li> <p>Do not stress it too much: You should not take more than a day to build a prototype.</p> </li> </ol> <p>A model prototype won't be your end solution to deploy. Firstly, a prototype has limited frontend flexibility, so eventually, you want to be able to build a fully custom UI for the model. Secondly, a prototype does not scale to many concurrent requests. Once you start having users, you'll hit the scaling limits quickly.</p> <p></p> <p>Above is an abstract diagram of how your application might look. The client is your user's device that interacts with your application. This device can be a browser, a vehicle, or a mobile phone. This device calls over a network to a server. The server talks to a database (where data is stored), used to power the application.</p> <p></p> <p>There are different ways of structuring your application to fit an ML model inside. The prototype approach mentioned in the beginning fits into the model-in-service approach - where your hosted web server has a packaged version of the model sitting inside it. This pattern has pros and cons.</p> <p>The biggest pro is that if you are doing something complex, you get to reuse your existing infrastructure. It does not require you as a model developer to set up new things from scratch.</p> <p>However, there is a number of pronounced cons:</p> <ol> <li> <p>Your web server may be written in a different language, so getting your model into that language can be difficult.</p> </li> <li> <p>Models may change more frequently than server code (especially early in the lifecycle of building your model). If you have a well-established application and a nascent model, you do not want to redeploy the entire application every time that you make an update to the model (sometimes multiple updates per day).</p> </li> <li> <p>If you have a large model to run inference on, you'll have to load that model on your web server. Large models can eat into the resources for your web server. That might affect the user experience for people using that web server, even if they are not interacting with the model.</p> </li> <li> <p>Server hardware is generally not optimized for ML workloads. In particular, you rarely will have a GPU on these devices.</p> </li> <li> <p>Your model and application may have different scaling properties, so you might want to be able to scale them differently.</p> </li> </ol>"},{"location":"course/2022/lecture-5-deployment/#2-separate-your-model-from-your-ui","title":"2 - Separate Your Model From Your UI","text":""},{"location":"course/2022/lecture-5-deployment/#21-batch-prediction","title":"2.1 - Batch Prediction","text":"<p>The first pattern to pull your model from your UI is called batch prediction. You get new data in and run your model on each data point. Then, you save the results of each model inference into a database. This can work well under some circumstances. For example, if there are not a lot of potential inputs to the model, you can re-run your model on some frequency (every hour, every day, or every week). You can have reasonably fresh predictions to return to those users that are stored in your database. Examples of these problems include the early stages of building recommender systems and internal-facing tools like marketing automation.</p> <p>To run models on a schedule, you can leverage the data processing and workflow tools mentioned in our previous lecture on data management. You need to re-run data processing, load the model, run predictions, and store those predictions in your database. This is exactly a Directed Acyclic Graph workflow of data operations that tools like Dagster, Airflow, or Prefect are designed to solve. It's worth noting that there are also tools like Metaflow that are designed more for ML or data science use cases that might be potentially even an easier way to get started.</p> <p>Let's visit the pros and cons of this batch prediction pattern. Starting with the pros:</p> <ol> <li> <p>Batch prediction is simple to implement since it reuses existing batch processing tools that you may already be using for training your model.</p> </li> <li> <p>It scales very easily because databases have been engineered for decades for such a purpose.</p> </li> <li> <p>Even though it looks like a simple pattern, it has been used in production by large-scale production systems for years. This is a tried-and-true pattern you can run and be confident that it'll work well.</p> </li> <li> <p>It is fast to retrieve the prediction since the database is designed for the end application to interact with.</p> </li> </ol> <p>Switching to the cons:</p> <ol> <li> <p>Batch prediction doesn't scale to complex input types. For instance, if the universe of inputs is too large to enumerate every single time you need to update your predictions, this won't work.</p> </li> <li> <p>Users won't be getting the most up-to-date predictions from your model. If the feature that goes into your model changes every hour, minute, or subsecond, but you only run your batch prediction job every day, the predictions your users see might be slightly stale.</p> </li> <li> <p>Models frequently become \"stale.\" If your batch jobs fail for some reason, it can be hard to detect these problems.</p> </li> </ol>"},{"location":"course/2022/lecture-5-deployment/#22-model-as-service","title":"2.2 - Model-as-Service","text":"<p>The second pattern is called model-as-service: we run the model online as its own service. The service is going to interact with the backend or the client itself by making requests to the model service and receiving responses back.</p> <p></p> <p>The pros of this pattern are:</p> <ol> <li> <p>Dependability - model bugs are less likely to crash the web application.</p> </li> <li> <p>Scalability - you can choose optimal hardware for the model and scale it appropriately.</p> </li> <li> <p>Flexibility - you can easily reuse a model across multiple applications.</p> </li> </ol> <p>The cons of this pattern are:</p> <ol> <li> <p>Since this is a separate service, you add a network call when your server or client interacts with the model. That can add latency to your application.</p> </li> <li> <p>It also adds infrastructural complexity because you are on the hook for hosting and managing a separate service.</p> </li> </ol> <p>Even with these cons, the model-as-service pattern is still a sweet spot for most ML-powered products since you really need to be able to scale independently of the application in most complex use cases. We'll walk through the basic components of building your model service - including REST APIs, dependency management, performance optimization, horizontal scaling, rollout, and managed options.</p>"},{"location":"course/2022/lecture-5-deployment/#rest-apis","title":"REST APIs","text":"<p>Rest APIs serve predictions in response to canonically-formatted HTTP requests. There are other alternative protocols to interact with a service that you host on your infrastructures, such as GRPC (used in TensorFlow Serving) and GraphQL (common in web development but not terribly relevant to model services).</p> <p></p> <p>Unfortunately, there is currently no standard for formatting requests and responses for REST API calls.</p> <ol> <li> <p>Google Cloud expects a batch of inputs structured as a list called \"instances\" (with keys and values).</p> </li> <li> <p>Azure expects a list of things called \"data\", where the data structure itself depends on what your model architecture is.</p> </li> <li> <p>AWS Sagemaker expects instances that are formatted differently than they are in Google Cloud.</p> </li> </ol> <p>Our aspiration for the future is to move toward a standard interface for making REST API calls for ML services. Since the types of data that you might send to these services are constrained, we should be able to develop a standard as an industry.</p>"},{"location":"course/2022/lecture-5-deployment/#dependency-management","title":"Dependency Management","text":"<p>Model predictions depend on code, model weights, and dependencies. In order for your model to make a correct prediction, all of these dependencies need to be present on your web server. Unfortunately, dependencies are a notorious cause of trouble as it is hard to ensure consistency between your development environment and your server. It is also hard to update since even changing a TensorFlow version can change your model.</p> <p>At a high level, there are two strategies for managing dependencies:</p> <ol> <li> <p>Constrain the dependencies for your model by saving your model in an agnostic format that can be run anywhere.</p> </li> <li> <p>Use containers to constrain the entire inference program.</p> </li> </ol> <p></p>"},{"location":"course/2022/lecture-5-deployment/#constraining-model-dependencies","title":"Constraining Model Dependencies","text":"<p>The primary way to constrain the dependencies of just your model is through a library called ONNX - the Open Neural Network Exchange. The goal of ONNX is to be an interoperability standard for ML models. The promise is that you can define a neural network in any language and run it consistently anywhere. The reality is that since the underlying libraries used to build these models change quickly, there are often bugs in the translation layer, which creates even more problems to solve for you. Additionally, ONNX doesn't deal with non-library code such as feature transformations.</p>"},{"location":"course/2022/lecture-5-deployment/#containers","title":"Containers","text":"<p>To understand how to manage dependencies with containers, we need to understand the differences between Docker and Virtual Machines, how Docker images are built via Docker files and constructed via layers, the ecosystem around Docker, and specific wrappers around Docker that you can use for ML.</p> <p></p> <p>In a virtual machine, you package up the entire operating system (OS) as well as the libraries and applications that are built on top of that OS. A virtual machine tends to be very heavyweight because the OS itself has a lot of code and is expensive to run. A container such as Docker removes that need by packaging the libraries and applications together. A Docker engine that runs on top of your OS knows how to virtualize the OS and run the libraries/applications.</p> <p>By virtue of being lightweight, Docker is used differently than how Virtual Machines were used. A common pattern is to spin up a new Docker container for every discrete task. For example, a web application might have four containers: a web server, a database, a job queue, and a worker. These containers are run together as part of an orchestration system.</p> <p></p> <p>Docker containers are created from Docker files. Each Docker file runs a sequence of steps to define the environment where you will run your code. Docker also allows you to build, store, and pull Docker containers from a Docker Hub that is hosted on some other servers or your cloud. You can experiment with a code environment that is on your local machine but will be identical to the environment you deploy on your server.</p> <p>Docker is separated into three different components:</p> <ol> <li> <p>The client is where you'll be running on your laptop to build an image from a Dockerfile that you define locally using some commands.</p> </li> <li> <p>These commands are executed by a Docker Host, which can run on either your laptop or your server (with more storage or more performance).</p> </li> <li> <p>That Docker Host talks to a registry - which is where all the containers you might want to access are stored.</p> </li> </ol> <p></p> <p>With this separation of concerns, you are not limited by the amount of compute and storage you have on your laptop to build, pull, and run Docker images. You are also not limited by what you have access to on your Docker Host to decide which images to run.</p> <p>In fact, there is a powerful ecosystem of Docker images that are available on different public Docker Hubs. You can easily find these images, modify them, and contribute them back to the Hubs. It's easy to store private images in the same place as well. Because of this community and the lightweight nature of Docker, it has become incredibly popular in recent years and is ubiquitous at this point.</p> <p>There is a bit of a learning curve to Docker. For ML, there are a few open-source packages designed to simplify this: Cog, BentoML, and Truss. They are built by different model hosting providers that are designed to work well with their model hosting service but also just package your model and all of its dependencies in a standard Docker container format.</p> <p></p> <p>These packages have two primary components: The first one is a standard way of defining your prediction service. The second one is a YAML file that defines the other dependencies and package versions that will go into the Docker container running on your laptop or remotely.</p> <p>If you want to have the advantages of using Docker for making your ML models reproducible but do not want to go through the learning curve of learning Docker, it's worth checking out these three libraries.</p>"},{"location":"course/2022/lecture-5-deployment/#performance-optimization","title":"Performance Optimization","text":"<p>What about performance monitoring?</p> <p>In this section, we focus on ways to improve the performance of your models, but we spend less time on how exactly that performance is monitored, which is a challenge in its own right.</p> <p>Luckily, one of the student projects for the 2022 cohort, Full Stack Stable Diffusion, took up that challenge and combined NVIDIA's Triton Inference Server, the Prometheus monitoring tool, and the Grafana analytics dashboarding tool to monitor a robust, scalable, and observable deployment of Stable Diffusion models.</p> <p>Check out the repo on GitHub here if you want to see a worked example of a fully-monitored DL-powered application.</p> <p>To make model inference on your machine more efficient, we need to discuss GPU, concurrency, model distillation, quantization, caching, batching, sharing the GPU, and libraries that automate these tasks for you.</p>"},{"location":"course/2022/lecture-5-deployment/#gpu-or-no-gpu","title":"GPU or no GPU?","text":"<p>There are some advantages to hosting your model on a GPU:</p> <ol> <li> <p>It's probably the same hardware you train your model on, to begin with. That can eliminate any lost-in-translation issues.</p> </li> <li> <p>As your model gets big and your techniques get advanced, your traffic gets large. GPUs provide high throughput to deal with that.</p> </li> </ol> <p>However, GPUs introduce a lot of complexity:</p> <ol> <li> <p>They are more complex to set up.</p> </li> <li> <p>They are more expensive.</p> </li> </ol> <p>As a result, just because your model is trained on a GPU does not mean that you need to actually host it on a GPU in order for it to work. In the early version of your model, hosting it on a CPU should suffice. In fact, it's possible to get high throughput from CPU inference at a low cost by using some other techniques.</p>"},{"location":"course/2022/lecture-5-deployment/#concurrency","title":"Concurrency","text":"<p>With concurrency, multiple copies of the model run in parallel on different CPUs or cores on a single host machine. To do this, you need to be careful about thread tuning. There's a great Roblox presentation on how they scaled BERT to serve a billion daily requests, just using CPUs.</p>"},{"location":"course/2022/lecture-5-deployment/#model-distillation","title":"Model Distillation","text":"<p>With model distillation, once you have a large model that you've trained, you can train a smaller model that imitates the behavior of your larger one. This entails taking the knowledge that your larger model learned and compressing that knowledge into a much smaller model that you may not have trained to the same degree of performance from scratch. There are several model distillation techniques pointed out in this blog post. They can be finicky to do by yourself and are infrequently used in practice. An exception is distilled versions of popular models (such as DistilBERT).</p>"},{"location":"course/2022/lecture-5-deployment/#quantization","title":"Quantization","text":"<p>With quantization, you execute some or potentially all of the operations in your model in a lower fidelity representation of the numbers that you are doing the math. These representations can be 16-bit floating point numbers or 8-bit integers. This introduces some tradeoffs with accuracy, but it's worth making these tradeoffs because the accuracy you lose is limited relative to the performance you gain.</p> <p>The recommended path is to use built-in quantization methods in PyTorch and TensorFlow. More specifically, HuggingFace Optimum is a good choice if you have already been using HuggingFace's pre-trained models. You can also run quantization-aware training, which often results in higher accuracy.</p> <p></p>"},{"location":"course/2022/lecture-5-deployment/#caching","title":"Caching","text":"<p>With caching, you realize that for some ML models, some inputs are more common than others. Instead of always calling the model every time a user makes a request, let's store the common requests in a cache. Then, let's check that cache before running an expensive operation. Caching techniques can get fancy, but the basic way of doing this is to use functools library in Python.</p> <p></p>"},{"location":"course/2022/lecture-5-deployment/#batching","title":"Batching","text":"<p>With batching, you take advantage of the fact that ML models often achieve a higher throughput when doing prediction in parallel, especially in a GPU. To accomplish this, you need to gather predictions until you have a batch, run those predictions, and return them to your user. You want to tune the batch size that deals optimally with the latency-throughput tradeoff. You also need to have a way to shortcut the process if latency becomes too long. Batching is complicated to implement, so you probably do not want to implement this yourself.</p>"},{"location":"course/2022/lecture-5-deployment/#sharing-the-gpu","title":"Sharing the GPU","text":"<p>Your model may not take up all of the GPU memory with your inference batch size. Why don't you run multiple models on the same GPU? This is a place where you want to use a model serving solution that supports GPU sharing out of the box.</p>"},{"location":"course/2022/lecture-5-deployment/#libraries","title":"Libraries","text":"<p>There are offerings from TensorFlow, PyTorch, and third-party tools from NVIDIA and Anyscale. NVIDIA's choice is probably the most powerful but can be difficult to get started with. Starting with Anyscale's Ray Serve may be an easier way to get started.</p> <p></p>"},{"location":"course/2022/lecture-5-deployment/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>If you're going to scale up to a large number of users interacting with your model, it's not going to be enough to get the most efficiency out of one server. At some point, you'll need to scale horizontally to have traffic going to multiple copies of your model running on different servers. This is called horizontal scaling. This technique involves taking traffic that would usually go to a single machine and splits across multiple machines.</p> <p>Each machine has a copy of the service, and a tool called a load balancer distributes traffic to each machine. In practice, there are two ways to do this: with either container orchestration (e.g. Kubernetes) or serverless (e.g. AWS Lambda).</p>"},{"location":"course/2022/lecture-5-deployment/#container-orchestration","title":"Container Orchestration","text":"<p>In container orchestration, we use Kubernetes to help manage containerized applications (in Docker containers, for example) and run them across machines.</p> <p></p> <p>Kubernetes is quite interesting, but it's probably overkilled to learn too much about it if your only goal is to deploy machine learning models. There are a number of frameworks that make it easiest to deploy ML models with Kubernetes, including Kubeflow, Seldon, etc.</p>"},{"location":"course/2022/lecture-5-deployment/#serverless","title":"Serverless","text":"<p>If Kubernetes isn't the path for you (e.g. you don't want to have to worry about infrastructure at all), serverless is another option for deploying models. In this paradigm, app code and dependencies are packaged into .zip files or Docker containers with a single entry point function, which is a single function (e.g. model.predict()) that will be run repeatedly. This package is then deployed to a service like AWS Lambda, which almost totally manages the infrastructure required to run the code based on the input. Scaling to thousands of requests and across multiple machines is taken care of by these services. In return, you pay for the compute time that you consume.</p> <p>Since model services tend to run discretely and not continuously (like a web server), serverless is a great fit for machine learning deployment.</p> <p></p> <p>Start with serverless! It's well worth the time saved in managing infrastructure and dealing with associated challenges. There are still some problems you should be aware of though.</p> <ol> <li> <p>First, the size of the actual deployment package that can be sent to a serverless service tends to be limited, which makes large models impossible to run.</p> </li> <li> <p>Second, there is also a cold start problem. If there is no traffic being sent to the service in question, the service will \"wind down\" to zero compute use, at which point it takes time to start again. This lag in starting up upon the first request to the serverless service is known as the \"cold start\" time. This can take seconds or even minutes.</p> </li> <li> <p>Third, it can be hard to actually build solid software engineering concepts, like pipelines, with serverless. Pipelines enable rapid iteration, while serverless offerings often do not have the tools to support rapid, automated changes to code of the kind pipelines are designed to do.</p> </li> <li> <p>Fourth, state management and deployment tooling are related challenges here.</p> </li> <li> <p>Finally, most serverless functions are CPU only and have limited execution time. If you need GPUs for inference, serverless might not be for you quite yet. There are, however, new offerings like Banana and Pipeline that are seeking to solve this problem of serverless GPU inference!</p> </li> </ol>"},{"location":"course/2022/lecture-5-deployment/#model-rollouts","title":"Model Rollouts","text":"<p>If serving is how you turn a model into something that can respond to requests, rollouts are how you manage and update these services. To be able to make updates effectively, you should be able to do the following:</p> <ol> <li> <p>Roll out gradually: You may want to incrementally send traffic to a new model rather than the entirety.</p> </li> <li> <p>Roll back instantly: You may want to immediately pull back a model that is performing poorly.</p> </li> <li> <p>Split traffic between versions: You may want to test differences between models and therefore send some traffic to each.</p> </li> <li> <p>Deploy pipelines of models: Finally, you may want to have entire pipeline flows that ensure the delivery of a model.</p> </li> </ol> <p>Building these capabilities is a reasonably challenging infrastructure problem that is beyond the scope of this course. In short, managed services are a good option for this that we'll now discuss!</p>"},{"location":"course/2022/lecture-5-deployment/#managed-options","title":"Managed Options","text":"<p>All of the major cloud providers offer their managed service options for model deployment. There are a number of startups offering solutions as well, like BentoML or Banana.</p> <p></p> <p>The most popular managed service is AWS Sagemaker. Working with Sagemaker is easier if your model is already in a common format like a Huggingface class or a SciKit-Learn model. Sagemaker has convenient wrappers for such scenarios. Sagemaker once had a reputation for being a difficult service to work with, but this is much less the case for the clear-cut use case of model inference. Sagemaker, however, does have real drawbacks around ease of use for custom models and around cost. In fact, Sagemaker instances tend to be 50-100% more expensive than EC2.</p>"},{"location":"course/2022/lecture-5-deployment/#23-takeaways","title":"2.3 - Takeaways","text":"<p>To summarize this section, remember the following:</p> <ol> <li> <p>You probably don't need GPU inference, which is hard to access and maintain. Scaling CPUs horizontally or using serverless can compensate.</p> </li> <li> <p>Serverless is probably the way to go!</p> </li> <li> <p>Sagemaker is a great way to get started for the AWS user, but it can get quite expensive.</p> </li> <li> <p>Don't try to do your own GPU inference; use existing tools like TFServing or Triton to save time.</p> </li> <li> <p>Watch out for new startups focused on GPU inference.</p> </li> </ol>"},{"location":"course/2022/lecture-5-deployment/#3-move-to-the-edge","title":"3 - Move to the Edge?","text":"<p>Let's now consider the case of moving models out of web service and all the way to the \"edge\", or wholly on-device. Some reasons you may need to consider this include a lack of reliable internet access for users or strict data security requirements.</p> <p>If such hard and fast requirements aren't in place, you'll need to take into account the tradeoff between accuracy and latency and how this can affect the end-user experience. Put simply, if you have exhausted all options to reduce model prediction time (a component of latency), consider edge deployment.</p> <p></p> <p>Edge deployment adds considerable complexity, so it should be considered carefully before being selected as an option. In edge prediction, model weights are directly loaded on our client device after being sent via a server (shown above), and the model is loaded and interacted with directly on the device.</p> <p>This approach has compelling pros and cons:</p> <ol> <li> <p>Some pros to particularly call out are the latency advantages that come without the need for a network and the ability to scale for \"free,\" or the simple fact that you don't need to worry about the challenges of running a web service if all inference is done locally.</p> </li> <li> <p>Some specific cons to call out are the often limited hardware and software resources available to run machine learning models on edge, as well as the challenge of updating models since users control this process more than you do as the model author.</p> </li> </ol>"},{"location":"course/2022/lecture-5-deployment/#31-frameworks","title":"3.1 - Frameworks","text":"<p>Picking the right framework to do edge deployment depends both on how you train your model and what the target device you want to deploy it on is.</p> <ul> <li> <p>TensorRT: If you're deploying to NVIDIA, this is the choice to go with.</p> </li> <li> <p>MLKit and CoreML: For phone-based deployment on either Android or iPhone, go with MLKit for the former and CoreML for the latter.</p> </li> <li> <p>PyTorch Mobile: For compatibility with both iOS and Android, use PyTorch Mobile.</p> </li> <li> <p>TFLite: A great choice for using TensorFlow in a variety of settings, not just on a phone or a common device.</p> </li> <li> <p>TensorFlow JS: The preferred framework for deploying machine learning in the browser.</p> </li> <li> <p>Apache TVM: A library agnostic, target device agnostic option. This is the choice for anyone trying to deploy to as diverse a number of settings as possible.</p> </li> </ul> <p>Keep paying attention to this space! There are a lot of startups like MLIR, OctoML, TinyML, and Modular that are aiming to solve some of these problems.</p>"},{"location":"course/2022/lecture-5-deployment/#32-efficiency","title":"3.2 - Efficiency","text":"<p>No software can help run edge-deployed models that are simply too large; model efficiency is important for edge deployment! We previously discussed quantization and distillation as options for model efficiency. However, there are also network architectures specifically designed to work better in edge settings like MobileNets. MobileNets replace the more expensive computations typical of server-run models with simpler computations and achieve acceptable performance oftentimes.</p> <p></p> <p>MobileNets are a great tool for model deployments and are a great case study in model efficiency. Another similarly great case study is DistillBERT.</p> <p></p>"},{"location":"course/2022/lecture-5-deployment/#33-mindsets","title":"3.3 - Mindsets","text":"<p>As we wrap up this lecture, keep in mind the following mindsets as you consider edge deployment:</p> <ol> <li> <p>Start with the edge requirement, not the architecture choice. It's easy to pick a high-performing model architecture, only to then find it impossible to run on the edge device. Avoid this scenario at all costs! Tricks like quantization can account for up to 10x improvement, but not much more.</p> </li> <li> <p>Once you have a model that works on the edge, you can iterate locally without too much additional re-deployment. In this case, make sure to add metrics around the model size and edge performance to your experiment tracking.</p> </li> <li> <p>Treat tuning the model as an additional risk and test accordingly. With the immaturity of edge deployment frameworks, it's crucial to be especially careful when testing your model on the exact hardware you'll be deploying on.</p> </li> <li> <p>Make sure to have fallbacks! Models are finicky and prone to unpredictable behavior. In edge cases, it's especially important to have easily available fallback options for models that aren't working.</p> </li> </ol>"},{"location":"course/2022/lecture-5-deployment/#34-conclusion","title":"3.4 - Conclusion","text":"<p>To summarize this section:</p> <ol> <li> <p>Web deployment is easier, so use edge deployment only if you need to.</p> </li> <li> <p>Choose your framework to match the available hardware and corresponding mobile frameworks, or try Apache TVM to be more flexible.</p> </li> <li> <p>Start considering hardware constraints at the beginning of the project and choose architectures accordingly.</p> </li> </ol>"},{"location":"course/2022/lecture-6-continual-learning/","title":"Lecture 6: Continual Learning","text":"<p>Lecture by Josh Tobin. Notes by James Le and Vishnu Rachakonda. Published September 12, 2022. Download slides.</p>"},{"location":"course/2022/lecture-6-continual-learning/#1-overview","title":"1 - Overview","text":"<p>The core justification for continual learning is that, unlike in academia, we never deal with static data distributions in the real world. The implication is that: if you want to use ML in production and build ML-powered products, you need to think about your goal of building a continual learning system, not just a static model.</p> <p>Recalling the data flywheel that we've described in this class before: as you get more users, those users bring more data. You can use the data to make a better model. A better model helps you attract even more users and build a better model over time. Andrej Karpathy described the most optimistic version of it as \"Operation Vacation\" - if we make our continual learning system good enough, it'll get better on its own over time, and ML engineers can just go on vacation.</p> <p></p> <p>The reality is quite different. Initially, we gather, clean, and label some data. We train a model on that data. Then we evaluate the model and loop back to training the model to improve it based on our evaluations. Finally, we get a minimum viable model and deploy it into production.</p> <p></p> <p>The problem begins after we deploy the model: we generally don't have a great way of measuring how our models are actually performing in production. Often, we just spot-check some predictions to see if they are doing what they are supposed to do. If it seems to work, then it's great. We move on to work on other things.</p> <p></p> <p>Unfortunately, the ML engineer is probably not the one who discovers the problems, to begin with. Some business user or product manager gets complaints from users about a dipping metric, which leads to an investigation. This already costs the company money because the product and business teams must investigate the problem.</p> <p></p> <p>Eventually, they point back to the ML engineer and the model he is responsible for. At this point, we are stuck on doing ad-hoc analyses because we don't know what caused the model failure. Eventually, we can run a bunch of SQL queries and paste together some Jupyter notebooks to figure out what the problem is. If we are lucky, we can run an A/B test. If the test looks good, we'll deploy it into production. Then, we are back to where we started - not getting ongoing feedback about how the model is doing in production.</p> <p>The upshot is that continual learning is the least well-understood part of the production ML lifecycle. Very few companies are doing this in production today. This lecture focuses on how to improve different steps of the continual learning process, pointers to learn about each step, and recommendations for doing it pragmatically and adopting it gradually.</p>"},{"location":"course/2022/lecture-6-continual-learning/#2-how-to-think-about-continual-learning","title":"2 - How to Think About Continual Learning","text":"<p>Our opinionated view about continual learning is training a sequence of models that can adapt to a continuous stream of data that comes into production. You can think about continual learning as an outer loop in your training process. On one end of the loop is your application, which consists of a model and some other code that users interact with that application by submitting requests, getting predictions back, and submitting feedback about how well the model did at providing that prediction.</p> <p>The continual learning loop starts with logging, which is how we get all the data into the loop. Then we have data curation, triggers for the retraining process, dataset formation to pick the data to retrain on, the training process itself, and offline testing to validate whether the retrained model is good enough to go into production. After the model is deployed, we have online testing, and that brings the next version of the model into production, where we can start the loop all over.</p> <p>Each of these stages passes the output to the next step. Output is defined by a set of rules. These rules combine to form our retraining strategy. Let's discuss what the retraining strategy looks like for each stage:</p> <p></p> <p>At the logging stage, the key question answered by the retraining strategy is what data should we store? At the end of this stage, we have an \"infinite stream\" of potentially unlabeled data coming from production and can be used for downstream analysis.</p> <p></p> <p>At the curation stage, the key rules we need to define are what data from that infinite stream will we prioritize for labeling and potential retraining? At the end of this stage, we have a reservoir of candidate training points that have labels and are fully ready to be fed back into a training process.</p> <p></p> <p>At the retraining trigger stage, the key question is when should we retrain? The output of this stage is a signal to kick off a retraining job.</p> <p></p> <p>At the dataset formation stage, the key rules we need to define are from this entire reservoir of data, what specific subset of that data are we using to train on for this particular training job? The output of this stage is a view into that reservoir or training data that specifies the exact data points to be used for the training job.</p> <p></p> <p>At the offline testing stage, the key rule we need to define is what \"good enough\" looks like for all stakeholders. The output of this stage is equivalent to a \"pull request\" report card for your model with a clear sign-off process. Once you are signed off, the new model will roll out into production.</p> <p></p> <p>Finally, at the deployment and online testing stage, the key rule to define is how do we know if this deployment was successful? The output of this stage is a signal to roll this model out fully to all of your users.</p> <p>In an idealized world, from an ML engineer's perspective, once the model is deployed, the first version of the model is to not retrain the model directly. Instead, we want the model to sit on top of the retraining strategy and try to improve that strategy over time. Rather than training models daily, we look at metrics about how well the strategy is working and how well it's solving the task of improving our model over time in response to changes in the world. The input that we provide is by tuning the strategy to do a better job of solving that task.</p> <p>For most ML engineers, our jobs don't feel like that at a high level. Our retraining strategy is just retraining models whenever we feel like it. We can get good results from ad-hoc retraining, but when you start getting consistent results and no one is actively working on the model day to day anymore, then it's worth starting to add some automation. Alternatively, if you find yourself needing to retrain the model more than once a week (or even more frequently than that) to deal with changing results in the real world, then it's worth investing in automation just to save yourself.</p>"},{"location":"course/2022/lecture-6-continual-learning/#3-periodic-retraining","title":"3 - Periodic Retraining","text":"<p>The first baseline retraining strategy that you should consider after you move on from ad-hoc is just periodic retraining:</p> <ol> <li> <p>At the logging stage, we simply log everything.</p> </li> <li> <p>At the curation stage, we sample uniformly at random from the data that we've logged up until we get the maximum number of data points that we are able to handle. Then we label them using some automated tools.</p> </li> <li> <p>Our retraining trigger will just be periodic.</p> </li> <li> <p>We train once a week, but we do it on the last month's data, for example.</p> </li> <li> <p>Then we compute the test set accuracy after each training, set a threshold on that, or more likely manual review the results each time, and spot-check some of the predictions.</p> </li> <li> <p>When we deploy the model, we do spot evaluations of that deployed model on a few individual predictions to make sure things look healthy.</p> </li> </ol> <p></p> <p>Periodic retraining won't work in every circumstance. There are several failure modes:</p> <ol> <li> <p>The first category is that you have more data than you can log or label. If you have a high volume of data, you might need to be more careful about what data to sample and enrich, particularly if that data comes from a long-tail distribution - where you have edge cases that your model needs to perform well on, but those edge cases might not be caught by just doing standard uniform sampling. Or if that data is expensive to label like in a human-in-the-loop scenario - where you need custom labeling rules or labeling is a part of the product. In either of those cases, you need to be more careful about what subset of your data you log and enrich to be used down the road.</p> </li> <li> <p>The second category has to do with managing the cost of retraining. If your model is expensive to retrain, retraining it periodically is not going to be the most cost-efficient way to go, especially if you do it on a rolling window of data every single time. You will leave a lot of performance on the table by not retraining more frequently. You can partially solve this by increasing the retraining frequency, but this will increase the costs even further.</p> </li> <li> <p>The final failure mode is situations where you have a high cost of bad predictions. Every time you retrain your model, it introduces risk, which comes from the fact that the data you're training the model on might be bad in some way. It might be corrupted, might have been attacked by an adversary, or might not be representative anymore of all the cases that your model needs to perform well on. The more frequently you retrain and the more sensitive you are to model failures, the more thoughtful you need to be about careful model evaluation such that you are not unduly taking on too much risk from frequent retraining.</p> </li> </ol>"},{"location":"course/2022/lecture-6-continual-learning/#4-iterating-on-your-retraining-strategy","title":"4 - Iterating On Your Retraining Strategy","text":"<p>The main takeaway from this section is that we will use monitoring and observability to determine what changes we want to make to our retraining strategy.</p> <ol> <li> <p>We'll do that by monitoring just the metrics that actually that matter and using all other metrics for debugging.</p> </li> <li> <p>When we debug an issue with our model, that will lead to potentially retraining our model. But more broadly than that, we can think of it as a change to the retraining strategy - changing our retraining triggers, our offline tests, our sampling strategies, the metrics for observability, etc.</p> </li> <li> <p>As we get more confident in our monitoring, we can introduce more automation to our system.</p> </li> </ol> <p>There are no real standards or best practices on model monitoring yet. The main principles we'll follow are: (1) We'll focus on monitoring what matters and what breaks empirically; and (2) We'll compute other signals too but use them for observability and debugging.</p> <p></p> <p>What does it mean to monitor a model in production? We think about it as: You have some metric to assess the model quality (i.e, accuracy) and a time series of how that metric changes over time. The question you try to answer is: Is this bad or okay? Do you need to pay attention to this degradation or not?</p> <p>The questions we'll need to answer are:</p> <ol> <li> <p>What metrics should we be looking at when we are monitoring?</p> </li> <li> <p>How can we tell if those metrics are bad and warrant an intervention?</p> </li> <li> <p>What are the tools that help us with this process?</p> </li> </ol>"},{"location":"course/2022/lecture-6-continual-learning/#what-metrics-to-monitor","title":"What Metrics to Monitor","text":"<p>Choosing the right metric to monitor is probably the most important part of this process. Below you can find different types of metrics ranked in order of how valuable they are.</p> <p></p>"},{"location":"course/2022/lecture-6-continual-learning/#outcomes-and-feedback-from-users","title":"Outcomes and Feedback From Users","text":"<p>The most valuable one to look at is outcome data or feedback from your users. Unfortunately, there are no one-size-fits-all ways to do this because it depends a lot on the specifics of the product you are building. This is more of a product management question of how to design your product in a way that you can capture feedback from your users as part of the product experience.</p>"},{"location":"course/2022/lecture-6-continual-learning/#model-performance-metrics","title":"Model Performance Metrics","text":"<p>The next most valuable signal to look at is model performance metrics. These are offline metrics such as accuracy. This is less useful than user feedback because of loss mismatch. A common experience many ML practitioners have is that improving model performance leads to the same or worse outcome. There's very little excuse for not doing this. To some degree, you can label some production data each day by setting up an on-call rotation or throwing a labeling party. These practices will give you some sense of how your model performance trends over time.</p> <p></p>"},{"location":"course/2022/lecture-6-continual-learning/#proxy-metrics","title":"Proxy Metrics","text":"<p>The next best thing to look at is proxy metrics, which are correlated with bad model performance. These are mostly domain-specific. For example, if you are building text generation with a language model, two examples would be repetitive and toxic outputs. If you are building a recommendation system, an example would be the share of personalized responses. Edge cases can be good proxy metrics. If there are certain problems you know that you have with your model, if those increase in prevalence, that might mean your model is not doing very well.</p> <p>There's an academic direction that aims at being able to take any metric you care about and approximate it on previously unseen data. How well do we think our model is doing on this new data? Which would make these proxy metrics a lot more practically useful? There are a number of different approaches here: from training an auxiliary model to predict how well your main model might do on this offline data, to using heuristics and human-in-the-loop methods.</p> <p></p> <p>An unfortunate result from this literature is that it's not possible to have a single method you use in all circumstances to approximate how your model is doing on out-of-distribution data. Let's say you are looking at the input data to predict how the model will perform on those input points. Then the label distribution changes. As a result, you won't be able to take into account that change in your approximate metric.</p>"},{"location":"course/2022/lecture-6-continual-learning/#data-quality","title":"Data Quality","text":"<p>The next signal to look at is data quality. Data quality testing is a set of rules you apply to measure the quality of your data. This deals with questions such as: How well does a piece of information reflect reality? Does it fulfill your expectations of what's comprehensive? Is your information available when you need it? Some common examples include checking whether the data has the right schema, the data is in the expected range, and the number of records is not anomalous.</p> <p></p> <p>This is useful because data problems tend to be the most common issue with ML models in practice. In a Google report which covered 15 years of different pipeline outages with a particular ML model, most of the outages that happened with that model were distributed systems problems, commonly data problems.</p>"},{"location":"course/2022/lecture-6-continual-learning/#distribution-drift","title":"Distribution Drift","text":""},{"location":"course/2022/lecture-6-continual-learning/#why-measure-distribution-drift","title":"Why Measure Distribution Drift?","text":"<p>Your model's performance is only guaranteed on data sampled from the same distribution as it was trained on. This can have a huge impact in practice. A recent example includes changes in model behavior during the pandemic. A bug in the retraining pipeline caused the recommendations not to be updated for new users, leading to millions of dollars in revenue lost.</p>"},{"location":"course/2022/lecture-6-continual-learning/#types-of-distribution-drift","title":"Types of Distribution Drift","text":"<p>Distribution drift manifests itself in different ways in the wild:</p> <ol> <li> <p>Instantaneous drift happens when a model is deployed in a new domain, a bug is introduced in the pre-processing pipeline, or a big external shift like COVID occurs.</p> </li> <li> <p>Gradual drift happens when users\\' preferences change or new concepts get introduced to the corpus over time.</p> </li> <li> <p>Periodic drift happens when users' preferences are seasonal or people in different time zones use your model differently.</p> </li> <li> <p>Temporary drift happens when a malicious user attacks your model, a new user tries your product and churns, or someone uses your product in an unintended way.</p> </li> </ol>"},{"location":"course/2022/lecture-6-continual-learning/#how-to-measure-it","title":"How to Measure It?","text":"<p>How to tell if your distribution is drifted?</p> <ol> <li> <p>Your first select a window of \"good\" data to serve as a reference. To select that reference, you can use a fixed window of production data you believe to be healthy. Some papers advocate for using a sliding window of production data. In practice, most of the time you probably should use your validation data as the reference.</p> </li> <li> <p>Once you have that reference data, you select a new window of production data to measure your distribution distance on. This is not a super principled approach and tends to be problem-dependent. A pragmatic solution is to pick one or several window sizes with a reasonable amount of data and slide them.</p> </li> <li> <p>Finally, once you have your reference window and production window, you compare the windows using a distribution distance metric.</p> </li> </ol>"},{"location":"course/2022/lecture-6-continual-learning/#what-metrics-to-use","title":"What Metrics To Use?","text":"<p>Let's start by considering the one-dimensional case, where you have a particular feature that is one-dimensional and can compute a density of that feature on your reference/production windows. You want some metric that approximates the distance between these two distributions.</p> <p></p> <p>There are a few options here:</p> <ol> <li> <p>The commonly recommended ones are the KL divergence and the KS test. But they are actually bad choices.</p> </li> <li> <p>Sometimes-better options would be (1) infinity norm or 1-norm of the diff between probabilities for each category, and (2) Earth-mover's distance (a more statistically principled approach).</p> </li> </ol> <p>Check out this Gantry blog post to learn more about why the commonly recommended metrics are not so good and the other ones are better.</p>"},{"location":"course/2022/lecture-6-continual-learning/#dealing-with-high-dimensional-data","title":"Dealing with High-Dimensional Data","text":"<p>In the real world for most models, we have potentially many input features or even unstructured data that is very high-dimensional. How do we deal with detecting distribution drift in those cases?</p> <ol> <li> <p>You can measure drift on all of the features independently: If you have a lot of features, you will hit the multiple hypothesis testing problem. Furthermore, this doesn't capture cross-correlation.</p> </li> <li> <p>You can measure drift on only the important features: Generally speaking, it's a lot more useful to measure drift on the outputs of the model than the inputs. You can also rank the importance of your input features and measure drift on the most important ones.</p> </li> <li> <p>You can look at metrics that natively compute or approximate the distribution distance between high-dimensional distributions: The two that are worth checking out are maximum mean discrepancy and approximate Earth-mover's distance. The caveat here is that they are pretty hard to interpret.</p> </li> </ol> <p></p> <p>A more principled way to measure distribution drift for high-dimensional inputs to the model is to use projections. The idea of a projection is that:</p> <ol> <li> <p>You first take some high-dimensional input to the model and run that through a function.</p> </li> <li> <p>Each data point your model makes a prediction on gets tagged by this projection function. The goal of this projection function is to reduce the dimensionality of that input.</p> </li> <li> <p>Once you've reduced the dimensionality, you can do drift detection on that lower-dimensional representation of the high-dimensional data.</p> </li> </ol> <p>This approach works for any kind of data, no matter what the dimensionality is or what the data type is. It's also highly flexible. There are different types of projections that can be useful: analytical projections (e.g., mean pixel value, length of sentence, or any other function), random projections (e.g., linear), and statistical projections (e.g., autoencoder or other density models, T-SNE).</p>"},{"location":"course/2022/lecture-6-continual-learning/#cons-of-looking-at-distribution-drift","title":"Cons of Looking at Distribution Drift","text":"<p>Models are designed to be robust to some degree of distribution drift. The figure on the left above shows a toy example to demonstrate this point. We have a classifier that's trained to predict two classes. We've induced a synthetic distribution shift to shift the red points on the top left to bottom. These two distributions are extremely different, but the model performs equally well on the training data and the production data. In other words, knowing the distribution shift doesn't tell you how the model has reacted to that shift.</p> <p>The figure on the right is a research project that used data generated from a physics simulator to solve problems on real-world robots. The training data was highly out of distribution (low-fidelity, random images). However, by training on this set of training data, the model was able to generalize to real-world scenarios on the test data.</p> <p>Beyond the theoretical limitations of measuring distribution drift, this is just hard to do in practice. You have to window size correctly. You have to keep all this data around. You need to choose metrics. You need to define projections to make your data lower-dimensional.</p>"},{"location":"course/2022/lecture-6-continual-learning/#system-metrics","title":"System Metrics","text":"<p>The last thing to consider looking at is your standard system metrics such as CPU utilization, GPU memory usage, etc. These don't tell you anything about how your model is actually performing, but they can tell you when something is going wrong.</p>"},{"location":"course/2022/lecture-6-continual-learning/#practical-recommendations","title":"Practical Recommendations","text":"<p>We also want to look at how hard it is to compute the aforementioned stages in practice. As seen below, the Y-axis shows the value of each signal and the X-axis shows the feasibility of measuring each signal.</p> <ol> <li> <p>Measuring outcomes or feedback has pretty wide variability in terms of how feasible it is to do, as it depends on how your product is set up.</p> </li> <li> <p>Measuring model performance tends to be the least feasible thing to do because it involves collecting some labels.</p> </li> <li> <p>Proxy metrics are easier to compute because they don't involve labels.</p> </li> <li> <p>System metrics and data quality metrics are highly feasible because you have off-the-shelf tools for them.</p> </li> </ol> <p></p> <p>Here are our practical recommendations:</p> <ol> <li> <p>Basic data quality checks are zero-regret, especially if you are retraining your model.</p> </li> <li> <p>Get some way to measure feedback, model performance, or proxy metrics, even if it's hacky or not scalable.</p> </li> <li> <p>If your model produces low-dimensional outputs, monitoring those for distribution shifts is also a good idea.</p> </li> <li> <p>As you evolve your system, practice the observability mindset.</p> </li> </ol> <p>While you can think of monitoring as measuring the known unknowns (e.g., setting alerts on a few key metrics), observability is measuring unknown unknowns (e.g., having the power to ask arbitrary questions about your system when it breaks). An observability mindset means two implications:</p> <ol> <li> <p>You should keep around the context or raw data that makes up the metrics that you are computing since you want to be able to drill all the way down to potentially the data points themselves that make up the degraded metric.</p> </li> <li> <p>You can go crazy with measurement by defining a lot of different metrics. You shouldn't necessarily set alerts on each of those since you don't want too many alerts. Drift is a great example since it is useful for debugging but less so for monitoring.</p> </li> </ol> <p>Finally, it's important to go beyond aggregate metrics. If your model is 99% accurate in aggregate but only 50% accurate for your most important user, is it still \"good\"? The way to deal with this is by flagging important subgroups or cohorts of data and alerting on important metrics across them. Some examples are categories you don't want to be biased against, \"important\" categories of users, and categories you might expect to perform differently on (languages, regions, etc.).</p>"},{"location":"course/2022/lecture-6-continual-learning/#how-to-tell-if-those-metrics-are-bad","title":"How To Tell If Those Metrics are \"Bad\"","text":"<p>We don't recommend statistical tests (e.g., KS-Test) because they try to return a p-value for the likelihood that the data distributions are not the same. When you have a lot of data, you will get very small p-values for small shifts. This is not what we actually care about since models are robust to a small number of distribution shifts.</p> <p>Better options than statistical tests include fixed rules, specific ranges, predicted ranges, and unsupervised detection of new patterns. This article on dynamic data testing has the details.</p> <p></p>"},{"location":"course/2022/lecture-6-continual-learning/#tools-for-monitoring","title":"Tools for Monitoring","text":"<p>The first category is system monitoring tools, a premature category with different companies in it (Datadog, Honeycomb, New Relic, Amazon CloudWatch, etc.). They help you detect problems with any software system, not just ML models. They provide functionality for setting alarms when things go wrong. Most cloud providers have decent monitoring solutions, but if you want something better, you can look at monitoring-specific tools to monitor anything.</p> <p>This raises the question of whether we should just use these system monitoring tools to monitor ML metrics as well. This blog post explains that it's feasible but highly painful due to many technical reasons. Thus, it's better to use ML-specific tools.</p> <p>Two popular open-source monitoring tools are EvidentlyAI and whylogs.</p> <ul> <li> <p>Both are similar in that you provide them with samples of data and they produce a nice report that tells you where their distribution shifts are.</p> </li> <li> <p>The big limitation of both is that they don't solve the data infrastructure and the scale problem. You still need to be able to get all that data into a place where you can analyze it with these tools.</p> </li> <li> <p>The main difference between them is that whylogs is more focused on gathering data from the edge by aggregating the data into statistical profiles at inference time. You don't need to transport all the data from your inference devices back to your cloud.</p> </li> </ul> <p></p> <p>Lastly, there are a bunch of different SaaS vendors for ML monitoring and observability: Gantry, Aporia, Superwise, Arize, Fiddler, Arthur, etc.</p>"},{"location":"course/2022/lecture-6-continual-learning/#5-retraining-strategy","title":"5 - Retraining Strategy","text":"<p>We\u2019ve talked about monitoring and observability, which allow you to identify issues with your continual learning system. Now, we\u2019ll talk about how we will fix the various stages of the continual learning process based on what we learn from monitoring and observability.</p>"},{"location":"course/2022/lecture-6-continual-learning/#logging","title":"Logging","text":"<p>The first stage of the continual learning loop is logging. As a reminder, the goal of logging is to get data from your model to a place where you can analyze it. The key question to answer here is: \u201cwhat data should I actually log?\u201d</p> <p>For most of us, the best answer is just to log all of the data. Storage is cheap. It's better to have data than not to have it. There are, however, some situations where you can't do that. For example, if you have too much traffic going through your model to the point where it's too expensive to log all of it, or if you have data privacy concerns, or if you're running your model at the edge, you simply may not be able to log all your data. </p> <p>In these situations, there are two approaches that you can take. The first approach is profiling. With profiling, rather than sending all the data back to your cloud and then using that to monitor, you instead compute statistical profiles of your data on the edge that describe the data distribution that you're seeing. This is great from a data security perspective because it doesn't require you to send all the data back home. It minimizes your storage cost. Finally, you don't miss things that happen in the tails, which is an issue for the next approach. That'll describe the place to use. This approach is best used for security-critical applications. Computing statistical profiles is a pretty interesting topic in computer science and data summarization that is worth checking out if you\u2019re interested in this approach. </p> <p></p> <p>The other approach is sampling. With sampling, you'll just take certain data points and send those back to your monitoring and logging system. The advantage of sampling is that it has minimal impact on your inference resources. You don't have to actually spend the computational budget to compute profiles. You also get to have access to the raw data for debugging and retraining, albeit a smaller amount. This is the approach we recommend for any other kind of application.</p>"},{"location":"course/2022/lecture-6-continual-learning/#curation","title":"Curation","text":"<p>The next step in the continual learning loop is curation. The goal of curation is to take the infinite stream of production data, which is potentially unlabeled, and turn it into a finite reservoir of enriched data suitable for training. Here, we must answer, \u201cwhat data should be enriched?\u201d</p> <p>You could sample and enrich data randomly, but that may not prove helpful to your model. Importantly, you miss rare classes or events. A better approach can be to perform stratified subsampling, wherein you sample specific proportions of individuals from various subpopulations (e.g. race). The most advanced strategy for picking data to enrich is to curate data points that are somehow interesting for the purpose of improving your model. </p> <p>There are a few different ways of doing this: user-driven curation loops via feedback loops, manual curation via error analysis, and automatic curation via active learning. </p> <p>User-driven curation is a great approach that is easy to implement, assuming you have a clear way of gathering user feedback. If your user churns, clicks thumbs down, or performs some other similar activity on the model\u2019s output, you have an easy way of understanding data that could be enriched for future training jobs.</p> <p></p> <p>If you don't have user feedback, or if you need even more ways of gathering interesting data from your system, the second most effective way is by doing manual error analysis. In this approach, we look at the errors that our model is making, reason about the different types of failure modes that we're seeing, and try to write functions or rules that help capture these error modes. We'll use those functions to gather more data that might represent those error cases. Some examples of these function-based approaches are similarity-based curation, which uses nearest neighbors, and projection-based curation, wherein we train a new function or model to recognize key data points.</p> <p>The last way to curate data is to do so automatically using a class of algorithms called active learning. The way active learning works is that, given a large amount of unlabeled data, we will try to determine which data points would improve model performance the most (if you were to label those data points next and train on them). These algorithms define a sampling strategy, rank all of your unlabeled examples using a scoring function that defines the sampling strategy, and mark the data points with the highest scores for future labeling. </p> <p>There are a number of different scoring function approaches that are shown below.</p> <ol> <li>Most uncertain: sample low-confidence and high-entropy predictions or predictions that an ensemble disagrees on.</li> <li>Highest predicted loss: train a separate model that predicts loss on unlabeled points, then sample the highest predicted loss.</li> <li>Most different from labels: train a model to distinguish labeled and unlabeled data, then sample the easiest to distinguish.</li> <li>Most representative: choose points such that no data is too far away from anything we sampled.</li> <li>Big impact on training: choose points such that the expected gradient is large or points where the model changes its mind the most about its prediction during training.</li> </ol> <p>Uncertainty scoring tends to be the most commonly used method since it is simple and easy to implement. </p> <p>You might have noticed that there's a lot of similarity between some of the ways that we do data curation and the way that we do monitoring. That's no coincidence--monitoring and data curation are two sides of the same coin! They're both interested in solving the problem of finding data points where the model may not be performing well or where we're uncertain about how the model is performing on those data points.</p> <p></p> <p>Some examples of people practically applying data curation are OpenAI\u2019s DALL-E 2, which uses active learning and manual curation, Tesla, which uses feedback loops and manual curation, and Cruise, which uses feedback loops.</p> <p>Some tools that help with data curation are Scale Nucleus, Aquarium, and Gantry. </p> <p>To summarize then, here are our final set of recommendations for applying data curation.</p> <ol> <li>Random sampling is a fine starting point. If you want to avoid bias or have rare classes, do stratified sampling instead.</li> <li>If you have a feedback loop, then user-driven curation is a no-brainer. If not, confidence-based active learning is easy to implement.</li> <li>As your model performance increases, you\u2019ll have to look harder for challenging training points. Manual techniques are unavoidable and should be embraced. Know your data!</li> </ol>"},{"location":"course/2022/lecture-6-continual-learning/#retraining-triggers","title":"Retraining Triggers","text":"<p>After we've curated our infinite stream of unlabeled data down to a reservoir of labeled data that's ready to potentially train on, the next thing that we'll need to decide is \u201cwhat trigger are we gonna use to retrain?\u201d</p> <p>The main takeaway here is that moving to automated retraining is not always necessary. In many cases, just manually retraining is good enough. It can save you time and lead to better model performance. It's worth understanding when it makes sense to actually make the harder move to automated retraining.</p> <p>The main prerequisite for moving to automated retraining is being able to reproduce model performance when retraining in a fairly automated fashion. If you're able to do that and you are not really working on the model actively, it's probably worth implementing some automated retraining. As a rule of thumb, if you\u2019re retraining the model more than once a month, automated retraining may make sense.</p> <p>When it's time to move to automated training, the main recommendation is to just keep it simple and retrain periodically, e.g. once a week. The main question though is, how do you pick the right training schedule? The recommendation here is to:</p> <ol> <li>Apply measurement to figure out a reasonable retraining schedule.</li> <li>Plot your model performance and degradation over time.</li> <li>Compare how retraining the model at various intervals would have resulted in improvements to its performance.</li> </ol> <p>As seen below, the area between the curves represents the opportunity cost, so always remember to balance the upside of retraining with the operational costs of retraining.</p> <p></p> <p>This is a great area for future academic research! More specifically, we can look at ways to automate determining the optimal retraining strategy based on performance decay, sensitivity to performance, operational costs, and retraining costs.</p> <p>An additional option for retraining, rather than time-based intervals, is performance triggers (e.g. retrain when the model accuracy dips below 90%). This helps react more quickly to unexpected changes and is more cost-optimal, but requires very good instrumentation to process these signals along with operational complexity.</p> <p>An idea that probably won't be relevant but is worth thinking about is online learning. In this paradigm, you train on every single data point as it comes in. It's not very commonly used in practice. </p> <p>A version of this idea that is used fairly frequently in practice is online adaptation. This method operates not at the level of retraining the whole model itself but rather on the level of adapting the policy that sits on top of the model. What is a policy you ask? A policy is the set of rules that takes the raw prediction that the model made, like the score or the raw output of the model, and turns it into the output the user sees. In online adaptation, we use algorithms like multi-armed bandits to tune these policies. If your data changes very frequently, it is worth looking into this method.</p>"},{"location":"course/2022/lecture-6-continual-learning/#dataset-formation","title":"Dataset Formation","text":"<p>Imagine we've fired off a trigger to start a new training job. The next question we need to answer is, among all of the labeled data in our reservoir of data, what specific data points should we train on for this particular new training job?</p> <p>We have four options here. Most of the time in deep learning, we'll just use the first option and train on all the data that we have available to us. Remember to keep your data version controlled and your curation rules consistent.</p> <p></p> <p>If you have too much data to do that, you can use recency as a heuristic for a second option and train on only a sliding window of the most recent data (if recency is important) or sample a smaller portion (if recency isn\u2019t). In the latter case, compare the aggregate statistics between the old and new windows to ensure there aren\u2019t any bugs. It\u2019s also important in both cases to compare the old and new datasets as they may not be related in straightforward ways. </p> <p></p> <p>A useful third option is online batch selection, which can be used when recency doesn\u2019t quite matter. In this method, we leverage label-aware selection functions to choose which items in mini-batches to train on. </p> <p></p> <p>A more difficult fourth option that isn\u2019t quite recommended is continual fine-tuning. Rather than retraining from scratch every single time, you train your existing model on just new data. The reason why you might wanna do this primarily is because it's much more cost-effective. The paper below shares some findings from GrubHub, where they found a 45x cost improvement by doing this technique relative to sliding windows.</p> <p></p> <p>The big challenge here is that unless you're very careful, it's easy for the model to forget what it learned in the past. The upshot is that you need to have mature evaluation practices to be very careful that your model is performing well on all the types of data that it needs to perform well on.</p>"},{"location":"course/2022/lecture-6-continual-learning/#offline-testing","title":"Offline Testing","text":"<p>After the previous steps, we now have a new candidate model that we think is ready to go into production. The next step is to test that model. The goal of this stage is to produce a report that our team can sign off on that answers the question of whether this new model is good enough or whether it's better than the old model. The key question here is, \u201cwhat should go into that report?\u201d</p> <p>This is a place where there's not a whole lot of standardization, but the recommendation we have here is to compare your current model with the previous version of the model on all of the metrics that you care about, all of the subsets of data that you've flagged are important, and all the edge cases you\u2019ve defined. Remember to adjust the comparison to account for any sampling bias.</p> <p>Below is a sample comparison report. Note how the validation set is broken out into concrete subgroups. Note also how there are specific validation sets assigned to common error cases.</p> <p></p> <p>In continual learning, evaluation sets are dynamically refined just as much as training sets are. Here are some guidelines for how to manage evaluation sets in a continual learning system:</p> <ol> <li>As you curate new data, add some of it to your evaluation sets. For example, if you change how you do sampling, add that newly sampled data to your evaluation set. Or if you encounter a new edge case, create a test case for it.</li> <li>Corollary 1: you should version control your evaluation sets as well.</li> <li>Corollary 2: if your data changes quickly, always hold out the most recent data for evaluation.</li> </ol> <p>Once you have the testing basics in place, a more advanced option that you can look into here is expectation testing. Expectation tests work by taking pairs of examples where you know the relationship between the two. These tests help a lot with understanding the generalizability of models.</p> <p></p> <p>Just like how data curation is highly analogous to monitoring, so is offline testing. We want to observe our metrics, not just in aggregate but also across all of our important subsets of data and across all of our edge cases. One difference between these two is that you will have different metrics available in offline testing and online testing. For example, you\u2019re much more likely to have labels offline. Online, you\u2019re much more likely to have feedback. We look forward to more research that can predict online metrics from offline ones.</p>"},{"location":"course/2022/lecture-6-continual-learning/#online-testing","title":"Online Testing","text":"<p>Much of this we covered in the last lecture, so we\u2019ll keep it brief! Use shadow mode and A/B tests, roll out models gradually, and roll back models if you see issues during rollout. </p>"},{"location":"course/2022/lecture-6-continual-learning/#6-the-continual-improvement-workflow","title":"6 - The Continual Improvement Workflow","text":"<p>To tie it all together, we\u2019ll conclude with an example. Monitoring and continual learning are two sides of the same coin. We should be using the signals that we monitor to very directly change our retraining strategy. This section describes the future state that comes as a result of investing in the steps laid out previously. </p> <p>Start with a place to store and version your strategy. The components of your continual learning strategy should include the following:</p> <ul> <li>Inputs, predictions, user feedback, and labels.</li> <li>Metric definitions for monitoring, observability, and offline testing.</li> <li>Projection definitions for monitoring and manual data curation.</li> <li>Subgroups and cohorts of interest for monitoring and offline testing.</li> <li>Data curation logic.</li> <li>Datasets for training and evaluation.</li> <li>Model comparison reports.</li> </ul> <p>Walk through this example to understand how changes to the retraining strategy occur as issues surface in our machine learning system.</p> <p></p>"},{"location":"course/2022/lecture-6-continual-learning/#7-takeaways","title":"7 - Takeaways","text":"<p>To summarize, continual learning is a nascent, poorly understood topic that is worth continuing to pay attention to. Watch this space! In this lecture, we focused on all the steps and techniques that allow you to use retraining effectively. As MLEs, leverage monitoring to strategically improve your model. Always start simple, and get better!</p>"},{"location":"course/2022/lecture-7-foundation-models/","title":"Lecture 7: Foundation Models","text":"<p>Lecture by Sergey Karayev. Notes by James Le and Vishnu Rachakonda. Published September 19, 2022. Download slides.</p> <p>Foundation models are very large models trained on very large datasets that can be used for multiple downstream tasks.</p> <p>We\u2019ll talk about fine-tuning, Transformers, large language models, prompt engineering, other applications of large models, and vision and text-based models like CLIP and image generation.</p> <p></p>"},{"location":"course/2022/lecture-7-foundation-models/#1-fine-tuning","title":"1 - Fine-Tuning","text":"<p>Traditional ML uses a lot of data and a large model, which takes a long time. But if you have a small amount of data, you can use transfer learning to benefit from the training on a lot of data. You basically use the same model that you have pre-trained, add a few layers, and unlock some weights.</p> <p>We have been doing this in computer vision since 2014. Usually, you train a model on ImageNet, keep most of the layers, and replace the top three or so layers with newly learned weights. Model Zoos are full of these models like AlexNet, ResNet, etc. in both TensorFlow and PyTorch.</p> <p>In NLP, pre-training was initially limited only to the first step: word embeddings. The input to a language model is words. One way you can encode them to be a vector (instead of a word) is one-hot encoding. Given a large matrix of words, you can make an embedding matrix and embed each word into a real-valued vector space. This new matrix is down to the dimension on the order of a thousand magnitude. Maybe those dimensions correspond to some semantic notion.</p> <p></p> <p>Word2Vec trained a model like this in 2013. It looked at which words frequently co-occur together. The learning objective was to maximize cosine similarity between their embeddings. It could do cool demos of vector math on these embeddings. For example, when you embed the words \u201cking,\u201d \u201cman,\u201d and \u201cwoman,\u201d you can do vector math to get a vector that is close to the word \u201cqueen\u201d in this embedding space.</p> <p>It\u2019s useful to see more context to embed words correctly because words can play different roles in the sentence (depending on their context). If you do this, you\u2019ll improve accuracy on all downstream tasks. In 2018, a number of models such as ELMO and ULMFit published pre-trained LSTM-based models that set state-of-the-art results on most NLP tasks.</p> <p>But if you look at the model zoos today, you won\u2019t see any LSTMs. You\u2019ll only see Transformers everywhere. What are they?</p>"},{"location":"course/2022/lecture-7-foundation-models/#2-transformers","title":"2 - Transformers","text":"<p>Transformers come from a paper called \u201cAttention Is All You Need\u201d in 2017, which introduced a groundbreaking architecture that sets state-of-the-art results on translation first and a bunch of NLP tasks later.</p> <p></p> <p>It has a decoder and an encoder. For simplicity, let\u2019s take a look at the encoder. The interesting components here are self-attention, positional encoding, and layer normalization.</p>"},{"location":"course/2022/lecture-7-foundation-models/#self-attention","title":"Self-Attention","text":"<p>Basic self-attention follows: Given an input sequence of vectors x of size t, we will produce an output sequence of tensors of size t. Each tensor is a weighted sum of the input sequence. The weight here is just a dot product of the input vectors. All we have to do is to make that weighted vector sum to 1. We can represent it visually, as seen below. The input is a sentence in English, while the output is a translation in French.</p> <p></p> <p>So far, there are no learned weights and no sequence order. Let\u2019s learn some weights! If we look at the input vectors, we use them in three ways: as queries to compare two other input vectors, as keys to compare them to input vectors and produce the corresponding output vector, and as values to sum up all the input vectors and produce the output vector. * We can process each input vector with three different matrices to fulfill these roles of query, key, and value. We will have three weighted matrices, and everything else remains the same. If we learn these matrices, we learn attention. * It\u2019s called multi-head attention *because we learn different sets of weighted matrices simultaneously, but we implement them as just a single matrix.</p> <p>So far, we have learned the query, key, and value. Now we need to introduce some notion of order to the sequence by encoding each vector with its position. This is called positional encoding.</p>"},{"location":"course/2022/lecture-7-foundation-models/#positional-encoding","title":"Positional Encoding","text":"<p>Let\u2019s say we have an input sequence of words</p> <p>]* The first step is to embed the words into a dense, real-valued word embedding. This part can be learned. * However, there is no order to that embedding. Thus, we will add another embedding that only encodes the position. * In brief, the first embedding encodes only the content, while the second embedding encodes only the position. If you add them, you now have information about both the content and the position.</p>"},{"location":"course/2022/lecture-7-foundation-models/#layer-normalization","title":"Layer Normalization","text":"<p>Neural network layers work best when the input vectors have uniform mean and standard deviation in each dimension. As activations flow through the network, the means and standard deviations get blown out by the weight matrices. Layer normalization is a hack to re-normalize every activation to where we want them between each layer.</p> <p>That\u2019s it! All the amazing results you\u2019ll see from now on are just increasingly large Transformers with dozens of layers, dozens of heads within each layer, large embedding dimensions, etc. The fundamentals are the same. It\u2019s just the Transformer model.</p> <p>Anthropic has been publishing great work lately to investigate why Transformers work so well. Check out these publications:</p> <ol> <li>A Mathematical Framework for Transformer Circuits</li> <li>In-Context Learning and Induction Heads</li> <li>Toy Models of Superposition</li> </ol>"},{"location":"course/2022/lecture-7-foundation-models/#3-large-language-models","title":"3 - Large Language Models","text":""},{"location":"course/2022/lecture-7-foundation-models/#models","title":"Models","text":"<p>GPT and GPT-2 came out in 2018 and 2019, respectively. The name means \u201cgenerative pre-trained Transformers.\u201d They are decoder-only models and use masked self-attention. This means: At a poi that at the output sequence, you can only attend to two input sequence vectors that came before that point in the sequence.</p> <p></p> <p>These models were trained on 8 million web pages. The largest model has 1.5 billion parameters. The task that GPT-2 was trained on is predicting the next word in all of this text on the web. They found that it works increasingly well with an increasing number of parameters.</p> <p></p> <p>BERT came out around the same time as Bidirectional Encoder Representations for Transformers. It is encoder-only and does not do attention masking. It has 110 million parameters. During training, BERT masks out random words in a sequence and has to predict whatever the masked word is.</p> <p></p> <p>T5 (Text-to-Text Transformer) came out in 2020. The input and output are both text strings, so you can specify the task that the model supposes to be doing. T5 has an encoder-decoder architecture. It was trained on the C4 dataset (Colossal Clean Crawled Corpus), which is 100x larger than Wikipedia. It has around 10 billion parameters. You can download the open-sourced model and run it on your machine.</p> <p>GPT-3 was one of the state-of-the-art models in 2020. It was 100x larger than GPT/GPT-2 with 175 billion parameters. Because of its size, GPT-3 exhibits unprecedented capabilities of few-shot and zero-shot learning. As seen in the graph below, the more examples you give the model, the better its performance is. The larger the model is, the better its performance is. If a larger model was trained, it would be even better.</p> <p></p> <p>OpenAI also released Instruct-GPT earlier this year. It had humans rank different GPT-3 outputs and used reinforcement learning to fine-tune the model. Instruct-GPT was much better at following instructions. OpenAI has put this model, titled \u2018text-davinci-002,\u2019 in their API. It is unclear how big the model is. It could be ~10x smaller than GPT-3.</p> <p></p> <p>DeepMind released RETRO (Retrieval-Enhanced Transformers) in 2021. Instead of learning language and memorizing facts in the model\u2019s parameters, why don\u2019t we just learn the language in parameters and retrieve facts from a large database of internal text? To implement RETRO, they encode a bunch of sentences with BERT and store them in a huge database with more than 1 trillion tokens. At inference time, they fetch matching sentences and attend to them. This is a powerful idea because RETRO is connected to an always updated database of facts.</p> <p></p> <p>DeepMind released another model called Chinchilla in 2022, which observed the scaling laws of language models. They trained over 400 language models from 70 million to 16 billion parameters on 5 billion to 500 billion tokens. They then derived formulas for optimal model and training set size, given a fixed compute budget. They found that most large language models are \u201cundertrained,\u201d meaning they haven\u2019t seen enough data.</p> <p></p> <p>To prove this, they trained a large model called Gopher with 280 billion parameters and 300 billion tokens. With Chincilla, they reduced the number of parameters to 70 billion and used four times as much data (1.4 trillion tokens). Chinchilla not only matched Gopher\u2019s performance but actually exceeded it. Check out this LessWrong post if you want to read about people\u2019s opinions on it.</p>"},{"location":"course/2022/lecture-7-foundation-models/#vendors","title":"Vendors","text":"<p>OpenAI offers four model sizes: Ada, Babbage, Curie, and Davinci. Each has a different price and different capabilities. Most of the impressive GPT-3 results on the Internet came from Davinci. These correspond to 350M, 1.3B, 6.7B, and 175B parameters. You can also fine-tune models for an extra cost. The quota you get when you sign up is pretty small, but you can raise it over time. You have to apply for review before going into production.</p> <p>There are some alternatives to OpenAI:</p> <ol> <li>Cohere AI has similar models for similar prices.</li> <li>AI21 also has some large models.</li> <li>There are also open-source large language models, such as Eleuther GPT-NeoX (20B parameters), Facebook OPT-175B (175B parameters), and BLOOM from BigScience (176B parameters). If you want to use one of these open-source models but do not have to be responsible for deploying it, you can use HuggingFace\u2019s inference API.</li> </ol>"},{"location":"course/2022/lecture-7-foundation-models/#4-prompt-engineering","title":"4 - Prompt Engineering","text":"<p>GPT-3 and other large language models are mostly alien technologies. It\u2019s unclear how they exactly work. People are finding out how they work by playing with them. We will cover some notable examples below. Note that if you play around with them long enough, you are likely to discover something new.</p> <p>GPT-3 is surprisingly bad at reversing words due to tokenization: It doesn\u2019t see letters and words as humans do. Instead, it sees \u201ctokens,\u201d which are chunks of characters. Furthermore, it gets confused with long-ish sequences. Finally, it has trouble merging characters. For it to work, you have to teach GPT-3 the algorithm to use to get around its limitations. Take a look at this example from Peter Welinder.</p> <p></p> <p>Another crazy prompt engineering is \u201cLet\u2019s Think Step By Step.\u201d This comes from a paper called \u201cLarge Language Models are Zero-Shot Reasoners.\u201d Simply adding \u201cLet\u2019s Think Step By Step\u201d into the prompt increases the accuracy of GPT-3 on one math problem dataset from 17% to 78% and another math problem dataset from 10% to 40%.</p> <p></p> <p>Another unintuitive thing is that the context length of GPT is long. You can give it a long instruction and it can return the desired output. This example shows how GPT can output a CSV file and write the Python code as stated. You can also use formatting tricks to reduce the training cost, as you can do multiple tasks per call. Take a look at this example for inspiration.</p> <p>We have to be careful since our models might get pwnage or possessed. User input in the prompt may instruct the model to do something naughty. This input can even reveal your prompt to prompt injection attacks and possess your AI. This actually works in GPT-3-powered production apps.</p> <p></p> <p>Further work is needed before putting GPT-3-powered apps into production. There are some tools for prompt engineering such as PromptSource and OpenPrompt, but we definitely need better tools.</p>"},{"location":"course/2022/lecture-7-foundation-models/#5-other-applications","title":"5 - Other Applications","text":""},{"location":"course/2022/lecture-7-foundation-models/#code","title":"Code","text":"<p>One notable application of large foundation models is code generation. With a 40- billion-parameter Transformer model pre-trained on all the Github code it could find, DeepMind Alphacode was able to achieve an above-average score on the Codeforce competition. To do this, they used a model to generate a large set of potential solutions and another model to winnow down the options by actually executing them. </p> <p>The general idea to highlight from this is filtering the outputs of a model. You can have a separate model that does filtering, or you can have some kind of verification + validation process. This can really significantly boost accuracy. OpenAI demonstrates impressive results on different math word problems, as seen below.</p> <p></p> <p>Code generation has moved into products of late, like Github Copilot. We highly recommend trying it out! Another option for a similar tool is replit\u2019s new tool for coding. </p> <p>We\u2019re just getting started with the applications of foundation models to the programming workflow. In fact, things are about to start getting really wild. A recent paper showed that a large language model that generated its own synthetic puzzles to learn to code could improve significantly. Models are teaching themselves to get better!</p> <p></p> <p>Playing around with systems like GPT-3 and their ability to generate code can feel quite remarkable! Check out some fun experiments Sergey ran (here and here).</p> <p></p>"},{"location":"course/2022/lecture-7-foundation-models/#semantic-search","title":"Semantic Search","text":"<p>Semantic search is another interesting application area. If you have texts like words, sentences, paragraphs, or even whole documents, you can embed that text with large language models to get vectors. If you have queries in sentences or paragraphs, you can also embed them in the same way. With this function, you can generate embeddings and easily find semantic overlap by examining the cosine similarity between embedding vectors.</p> <p></p> <p>Implementing this semantic search is hard. Computations on large, dense vectors with float data types are intensive. Companies like Google and Facebook that use this approach have developed libraries like FAISS and ScaNN to solve the challenges of implementing semantic search. </p> <p>Some open-source options for this include Haystack from DeepSet and Jina.AI. Other vendor options include Pinecone, Weaviate, Milvus, Qdrant, Google Vector AI Matching Engine, etc.</p>"},{"location":"course/2022/lecture-7-foundation-models/#going-cross-modal","title":"Going Cross-Modal","text":"<p>Newer models are bridging the gap between data modalities (e.g. using both vision and text). One such model is the Flamingo model, which uses a special model component called a perceiver resampler (an attention module that translates images into fixed-length sequences of tokens).</p> <p></p> <p>Another paper about Socratic Models was recently published. The author trained several large models (a vision model, a language model, and an audio model) that are able to interface with each other using language prompts to perform new tasks.</p> <p>Finally, the concept of \u201cFoundation Models\u201d came from the paper \u201cOn the Opportunities and Risks of Foundation Models\u201d by researchers at Stanford Institute for Human-Centered AI. We think \u201cLarge Language Models\u201d or \u201cLarge Neural Networks\u201d might be more useful terms.</p>"},{"location":"course/2022/lecture-7-foundation-models/#6-clip-and-image-generation","title":"6 - CLIP and Image Generation","text":"<p>Now, let's talk about some of the most exciting applications of this kind of model: in vision!</p> <p>In a 2021 OpenAI paper called \u201cLearning transferrable visual models from natural language supervision\u201d, CLIP (Contrastive Language\u2013Image Pre-training) was introduced. In this paper, the authors encode text via Transforms, encode images via ResNets or Visual Transformers, and apply contrastive training to train the model. Contrastive training matches correct image and text pairs using cosine similarity. The code for this is tremendously simple!</p> <p></p> <p>With this powerful trained model, you can map images and text using embeddings, even on unseen data. There are two ways of doing this. One is to use a \u201clinear probe\u201d by training a simple logistic regression model on top of the features CLIP outputs after performing inference. Otherwise, you can use a \u201czero-shot\u201d technique that encodes all the text labels and compares them to the encoded image. Zero-shot tends to be better, but not always.</p> <p>Since OpenAI CLIP was released in an open-source format, there have been many attempts to improve it, including the OpenCLIP model, which actually outperforms CLIP.</p> <p>To clarify, CLIP doesn\u2019t go directly from image to text or vice versa. It uses embeddings. This  embedding space, however, is super helpful for actually performing searches across modalities. This goes back to our section on vector search. There are so many cool projects that have come out of these efforts! (like this and this)</p> <p>To help develop mental models for these operations, consider how to actual perform image captioning (image -&gt; text) and image generation (text -&gt; image). There are two great examples of this written in the ClipCap paper. At a high level, image captioning is performed through training a separate model to mediate between a frozen CLIP, which generates a series of word embeddings, and a frozen GPT-2, which takes these word embeddings and generates texts. </p> <p>The intermediate model is a Transformer model that gets better at modeling images and captions.</p> <p></p> <p>In image generation, the most well-known approach is taken by DALL-E 2 or unCLIP. In this method, two additional components are introduced to a CLIP system, a prior that maps from text embedding to image embeddings and a decoder that maps from image embedding to image. The prior exists to solve the problem that many text captions can accurately work for an image. </p> <p></p> <p>In DALL-E 2\u2019s case, they use an approach for the prior called a diffusion model. Diffusion models are trained to denoise data effectively through training on incrementally noisy data.</p> <p></p> <p>In DALL-E 2, the diffusion method is applied to the prior model, which trains its denoising approach on a sequence of encoded text, CLIP text embedding, the diffusion timestamp, and the noised CLIP embedding, all so it can predict the un-noised CLIP image embedding. In doing so, it helps us bridge the gap between the raw text caption to the model, which can be infinitely complicated and \u201cnoisy\u201d, and the CLIP image embedding space.</p> <p></p> <p>The decoder helps us go from the prior\u2019s output of an image embedding to an image. This is a much simpler approach for us to understand. We apply a U-Net structure to a diffusion training process that is able to ultimately \u201cde-noise\u201d the input image embedding and output an image.</p> <p></p> <p>The results of this model are incredible! You can even generate images and merge images using CLIP embeddings. There are all kinds of funky ways of playing with the embeddings to create various image outputs.</p> <p></p> <p>Other models of interest are Parti and StableDiffusion.</p> <ul> <li>Google published Parti soon after DALL-E 2. Parti uses a VQGAN model instead of a diffusion model, where the image is represented as a sequence of high-dimensional tokens.</li> <li>StableDiffusion has been released publicly, so definitely check it out! It uses a \u201clatent diffusion\u201d model, which diffuses the image in a low-dimensional latent space and decodes the image back into a pixel space.</li> </ul> <p></p> <p>There has been an absolute explosion of these applications. Check out these examples on image-to-image, video generation, and photoshop plugins. The sky is the limit.</p> <p>Prompting these models is interesting and can get pretty involved. Someday this may even be tool and code-based. You can learn from other people on Lexica and promptoMANIA.</p> <p>It\u2019s truly a remarkable time to be involved with AI models as they scale to new heights.</p>"},{"location":"course/2022/lecture-8-teams-and-pm/","title":"Lecture 8: ML Teams and Project Management","text":"<p>Lecture by Josh Tobin. Notes by James Le and Vishnu Rachakonda. Published September 26, 2022. Download slides.</p>"},{"location":"course/2022/lecture-8-teams-and-pm/#0-why-is-this-hard","title":"0 - Why is this hard?","text":"<p>Building any product is hard:</p> <ul> <li> <p>You have to hire great people.</p> </li> <li> <p>You have to manage and develop those people.</p> </li> <li> <p>You have to manage your team's output and make sure your vectors are aligned.</p> </li> <li> <p>You have to make good long-term technical choices and manage technical debt.</p> </li> <li> <p>You have to manage expectations from leadership.</p> </li> <li> <p>You have to define and communicate requirements with stakeholders.</p> </li> </ul> <p>Machine Learning (ML) adds complexity to that process:</p> <ul> <li> <p>ML talent is expensive and scarce.</p> </li> <li> <p>ML teams have a diverse set of roles.</p> </li> <li> <p>Projects have unclear timelines and high uncertainty.</p> </li> <li> <p>The field is moving fast, and ML is the \"high-interest credit card of technical debt.\"</p> </li> <li> <p>Leadership often doesn't understand ML.</p> </li> <li> <p>ML products fail in ways that are hard for laypeople to understand.</p> </li> </ul> <p>In this lecture, we'll talk about:</p> <ol> <li> <p>ML-related roles and their required skills.</p> </li> <li> <p>How to hire ML engineers (and how to get hired).</p> </li> <li> <p>How ML teams are organized and fit into the broader organization.</p> </li> <li> <p>How to manage an ML team and ML products.</p> </li> <li> <p>Design considerations for ML products.</p> </li> </ol>"},{"location":"course/2022/lecture-8-teams-and-pm/#1-roles","title":"1 - Roles","text":""},{"location":"course/2022/lecture-8-teams-and-pm/#common-roles","title":"Common Roles","text":"<p>Let's look at the most common ML roles and the skills they require:</p> <ul> <li> <p>The ML Product Manager works with the ML team, other business functions, the end-users, and the data owners. This person designs docs, creates wireframes, and develops a plan to prioritize and execute ML projects.</p> </li> <li> <p>The MLOps/ML Platform Engineer builds the infrastructure to make models easier and more scalable to deploy. This person handles the ML infrastructure that runs the deployed ML product using platforms like AWS, GCP, Kafka, and other ML tooling vendors.</p> </li> <li> <p>The ML Engineer trains and deploys prediction models. This person uses tools like TensorFlow and Docker to work with prediction systems running on real data in production.</p> </li> <li> <p>The ML Researcher trains prediction models, often those that are forward-looking or not production-critical. This person uses libraries like TensorFlow and PyTorch on notebook environments to build models and reports describing their experiments.</p> </li> <li> <p>The Data Scientist is a blanket term used to describe all of the roles above. In some organizations, this role entails answering business questions via analytics. This person can work with wide-ranging tools from SQL and Excel to Pandas and Scikit-Learn.</p> </li> </ul> <p></p>"},{"location":"course/2022/lecture-8-teams-and-pm/#skills-required","title":"Skills Required","text":"<p>What skills are needed for these roles? The chart below displays a nice visual - where the horizontal axis is the level of ML expertise and the size of the bubble is the level of communication and technical writing (the bigger, the better).</p> <p></p> <ul> <li> <p>The MLOps is primarily a software engineering role, which often comes from a standard software engineering pipeline.</p> </li> <li> <p>The ML Engineer requires a rare mix of ML and Software Engineering skills. This person is either an engineer with significant self-teaching OR a science/engineering Ph.D. who works as a traditional software engineer after graduate school.</p> </li> <li> <p>The ML Researcher is an ML expert who usually has an MS or Ph.D. degree in Computer Science or Statistics or finishes an industrial fellowship program.</p> </li> <li> <p>The ML Product Manager is just like a traditional Product Manager but with a deep knowledge of the ML development process and mindset.</p> </li> <li> <p>The Data Scientist role constitutes a wide range of backgrounds, from undergraduate to Ph.D. students.</p> </li> </ul> <p>There is an important distinction between a task ML engineer and a platform ML engineer, coined by Shreya Shankar in this blog post:</p> <ol> <li> <p>Task ML engineers are responsible for maintaining specific ML pipelines. They only focus on ensuring that these ML models are healthy and updated frequently. They are often overburdened.</p> </li> <li> <p>Platform ML engineers help task ML engineers automate tedious parts of their jobs. They are called MLOps/ML Platform engineers in our parlance.</p> </li> </ol>"},{"location":"course/2022/lecture-8-teams-and-pm/#2-hiring","title":"2 - Hiring","text":""},{"location":"course/2022/lecture-8-teams-and-pm/#the-ai-talent-gap","title":"The AI Talent Gap","text":"<p>In 2018 (when we started FSDL), the AI talent gap was the main story. There were so few people who understood this technology, so the biggest block for organizations was that they couldn't find people who were good at ML.</p> <p>In 2022, the AI talent gap persists. But it tends to be less of a blocker than it used to be because we have had four years of folks switching careers into ML and software engineers emerging from undergraduate with at least a couple of ML classes under their belts.</p> <p>The gap tends to be in folks that understand more than just the underlying technology but also have experience in seeing how ML fails and how to make ML successful when it's deployed. That's the reality of how difficult it is to hire ML folks today, especially those with production experience.</p>"},{"location":"course/2022/lecture-8-teams-and-pm/#sourcing","title":"Sourcing","text":"<p>Because of this shallow talent pool and the skyrocketing demand, hiring for ML positions is pretty hard. Typical ML roles come in the following structure:</p> <ul> <li> <p>ML Adjacent roles: ML product manager, DevOps, Data Engineer</p> </li> <li> <p>Core ML Roles: ML Engineer, ML Research/ML Scientist</p> </li> <li> <p>Business analytics roles: Data Scientist</p> </li> </ul> <p>For ML-adjacent roles, traditional ML knowledge is less important, as demonstrated interest, conversational understanding, and experience can help these professionals play an impactful role on ML teams. Let's focus on how to hire for the core ML roles.</p> <p></p> <p>While there's no perfect way to hire ML engineers, there's definitely a wrong way to hire them, with extensive job descriptions that demand only the best qualifications (seen above). Certainly, there are many good examples of this bad practice floating around.</p> <ul> <li> <p>Rather than this unrealistic process, consider hiring for software engineering skills, an interest in ML, and a desire to learn. You can always train people in the art and science of ML, especially when they come with strong software engineering fundamentals.</p> </li> <li> <p>Another option is to consider adding junior talent, as many recent grads come out with good ML knowledge nowadays.</p> </li> <li> <p>Finally, and most importantly, be more specific about what you need the position and professional to do. It's impossible to find one person that can do everything from full-fledged DevOps to algorithm development.</p> </li> </ul> <p>To hire ML researchers, here are our tips:</p> <ul> <li> <p>Evaluate the quality of publications, over the quantity, with an eye toward the originality of the ideas, the execution, etc.</p> </li> <li> <p>Prioritize researchers that focus on important problems instead of trendy problems.</p> </li> <li> <p>Experience outside academia is also a positive, as these researchers may be able to transition to industry more effectively.</p> </li> <li> <p>Finally, keep an open mind about research talent and consider talented people without PhDs or from adjacent fields like physics, statistics, etc.</p> </li> </ul> <p>To find quality candidates for these roles, here are some ideas for sourcing:</p> <ul> <li> <p>Use standard sources like LinkedIn, recruiters, on-campus recruiting, etc.</p> </li> <li> <p>Monitor arXiv and top conferences and flag the first authors of papers you like.</p> </li> <li> <p>Look for good implementations of papers you like.</p> </li> <li> <p>Attend ML research conferences (NeurIPS, ICML, ICLR).</p> </li> </ul> <p></p> <p>As you seek to recruit, stay on top of what professionals want and make an effort to position your company accordingly. ML practitioners want to be empowered to do great work with interesting data. Building a culture of learning and impact can help recruit the best talent to your team. Additionally, sell sell sell! Talent needs to know how good your team is and how meaningful the mission can be.</p>"},{"location":"course/2022/lecture-8-teams-and-pm/#interviewing","title":"Interviewing","text":"<p>As you interview candidates for ML roles, try to validate your hypotheses of their strengths while testing a minimum bar on weaker aspects. For example, ensure ML researchers can think creatively about new ML problems while ensuring they meet a baseline for code quality. It's essential to test ML knowledge and software engineering skills for all industry professionals, though the relative strengths can vary.</p> <p>The actual ML interview process is much less well-defined than software engineering interviews, though it is modeled off of it. Some helpful inclusions are projects or exercises that test the ability to work with ML-specific code, like take-home ML projects. Chip Huyen's \"Introduction to ML Interviews Book\" is a great resource.</p>"},{"location":"course/2022/lecture-8-teams-and-pm/#finding-a-job","title":"Finding A Job","text":"<p>To find an ML job, you can take a look at the following sources:</p> <ul> <li> <p>Standard sources such as LinkedIn, recruiters, on-campus recruiting, etc.</p> </li> <li> <p>ML research conferences (NeurIPS, ICLR, ICML).</p> </li> <li> <p>Apply directly (remember, there's a talent gap!).</p> </li> </ul> <p>Standing out for competitive roles can be tricky! Here are some tips (in increasing order of impressiveness) that you can apply to differentiate yourself:</p> <ol> <li> <p>Exhibit ML interest (e.g., conference attendance, online course certificates, etc.).</p> </li> <li> <p>Build software engineering skills (e.g., at a well-known software company).</p> </li> <li> <p>Show you have a broad knowledge of ML (e.g., write blog posts synthesizing a research area).</p> </li> <li> <p>Demonstrate ability to get ML projects done (e.g., create side projects, re-implement papers).</p> </li> <li> <p>Prove you can think creatively in ML (e.g., win Kaggle competitions, publish papers).</p> </li> </ol>"},{"location":"course/2022/lecture-8-teams-and-pm/#3-organizations","title":"3 - Organizations","text":""},{"location":"course/2022/lecture-8-teams-and-pm/#organization-archetypes","title":"Organization Archetypes","text":"<p>There exists not yet a consensus on the right way to structure an ML team. Still, a few best practices are contingent upon different organization archetypes and their ML maturity level. First, let's see what the different ML organization archetypes are.</p> <p>Archetype 1 - Nascent and Ad-Hoc ML</p> <ul> <li> <p>These are organizations where no one is doing ML, or ML is done on an ad-hoc basis. Obviously, there is little ML expertise in-house.</p> </li> <li> <p>They are either small-to-medium businesses or less technology-forward large companies in industries like education or logistics.</p> </li> <li> <p>There is often low-hanging fruit for ML.</p> </li> <li> <p>But there is little support for ML projects, and it's challenging to hire and retain good talent.</p> </li> </ul> <p>Archetype 2 - ML R&amp;D</p> <ul> <li> <p>These are organizations in which ML efforts are centered in the R&amp;D arm of the organization. They often hire ML researchers and doctorate students with experience publishing papers.</p> </li> <li> <p>They are larger companies in sectors such as oil and gas, manufacturing, or telecommunications.</p> </li> <li> <p>They can hire experienced researchers and work on long-term business priorities to get big wins.</p> </li> <li> <p>However, it is very difficult to get quality data. Most often, this type of research work rarely translates into actual business value, so usually, the amount of investment remains small.</p> </li> </ul> <p>Archetype 3 - ML Embedded Into Business and Product Teams</p> <ul> <li> <p>These are organizations where certain product teams or business units have ML expertise alongside their software or analytics talent. These ML individuals report up to the team's engineering/tech lead.</p> </li> <li> <p>They are either software companies or financial services companies.</p> </li> <li> <p>ML improvements are likely to lead to business value. Furthermore, there is a tight feedback cycle between idea iteration and product improvement.</p> </li> <li> <p>Unfortunately, it is still very hard to hire and develop top talent, and access to data and compute resources can lag. There are also potential conflicts between ML project cycles and engineering management, so long-term ML projects can be hard to justify.</p> </li> </ul> <p>Archetype 4 - Independent ML Function</p> <ul> <li> <p>These are organizations in which the ML division reports directly to senior leadership. The ML Product Managers work with Researchers and Engineers to build ML into client-facing products. They can sometimes publish long-term research.</p> </li> <li> <p>They are often large financial services companies.</p> </li> <li> <p>Talent density allows them to hire and train top practitioners. Senior leaders can marshal data and compute resources. This gives the organizations to invest in tooling, practices, and culture around ML development.</p> </li> <li> <p>A disadvantage is that model handoffs to different business lines can be challenging since users need the buy-in to ML benefits and get educated on the model use. Also, feedback cycles can be slow.</p> </li> </ul> <p>Archetype 5 - ML-First Organizations</p> <ul> <li> <p>These are organizations in which the CEO invests in ML, and there are experts across the business focusing on quick wins. The ML division works on challenging and long-term projects.</p> </li> <li> <p>They are large tech companies and ML-focused startups.</p> </li> <li> <p>They have the best data access (data thinking permeates the organization), the most attractive recruiting funnel (challenging ML problems tends to attract top talent), and the easiest deployment procedure (product teams understand ML well enough).</p> </li> <li> <p>This type of organization archetype is hard to implement in practice since it is culturally difficult to embed ML thinking everywhere.</p> </li> </ul>"},{"location":"course/2022/lecture-8-teams-and-pm/#team-structure-design-choices","title":"Team Structure Design Choices","text":"<p>Depending on the above archetype that your organization resembles, you can make the appropriate design choices, which broadly speaking follow these three categories:</p> <ol> <li> <p>Software Engineer vs. Research: To what extent is the ML team responsible for building or integrating with software? How important are Software Engineering skills on the team?</p> </li> <li> <p>Data Ownership: How much control does the ML team have over data collection, warehousing, labeling, and pipelining?</p> </li> <li> <p>Model Ownership: Is the ML team responsible for deploying models into production? Who maintains the deployed models?</p> </li> </ol> <p>Below are our design suggestions:</p> <p>If your organization focuses on ML R&amp;D:</p> <ul> <li> <p>Research is most definitely prioritized over Software Engineering skills. Because of this, there would potentially be a lack of collaboration between these two groups.</p> </li> <li> <p>ML team has no control over the data and typically will not have data engineers to support them.</p> </li> <li> <p>ML models are rarely deployed into production.</p> </li> </ul> <p>If your organization has ML embedded into the product:</p> <ul> <li> <p>Software Engineering skills will be prioritized over Research skills. Often, the researchers would need strong engineering skills since everyone would be expected to product-ionize his/her models.</p> </li> <li> <p>ML teams generally do not own data production and data management. They will need to work with data engineers to build data pipelines.</p> </li> <li> <p>ML engineers totally own the models that they deploy into production.</p> </li> </ul> <p>If your organization has an independent ML division:</p> <ul> <li> <p>Each team has a potent mix of engineering and research skills; therefore, they work closely together within teams.</p> </li> <li> <p>ML team has a voice in data governance discussions, as well as a robust data engineering function.</p> </li> <li> <p>ML team hands-off models to users but is still responsible for maintaining them.</p> </li> </ul> <p>If your organization is ML-First:</p> <ul> <li> <p>Different teams are more or less research-oriented, but in general, research teams collaborate closely with engineering teams.</p> </li> <li> <p>ML team often owns the company-wide data infrastructure.</p> </li> <li> <p>ML team hands the models to users, who are responsible for operating and maintaining them.</p> </li> </ul> <p>The picture below neatly sums up these suggestions:</p> <p></p>"},{"location":"course/2022/lecture-8-teams-and-pm/#4-managing","title":"4 - Managing","text":""},{"location":"course/2022/lecture-8-teams-and-pm/#managing-ml-teams-is-challenging","title":"Managing ML Teams Is Challenging","text":"<p>The process of actually managing an ML team is quite challenging for four reasons:</p> <ol> <li> <p>Engineering Estimation: It's hard to know how easy or hard an ML project is in advance. As you explore the data and experiment with different models, there is enormous scope for new learnings about the problem that materially impact the timeline. Furthermore, knowing what methods will work is often impossible. This makes it hard to say upfront how long or how much work may go into an ML project.</p> </li> <li> <p>Nonlinear Progress: As the chart below from a blog post by Lukas Biewald (CEO of Weights and Biases) shows, progress on ML projects is unpredictable over time, even when the effort expended grows considerably. It's very common for projects to stall for extended periods of time.</p> </li> </ol> <p></p> <ol> <li> <p>Cultural gaps: The relative culture of engineering and research professionals is very different. Research tends to favor novel, creative ideas, while engineering prefers tried and true methods that work. As a result, ML teams often experience a clash of cultures, which can turn toxic if not appropriately managed. A core challenge of running ML teams is addressing the cultural barriers between ML and software engineering so that teams can harmoniously experiment and deliver ML products.</p> </li> <li> <p>Leadership Deficits: It's common to see a lack of detailed understanding of ML at senior levels of management in many companies. As a result, expressing feasibility and setting the right expectations for ML projects, especially high-priority ones, can be hard.</p> </li> </ol>"},{"location":"course/2022/lecture-8-teams-and-pm/#how-to-manage-ml-teams-better","title":"How To Manage ML Teams Better","text":"<p>Managing ML teams is hardly a solved problem, but you can take steps to improve the process.</p> <p>Plan probabilistically</p> <p>Many engineering projects are managed in a waterfall fashion, with the sequential tasks defined up front clearly. Instead of forcing this method of engineering management on difficult ML projects, try assigning a likelihood of success to different tasks to better capture the experimental process inherent to ML engineering. As these tasks progress or stall, rapidly re-evaluate your task ordering to better match what is working. Having this sense of both (1) how likely a task is to succeed and (2) how important it is makes project planning considerably more realistic.</p> <p></p> <p>Have a portfolio of approaches</p> <p>Embrace multiple ideas and approaches to solve crucial research challenges that gate production ML. Don't make your plan dependent on one approach working!</p> <p>Measure inputs, not results</p> <p>As you work through several approaches in your portfolio, do not overly emphasize whose ideas ultimately work as a reflection of contribution quality. This can negatively impact team members' creativity, as they focus more on trying to find only what they currently think could work, rather than experimenting in a high-quality fashion (which is ultimately what leads to ML success).</p> <p>Have researchers and engineers work together</p> <p>The collaboration between engineering and research is essential for quality ML products to get into production. Emphasize collaboration across the groups and professionals!</p> <p>Get quick wins</p> <p>Taking this approach makes it more likely that your ML project will succeed in the long term. It allows you to demonstrate progress to your leadership more effectively and clearly.</p> <p>Educate leadership on uncertainty</p> <p>This can be hard, as leadership is ultimately accountable for addressing blind spots and understanding timeline risk. There are things you can do, however, to help improve leadership's knowledge about ML timelines.</p> <ul> <li> <p>Avoid building hype around narrow progress metrics material only to the ML team (e.g., \"We improved F1 score by 0.2 and have achieved awesome performance!\").</p> </li> <li> <p>Instead, be realistic, communicate risk, and emphasize real product impact (e.g., \"Our model improvements should increase the number of conversions by 10%, though we must continue to validate its performance on additional demographic factors.)</p> </li> <li> <p>Sharing resources like this a16z primer, this class from Prof. Pieter Abbeel, and this Google's People + AI guidebook can increase awareness of your company's leadership.</p> </li> </ul>"},{"location":"course/2022/lecture-8-teams-and-pm/#ml-pms-are-well-positioned-to-educate-the-organization","title":"ML PMs are well-positioned to educate the organization","text":"<p>There are two types of ML product managers.</p> <ol> <li> <p>Task PMs: These are the more common form of ML PM. They are generally specialized into a specific product area (e.g. trust and safety) and have a strong understanding of the particular use case.</p> </li> <li> <p>Platform PMs: These are a newer form of PMs. They have a broader mandate to ensure that the ML team (generally centralized in this context) is highest leverage. They manage workflow and priorities for this centralized team. To support this, they tend to have a broad understanding of ML themselves. These PMs are critical for educating the rest of the company about ML and ensuring that teams trust the output of models.</p> </li> </ol> <p>Both types of PMs are crucial for ML success. Platform PMs tend to have a particularly powerful role to play in pushing an organization's adoption of machine learning and making it successful.</p>"},{"location":"course/2022/lecture-8-teams-and-pm/#what-is-agile-for-ml","title":"What is \"Agile\" for ML?","text":"<p>There are two options similar to what Agile is for software development in the ML context. They are shown below:</p> <p></p> <p>They are both structured, data-science native approaches to project management. You can use them to provide standardization for project stages, roles, and artifacts.</p> <p>TDSP tends to be more structured and is a strong alternative to the Agile methodology. CRISP-DM is somewhat higher level and does not provide as structured a project management workflow. If you genuinely have a large-scale coordination problem, you can try these frameworks, but don't otherwise. They can slow you down since they are more oriented around \"traditional\" data science and not machine learning.</p>"},{"location":"course/2022/lecture-8-teams-and-pm/#5-design","title":"5 - Design","text":"<p>Let's talk about how to actually design machine learning products now. The biggest challenge with designing such products often isn't implementing them; it's bridging the gap between users' inflated expectations and the reality.</p> <p>Users often expect extremely sophisticated systems capable of solving many more problems than they actually can.</p> <p></p> <p>In reality, machine learning systems are more like dogs that are trained to do a special task; weird little guys with a penchant for distraction and an inability to do much more than they are explicitly told.</p> <p></p> <p>All this leads to a big gap between what can be done and what users expect!</p>"},{"location":"course/2022/lecture-8-teams-and-pm/#the-keys-to-good-ml-product-design","title":"The Keys to Good ML Product Design","text":"<p>In practice, good ML product design bridges users expectations and reality. If you can help users understand the benefits and limitations of the model, they tend to be more satisfied. Furthermore, always have backup plans for model failures! Over-automating systems tends to be a recipe for unhappy users. Finally, building in feedback loops can really increase satisfaction over time.</p> <p>There are a couple ways to explain the benefits and limitations of an ML system to users.</p> <ul> <li> <p>Focus on the problems it solves, not the fact that the system is \"AI-powered\".</p> </li> <li> <p>If you make the system feel \"human-like\" (unconstrained input, human-like responses), expect users to treat it as human-like.</p> </li> <li> <p>Furthermore, seek to include guardrails or prescriptive interfaces over open-ended, human-like experiences. A good example of the former approach is Amazon Alexa, which has specific prompts that its ML system responds to.</p> </li> </ul> <p></p> <p>Handling failures is a key part of keeping ML systems users happy. There's nothing worse than a \"smart\" system that conks out when you do something slightly unexpected. Having built-in solutions to solve for automation issues is extremely important. One approach is letting users be involved to correct improper responses. Another is to focus on the notion of \"model confidence\" and only offer responses when the threshold is met. A good example of a handling failure approach is how Facebook recommends photo tags for users, but doesn't go so far as to autoassign.</p>"},{"location":"course/2022/lecture-8-teams-and-pm/#types-of-user-feedback","title":"Types of User Feedback","text":"<p>How can you collect feedback from users in a way that avoids these issues? There are different types of user feedback and how they help with model improvement.</p> <p></p> <p>Let's go across this chart.</p> <ol> <li> <p>The simplest form of feedback is indirect implicit feedback. For example, did the user churn from the product? That tells you immediately how the user felt about the system without them giving a clear signal themselves.</p> </li> <li> <p>Another form is direct implicit feedback, which involves the user \"taking the next step\". For example, in an automated user onboarding flow, did the user click through into ensuing steps? This is trickier to implement, but can be useful for future training iterations.</p> </li> <li> <p>The next type of feedback is binary explicit feedback, wherein users are specifically asked (e.g. via thumbs up/down buttons) how they feel about the model performance.</p> </li> <li> <p>You can make this more sophisticated and add categorical explicit feedback, which allows users to sort their feedback into various types.</p> </li> <li> <p>To really get a sense of how users feel, consider offering free text feedback. This is tricky to use for model training and can be involved for users, but it's very useful to highlight the highest friction predictions.</p> </li> <li> <p>The gold standard, of course, are model corrections; they are free labels!</p> </li> </ol> <p>Whenever building explicit feedback into ML systems, avoid relying on users' altruism and be clear about why they should engage in the feedback. Instead, build positive feedback loops by allowing users to experience the benefits of their feedback quickly.</p> <p>Great ML product experiences are designed from scratch. ML is a very specific technology with clear advantages and drawbacks. Design needs to be thoughtfully executed around these products. It's especially important to allow users to interact safely with ML products that may fail in unexpected ways. Always try to find ways to build in feedback loops to make the ML product better over time.</p> <p>There are tons of resources that can help you get started with this emerging field.</p> <ul> <li> <p>Google's People + AI Guidebook</p> </li> <li> <p>Guidelines for Human-AI Interaction</p> </li> <li> <p>Agency Plus Automation: Designing AI into Interactive Systems</p> </li> <li> <p>Designing Collaborative AI</p> </li> </ul> <p>In conclusion, we talked through a number of adjacent considerations to building ML systems and products. In short, you ship the team as much you do the code; be thoughtful about how you hire, manage, and structure ML teams as much as ML products!</p> <p></p>"},{"location":"course/2022/lecture-9-ethics/","title":"Lecture 9: Ethics","text":"<p>Lecture by Charles Frye. Notes by James Le and Vishnu Rachakonda. Published October 03, 2022. Download slides.</p> <p>In this final lecture of FSDL 2022, we'll talk about ethics. After going through the context of what we mean by ethics, we'll go through three different areas where ethics come up:</p> <ol> <li> <p>Tech Ethics: ethics that anybody who works in the tech industry broadly needs to think about.</p> </li> <li> <p>ML Ethics: what ethics has specifically meant for the ML industry.</p> </li> <li> <p>AI Ethics: what ethics might mean in the future where true AGI exists.</p> </li> </ol>"},{"location":"course/2022/lecture-9-ethics/#1-overview-and-context","title":"1 - Overview and Context","text":"<p>All ethics lectures are wrong, but some are useful. They are more useful if we admit and state what our assumptions or biases are. We'll also talk about three general themes that come up often when ethical concerns are raised in tech/ML: alignment, trade-offs, and humility.</p> <p></p> <p>In this lecture, we'll approach ethics on the basis of concrete cases - specific instances where people have raised concerns. We'll talk about cases where people have taken actions that have led to claims and counter-claims of ethical or unethical behavior - such as the use of automated weapons, the use of ML systems to make decisions like sentencing and bail, and the use of ML algorithms to generate art. In each case when criticism has been raised, part of that criticism has been that the technology is unethical.</p> <p>Approaching ethics in this way allows us to answer the question of \"What is ethics?\" by way of Ludwig Wittgenstein's quote: \"The meaning of a word is its use in the language.\" We'll focus on times when people have used the word \"ethics\" to describe what they like or dislike about a specific technology.</p> <p>If you want to try it out for yourself, you should check out the game \"Something Something Soup Something.\" In this browser game, you are presented with a bunch of dishes and have to decide whether they are soup or not soup, as well as whether they can be served to somebody who ordered soup. By playing a game like this, you'll discover (1) how difficult it is to come up with a concrete definition of soup and (2) how poorly your working definition of soup fits with any given soup theory.</p> <p>Because of this case-based approach, we won't be talking about ethical schools or \"trolley\" problems. Rather than considering these hypothetical scenarios, we'll talk about concrete and specific examples from the past decade of work in our field and adjacent fields.</p> <p></p> <p>If you want another point of view that emphasizes the trolley problems, you should check out Sergey's lecture from the last edition of the course from 2021. It presented similar ideas from a different perspective and came to the same conclusion and some different conclusions.</p> <p>A useful theme from that lecture that we should all have in mind when we ponder ethical dilemmas is \"What Is Water?\" - which came up from a famous commencement speech by David Foster Wallace. If we aren't thoughtful and paying attention, things that are very important can become background, assumptions, and invisible to us.</p> <p>The approach of relying on prominent cases risks replicating social biases. Some ethical claims are amplified and travel more because people (who are involved) have more resources and are better connected. Using these forms of case-based reasoning (where you explain your beliefs in concrete detail) can hide the principles that are actually in operation, making them disappear like water.</p> <p>But in the end, so much of ethics is deeply personal that we can't expect to have a perfect approach. We can just do the best we can and hopefully become better every day.</p>"},{"location":"course/2022/lecture-9-ethics/#2-themes","title":"2 - Themes","text":"<p>We'll see three themes repeatedly coming up throughout this lecture:</p> <ol> <li> <p>Alignment: a conflict between what we want and what we get.</p> </li> <li> <p>Trade-Offs: a conflict between what we want and what others want.</p> </li> <li> <p>Humility: a response when we don't know what we want or how to get it.</p> </li> </ol>"},{"location":"course/2022/lecture-9-ethics/#alignment","title":"Alignment","text":"<p>The problem of alignment (where what we want and what we get differ) come up over and over again. A primary driver of this is called the proxy problem - in which we often optimize or maximize some proxies for the thing that we really care about. If the alignment (or loosely the correlation between that proxy and the thing we care about) is poor enough, then by trying to maximize that proxy, we can end up hurting the thing we originally cared about.</p> <p></p> <p>There was a recent paper that did a mathematical analysis of this idea. You can see these kinds of proxy problems everywhere once you look for them.</p> <ul> <li> <p>On the top right, we have a train and validation loss chart from one of the training runs for the FSDL text recognizer. The thing we can optimize is the training loss. That's what we can use to calculate gradients and improve the parameters of our network. But the thing we really care about is the performance of the network on data points that it has not seen (like the validation set, the test set, or data in production). If we optimize our training loss too much, we can actually cause our validation loss to go up.</p> </li> <li> <p>Similarly, there was an interesting paper suggesting that increasing your accuracy on classification tasks can actually result in a decrease in the utility of your embeddings in downstream tasks.</p> </li> <li> <p>You can find these proxy problems outside of ML as well. This thread reveals an example where a factory that was making chemical machines (rather than creating a machine that was cheaper and better) chose not to adopt producing that machine because their output was measured in weight. So the thing that the planners actually cared about, economic efficiency and output, was not optimized because it was too difficult to measure.</p> </li> </ul> <p>One reason why these kinds of proxy problems arise so frequently is due to issues of information. The information that we are able to measure is not the information that we want. At a higher level, we often don't know what it is that we truly needed. We may want the validation loss, but what we need is the loss in production or really the value our users will derive from this model.</p>"},{"location":"course/2022/lecture-9-ethics/#trade-offs","title":"Trade-Offs","text":"<p>Even when we know what we want or what we need, we are likely to run into the second problem - the tradeoff between stakeholders. It is sometimes said that the need to negotiate tradeoffs is one of the reasons why engineers do not like thinking about some of these problems around ethics. That's not quite right because we do accept tradeoffs as a key component of engineering.</p> <ul> <li> <p>In this O'Reilly book on the fundamentals of software architecture, the first thing they state at the beginning is that everything in software architecture is a tradeoff.</p> </li> <li> <p>This satirical O'Reilly book says that every programming question has the answer: \"It depends.\"</p> </li> </ul> <p></p> <p>The famous chart above compares the different convolutional networks on the basis of their accuracy and the number of operations to run them. Thinking about these tradeoffs between speed and correctness is exactly the thing we have to do all the time in our job as engineers.</p> <p>We can select the Pareto Front for the metrics we care about. A way to remember what a Pareto front is this definition of a data scientist from Josh Wills: \"Person who is better at statistics than any software engineer and better at software engineering than any statistician.\" The Pareto Front in the chart above includes the models that are more accurate than those with fewer FLOPs and use fewer FLOPs than those that are more accurate.</p> <p>A reason why engineers may dislike thinking about these problems is that it's hard to identify and quantify these tradeoffs. These are indeed proxy problems. Even further, once measured, where on that front do we fall? As engineers, we may develop expertise in knowing whether we want high accuracy or low latency, but we are not as comfortable deciding how many current orphans we want to trade for what amount of future health. This raises questions both in terms of measurement and decision-making that are outside of our expertise.</p>"},{"location":"course/2022/lecture-9-ethics/#humility","title":"Humility","text":"<p>The appropriate response is humility because most engineers do not explicitly train in these skills. Many engineers and managers in tech, in fact, constitutionally prefer optimizing single metrics that are not proxies. Therefore, when encountering a different kind of problem, it's important to bring a humble mindset, ask for help from experts, and recognize that the help you get might not be immediately obvious to what you are used to.</p> <p>Additionally, when intervening due to an ethical concern, it's important to remember this humility. It's easy to think that when you are on the good side, this humility is not necessary. But even trying to be helpful is a delicate and dangerous undertaking. We want to make sure that as we resolve ethical concerns, we come up with solutions that are not just parts of the problem.</p>"},{"location":"course/2022/lecture-9-ethics/#user-orientation-undergirds-each-theme","title":"User Orientation Undergirds Each Theme","text":"<p>We can resolve all of these via user orientation.</p> <ol> <li> <p>By getting feedback from users, we maintain alignment between our system and our users.</p> </li> <li> <p>When making tradeoffs, we should resolve them in consultation with users.</p> </li> <li> <p>Humility means we actually listen to our users because we recognize we don't have the answers to all the questions.</p> </li> </ol>"},{"location":"course/2022/lecture-9-ethics/#3-tech-ethics","title":"3 - Tech Ethics","text":"<p>The tech industry can't afford to ignore ethics as public trust in tech declines. We need to learn from other nearby industries that have done a better job on professional ethics. We'll also touch on some contemporary topics.</p>"},{"location":"course/2022/lecture-9-ethics/#tech-industrys-ethical-crisis","title":"Tech Industry's Ethical Crisis","text":"<p>Throughout the past decade, the tech industry has been plagued by scandal - whether that's how tech companies interface with national governments at the largest scale or how tech systems are being used or manipulated by people who create disinformation or fake social media accounts that hack the YouTube recommendation system.</p> <p>As a result, distrust in tech companies has risen markedly in the last ten years. This Public Affairs Pulse survey shows that in 2013, the tech industry was one of the industries with less trustworthiness on average. In 2021, it has rubbed elbows with famously more distrusted industries such as energy and pharmaceuticals.</p> <p></p> <p>Politicians care quite a bit about public opinion polls. In the last few years, the fraction of people who believe that large tech companies should be more regulated has gone up a substantial amount. Comparing it to 10 years ago, it's astronomically higher. So there will be a substantial impact on the tech industry due to this loss of public trust.</p> <p>We can learn from nearby fields: from the culture of professional ethics in engineering in Canada (by wearing the Iron Ring) to ethical standards for human subjects research (Nuremberg Code, 1973 National Research Act). We are at the point where we need a professional code of ethics for software. Hopefully, many codes of ethics developed in different communities can compete with each other and merge into something that most of us can agree on. That can be incorporated into our education for new members of our field.</p> <p>Let's talk about two particular ethical concerns that arise in tech in general: carbon emissions and dark/user-hostile design patterns.</p>"},{"location":"course/2022/lecture-9-ethics/#tracking-carbon-emissions","title":"Tracking Carbon Emissions","text":"<p>Because carbon emissions scale with cost, you only need to worry about them when the costs of what you are working on are very large. Then you won't be alone in making these decisions and can move a bit more deliberately to make these choices more thoughtfully.</p> <p>Anthropogenic climate change from carbon emissions raises ethical concerns - tradeoffs between the present and future generations. The other view is that this is an issue that arises from a classic alignment problem: many organizations are trying to maximize their profit, which is based on prices for goods that don't include externalities (such as environmental damage caused by carbon emissions, leading to increased temperatures and lactic change).</p> <p></p> <p>The primary dimension along which we have to worry about carbon emissions is in compute jobs that require power. That power can result in carbon emissions. This paper walks through how much carbon dioxide was emitted using typical US-based cloud infrastructure.</p> <ul> <li> <p>The top headline shows that training a large Transformer model with neural architecture search produces as much carbon dioxide as five cars create during their lifetimes.</p> </li> <li> <p>It's important to remember that power is not free. On US-based cloud infrastructure, \\$10 of cloud spent is roughly equal to \\$1 of air travel costs. That's on the basis of something like the numbers and the chart indicating air travel across the US from New York to San Francisco.</p> </li> <li> <p>Just changing cloud regions can actually reduce your emissions quite a bit. There's a factor of 50x from regions with the most to least carbon-intensive power generation.</p> </li> </ul> <p>The interest in this problem has led to new tools. Codecarbon.io allows you to track power consumption and reduce carbon emissions from your computing. ML CO2 Impact is oriented directly towards machine learning.</p>"},{"location":"course/2022/lecture-9-ethics/#deceptive-design-and-dark-patterns","title":"Deceptive Design and Dark Patterns","text":"<p>The other ethical concern in tech is deceptive design. An unfortunate amount of deception is tolerated in some areas of software. As seen below, on the left is a nearly complete history of the way Google displays ads in its search engine results. It started off very clearly colored and separated out with bright colors from the rest of the results. Then about ten years ago, that colored background was removed and replaced with a tiny little colored snippet that said \"Ad.\" Now, as of 2020, that small bit is no longer even colored. It is just bolded. This makes it difficult for users to know which content is being served to them because somebody paid for it (versus content served up organically).</p> <p></p> <p>A number of dark patterns of deceptive design have emerged over the last ten years. You can read about them on the website called deceptive.design. There's also a Twitter account called \\@darkpatterns that shares examples found in the wild.</p> <p>A practice in the tech industry that's on a very shaky ethical /legal ground is growth hacking. This entails a set of techniques for achieving rapid growth in user base or revenue for a product and has all the connotations you might expect from the name - with examples including LinkedIn and Hotmail.</p> <p></p> <p>ML can actually make this problem worse if we optimize short-term metrics. These growth hacks and deceptive designs can often drive user and revenue growth in the short term but worsen user experience and draw down on goodwill towards the brand in a way that can erode the long-term value of customers. When we incorporate ML into the design of our products with A/B testing, we have to watch out to make sure that the metrics that we are optimizing do not encourage this kind of deception.</p> <p>These arise inside another alignment problem. One broadly-accepted justification for the private ownership of the means of production is that private enterprise delivers broad social value aligned by price signals and market focus. But these private enterprises optimize metrics that are, at best, a proxy for social value. There's the possibility of an alignment problem where companies pursuing and maximizing their market capitalization can lead to net negative production of value. If you spend time at the intersection of funding, leadership, and technology, you will encounter it.</p> <p></p> <p>In the short term, you can push for longer-term thinking within your organization to allow for better alignment between metrics and goals and between goals and utility. You can also learn to recognize user-hostile designs and advocate for user-centered design instead.</p> <p>To wrap up this section on tech ethics:</p> <ol> <li> <p>The tech industry should learn from other disciplines if it wants to avoid a trust crisis.</p> </li> <li> <p>We can start by educating ourselves about common deceptive or user-hostile practices in our industry.</p> </li> </ol>"},{"location":"course/2022/lecture-9-ethics/#4-ml-ethics","title":"4 - ML Ethics","text":"<p>The ethical concerns raised about ML have gone beyond just the ethical questions about other kinds of technology. We'll talk about common ethical questions in ML and lessons learned from Medical ML.</p>"},{"location":"course/2022/lecture-9-ethics/#why-not-just-tech-ethics","title":"Why Not Just Tech Ethics?","text":"<p>ML touches human lives more intimately than other technologies. Many ML methods, especially deep neural networks, make human-legible data into computer-legible data. Humans are more sensitive to errors and have more opinions about visual and text data than they do about the type of data manipulated by computers. As a result, there are more stakeholders with more concerns that need to be traded off in ML applications.</p> <p>Broadly speaking, ML involves being wrong pretty much all the time. Our models are statistical and include \"randomness.\" Randomness is almost always an admission of ignorance. As we admit a certain degree of ignorance in our models, our models will be wrong and misunderstand situations that they are put into. It can be upsetting and even harmful to be misunderstood by our models.</p> <p>Against this backlash of greater interest or higher stakes, a number of common types of ethical concerns have coalesced in the last couple of years. There are somewhat established camps of answers to these questions, so you should at least know where you stand on the four core questions:</p> <ol> <li> <p>Is the model \"fair\"?</p> </li> <li> <p>Is the system accountable?</p> </li> <li> <p>Who owns the data?</p> </li> <li> <p>Should the system be built at all?</p> </li> </ol>"},{"location":"course/2022/lecture-9-ethics/#common-ethical-questions-in-ml","title":"Common Ethical Questions in ML","text":""},{"location":"course/2022/lecture-9-ethics/#is-the-model-fair","title":"Is The Model \"Fair\"?","text":"<p>The classic case on this comes from criminal justice with the COMPAS system for predicting whether a defendant will be arrested again before trial. If they are arrested again, that suggests they committed a crime during that time. This assesses a certain degree of risk for additional harm while the justice system decides what to do about a previous arrest and potential crime.</p> <p>The operationalization here was a 10-point re-arrest probability based on past data about this person, and they set a goal from the very beginning to be less biased than human judges. They operationalize that by calibrating these arrest probabilities across subgroups. Racial bias is a primary concern in the US criminal justice system, so they took care to make sure that these probabilities of re-arrest were calibrated for all racial groups.</p> <p></p> <p>The system was deployed and used all around the US. It's proprietary and difficult to analyze. But using the Freedom of Information Act and coalescing together a bunch of records, people at ProPublica were able to run their own analysis of this algorithm. They determined that the model was not more or less wrong for one racial group or another. It tended to have more false positives for Black defendants and more false negatives for White defendants. So despite the creators of COMPAS taking into account bias from the beginning, they still ended up with an algorithm with this undesirable property of being more likely to falsely accuse Black defendants than White defendants.</p> <p>It turned out that some quick algebra revealed that some form of race-based bias is inevitable in this setting, as indicated in this paper. There are a large number of fairness definitions that are mutually incompatible. This tutorial by Arvind Narayanan is an excellent one to display them.</p> <p>It is noteworthy that the impact of \"unfairness\" is not fixed. The story is often presented as \"no matter what, the journalists would have found something to complain about.\" But note that equalizing false positive rates and positive predictive value across groups would lead to a higher false negative rate for Black defendants relative to White defendants. In the context of American politics, that's not going to lead to complaints from the same people.</p> <p></p> <p>This is the story about the necessity of confronting the tradeoffs that will inevitably come up. Researchers at Google made a nice little tool where you can think through and make these tradeoffs for yourself. It's helpful for building intuition on these fairness metrics and what it means to pick one over the other.</p> <p>Events in this controversy kicked off a flurry of research on fairness. The Fairness, Accountability, and Transparency conference has been held for several years. There has been a ton of work on both algorithmic-level approaches on measuring and incorporating fairness metrics into training and qualitative work on designing systems that are more transparent and accountable.</p> <p>In the case of COMPAS, re-arrest is not the same as recidivism. Being rearrested requires that a police officer believes you committed a crime. Police officers are subject to their own biases and patterns of policing, which result in a far higher fraction of crimes being caught for some groups than for others. Our real goal, in terms of fairness and criminal justice, might be around reducing those kinds of unfair impacts and using past rearrest data that have these issues.</p>"},{"location":"course/2022/lecture-9-ethics/#representation-matters-for-model-fairness","title":"Representation Matters for Model Fairness","text":"<p>Unfortunately, it is easy to make ML-powered tech that fails for minoritized groups. For example, off-the-shelf computer vision tools often fail on darker sins (as illustrated in this talk by Joy Buolamwini). This is not a new issue in technology, just a more salient one with ML.</p> <p>There has been a good amount of progress on this in the last five years. An example is Google's Model Cards which show how well a model will perform on human subgroups of interest. HuggingFace has good integrations for creating these kinds of model cards.</p> <p>When you invite people for talks or hire people to join your organizations, you should work to reduce the bias of that discovery process by diversifying your network. Some good resources include Black in AI, Diversify Tech Job Board, Women in Data Science, and the You Belong in AI podcast. You can make professional connections via them to improve the representation of minoritized groups in the engineering, design, and product management process.</p>"},{"location":"course/2022/lecture-9-ethics/#is-the-system-accountable","title":"Is The System Accountable?","text":"<p>At a broader level than fairness, we should expect \"accountability\" from ML systems. Some societies and states, including the EU, consider \"the right to an explanation\" in the face of important judgments to be a part of human rights.</p> <p>In the GDPR act, there is a section that enshrines accountability. This isn't quite a totally new requirement; credit denials in the US have been required to be explained since 1974. People have a right to know what and why into making decisions for them!</p> <p>If you want to impose this \"accountability\" on a deep neural network and understand its selections, there are a number of methods that use the input-output gradient to explain the model. You can see a list of several methods in order of increasing performance below (from this paper). These approaches don't quite have strong theoretical underpinnings or a holistic explanation, and are not that robust as a result. A lot of these methods act primarily as edge detectors. The paper shows how even randomizing layers in a model does not materially change the interpretability output of GradCAM methods.</p> <p></p> <p>As a result, introspecting DNNs effectively requires reverse engineering the system to really understand what is going on, largely thanks to efforts like Distil and Transfomer Circuits.</p> <p>Due to these technical challenges, machine learning systems are prone to unaccountability that impacts most those least able to understand and influence their outputs. Books such as Automating Inequality describe the impacts of these systems. In such a context, you should seek to question the purpose of model, involve those impacted by the decisions (either through direct human inputs or through other means), and ensure that equal attention is paid to benefits and harms of automation.</p>"},{"location":"course/2022/lecture-9-ethics/#who-owns-the-data","title":"Who Owns The Data?","text":"<p>Humans justifiably feel ownership of the data they create, which is subsequently used to train machine learning models. Large datasets used to train models like GPT-3 are created by mining this data without the explicit involvement of those who create the data. Many people are not aware that this is both possible and legal. As technology has changed, what can be done with data has changed.</p> <p>You can even verify if your data has been used to train models on. Some of these images are potentially obtained illegally, as a result of sensitive data being posted openly without the recorded consent of the originator.</p> <p></p> <p>Each of these controversies around image generation and illegal data has opened up a new frontier in data governance. Focus will be placed on ensuring new ML systems are sensitive to personal and professional concerns of those who generate the data ML systems are trained on. Emad Mostaque, CEO of Stability AI, has gone so far as to offer future opt out systems from systems similar to Stable Diffusion.</p> <p>Here are some practical tips: Dataset cards can be helpful in providing documentation in a similar fashion to model cards. There are also ethics lists, like the deon ethic checklist that helps design proper systems. Deon also has a helpful list of failure cases.</p>"},{"location":"course/2022/lecture-9-ethics/#should-this-be-built-at-all","title":"Should This Be Built At All?","text":"<p>The undercurrent behind this conversation is the justifiable question of whether some of these systems should be built at all, let alone in an ethical way.</p> <p>ML-powered weaponry is the canonical example here, which is already in use. The definition of these systems are blurry, as both systems old and new have had various autonomous capacities. This is difficult to get a sense of due to the secrecy associated with weapon systems.</p> <p>Some have argued that \"autonomous weapons\" have existed for hundreds of years, but even this does not mean that they are ethical. Mines are good examples of these systems. Movements like the Campaign Against Killer Robots are trying to prevent the cycle we entered with mines - where we invented them, when we realized the incredible harm, and why we are trying to ban them. Why invent these at all?</p> <p>Let's wrap up this entire section with some closing questions that you should always have a thoughtful answer to as you build a machine learning system.</p> <ol> <li> <p>Is the model \"fair\"? Fairness is possible, but requires trade-offs.</p> </li> <li> <p>Is the system accountable? Accountability is easier than interpretability.</p> </li> <li> <p>Who owns the data? Answer this upfront. Changes are on the way.</p> </li> <li> <p>Should the system be built at all? Repeatedly ask this and use it to narrow scope.</p> </li> </ol>"},{"location":"course/2022/lecture-9-ethics/#what-can-we-learn-from-medical-ml","title":"What Can We Learn from Medical ML","text":"<p>Note: The FSDL team would like to thank Dr. Amir Ashraf Ganjouei for his feedback on this section.</p> <p>Interestingly, medicine can teach us a lot about how to apply machine learning in a responsible way. Fundamentally, this has led to a mismatch between how medicine works and how machine learning systems are built today.</p> <p>Let's start with a startling fact: the machine learning response to COVID-19 was an abject failure. In contrast, the biomedical response was a major triumph. For example, the vaccines were developed with tremendous speed and precision.</p> <p></p> <p>Machine learning did not acquit itself well with the COVID-19 problem. Two reviews (Roberts et al., 2021 and Wynants et al., 2020-2022) found that nearly all machine learning models were insufficiently documented, had little to no external validation, and did not follow model development best practices. A full 25% of the papers used a dataset incorrect for the task, which simply highlighted the difference between children and adults, not pneumonia and COVID.</p> <p>Medicine has a strong culture of ethics that professionals are integrated into from the point they start training. Medical professionals take the Hippocratic oath of practicing two things: either help or do not harm the patient. In contrast, the foremost belief associated with software development tends to be the infamous \"Move fast and break things.\" While this approach works for harmless software like web apps, it has serious implications for medicine and other more critical sectors. Consider the example of a retinal implant that was simply deprecated by developers and left hundreds without sight in this Statnews article.</p> <p></p> <p>Researchers are drawing inspiration from medicine to develop similar standards for ML.</p> <ul> <li> <p>For example, clinical trial standards have been extended to ML. These standards were developed through extensive surveys, conferences, and consensus building (detailed in these papers).</p> </li> <li> <p>Progress is being made in understanding how this problem presents. A recent study found that while clinical activities are generally performed at a high compliance level, statistical and data issues tend to suffer low compliance.</p> </li> <li> <p>New approaches are developing entire \"auditing\" procedures that exquisitely identify the activities required to effectively develop models.</p> </li> </ul> <p>Like medicine, machine learning is intimately intertwined with people's lives. The most important question to ask is \"Should this system be built at all?\". Always ask yourselves this and understand the implications!</p>"},{"location":"course/2022/lecture-9-ethics/#5-ai-ethics","title":"5 - AI Ethics","text":"<p>AI ethics are a frontier in both the technology and the ethics worlds. False claims and hype are the most pressing concerns, but other risks could present themselves soon.</p>"},{"location":"course/2022/lecture-9-ethics/#ai-snake-oils","title":"AI Snake Oils","text":"<p>False claims outpace the performance of AI. This poses a serious threat to adoption and satisfaction with AI systems long term.</p> <ul> <li> <p>For example, if you call something \"AutoPilot\", people might truly assume it is fully autonomous, as happened in the below case of a Tesla user. This goes back to our discussion about how AI systems are more like funky dogs than truly human intelligent systems.</p> </li> <li> <p>Another example of this is IBM's Watson system, which went from tackling the future of healthcare to being sold off for parts.</p> </li> </ul> <p></p> <p>These false claims tend to be amplified in the media. But this isn't confined to traditional media. Even Geoff Hinton, a godfather of modern machine learning, has been a little too aggressive in his forecasts for AI performance!</p> <p>You can call this \"AI Snake Oil\" as Arvind Narayanan does in his Substack and talk.</p> <p>Let's separate out where true progress has been made versus where progress is likely to be overstated. On some level, AI perception has seen tremendous progress, AI judgment has seen moderate progress, and AI prediction of social outcomes has seen not nearly as much progress.</p> <p></p>"},{"location":"course/2022/lecture-9-ethics/#frontiers-ai-rights-and-x-risk","title":"Frontiers: AI Rights and X-Risk","text":"<p>There's obvious rationale that should artificial sentient beings exist, tremendous ethical implications would be raised. Few people believe that we are truly on the precipice of sentient beings, but there is disagreement on how close we are.</p> <p></p> <p>There's a different set of concerns around how to regard self-improving intelligent beings, for which there is already evidence. Large Language Models have been show to be able to improve themselves in a range of studies (here and here).</p> <p>Failing to pursue this technology would lead to a huge opportunity cost (as argued by Nick Bostrom)! There truly is a great opportunity in having such systems help us sold major problems and lead better lives. The key though, is that such technology should be developed in the safest way possible, not the fastest way.</p> <p>The paperclip problem shows how the potential for misalignment between AI systems and humans could dramatically reduce human utility and even compromise our interests. Imagine a system designed to manufacture paperclips... could actually develop the intelligence to alter elements of society to favor paper clips?! This thought experiments illustrates how self-learning systems could truly change our world for the worse in a misaligned way.</p> <p>These ideas around existential risk are most associated with the Effective Altruism community. Check out resources like Giving What We Can and 80,000 Hours if you're interested!</p>"},{"location":"course/2022/lecture-9-ethics/#6-what-is-to-be-done","title":"6 - What Is To Be Done?","text":"<p>This course can't end on a dour a note as existential risk. What can be done to mitigate these consequences and participate in developing truly ethical AI?</p> <ol> <li> <p>The first step is to educate yourself on the topic. There are many great books that give lengthy, useful treatment to this topic. We recommend Automating Inequality, Weapons of Math Destruction, and The Alignment Problem.</p> </li> <li> <p>After reading this, consider how to prioritize your actions. What do you want to impact? When do you want to do that? Place them in this two-by-two to get a sense of where their importance is.</p> </li> </ol> <p></p> <p>Ethics cannot be purely negative. We do good, and we want to prevent bad! Focus on the good you can do and be mindful of the harm you can prevent.</p> <p>Leading organizations like DeepMind and OpenAI are leading from the front. Fundamentally, building ML well aligns with building ML for good. All the leading organizations emphasize effective and responsible best practices for building ML powered practices. Keep all this in mind as you make the world a better place with your AI-powered products!</p>"},{"location":"course/2022/project-showcase/","title":"Project Showcase","text":"<p>Students who registered for the synchronous version of the course formed teams and worked on their own deep learning-powered products.</p> <p>Whether you're looking for your next startup idea or deciding how to improve your portfolio, we hope these projects inspire you to build something real with DNNs!</p> <p>Info</p> <p>Many of these projects were made possible thanks to a generous donation of GPU-accelerated compute infrastructure by LambdaLabs. Check them out if you're looking for on-prem or cloud GPU machines!</p> <p>If you're interested in working on full stack projects, join us on Discord and post/ask around about group project work.</p>"},{"location":"course/2022/project-showcase/#course-co-pilot","title":"Course Co-Pilot","text":"<p>An ML powered application for streamlining the process of creating chapter markers and lesson summaries for course content creators.</p> <p>Team: Kurian Benoy, Wayde Gilliam, Suvash Thapaliya Live Demo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#admirer","title":"Admirer","text":"<p>A full-stack ML-powered website that utilizes users\u2019 webcam feeds to answer open-ended questions requiring outside knowledge.</p> <p>Team: Andrew Hinh Live Demo.</p>"},{"location":"course/2022/project-showcase/#green-screen-image-composition-transfer","title":"Green-Screen Image Composition-Transfer","text":"<p>An ML-powered app for adding (optionally Stable Diffusion-generated) virtual backgrounds to images that uses style transfer to match lighting anad composition.</p> <p>Team: Nitin Kishore Sai Samala Live Demo. Poster.</p>"},{"location":"course/2022/project-showcase/#weak-supervision-and-active-learning-with-text-data","title":"Weak Supervision and Active Learning with Text Data","text":"<p>An approach to minimise human labelling for text classification tasks.</p> <p>Team: Aleks Hiidenhovi, Bernardo Garc\u00eda, Diego Quintana, Edoardo Abati, Juan Manuel, Kushal Ramaiya GitHub Repo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#x-ray-diagnosis-ai-assistant","title":"X-Ray Diagnosis AI Assistant","text":"<p>An interface to support medical practitioners in diagnosing and interpreting x-ray images.</p> <p>Team: Arun Hegde, Samarth Keshari, Amulya Badal, Ross Cheung, Seyhan Karakulak GitHub Repo.</p>"},{"location":"course/2022/project-showcase/#moms-ai-food-logger","title":"Mom's AI Food Logger","text":"<p>An app for my mom that automatically identifies and tracks the food she eats.</p> <p>Team: Prince Javier Live Demo.</p>"},{"location":"course/2022/project-showcase/#archaeological-feature-detector","title":"Archaeological Feature Detector","text":"<p>A prototype web app to help archaeologists interpret automatically detected objects as part of a machine-learning-powered survey workflow.</p> <p>Team: Philanoe, jmmoreu, Kemp, lakillo Slide Deck.</p>"},{"location":"course/2022/project-showcase/#semantic-search-engine-for-images","title":"Semantic Search Engine for Images","text":"<p>A semantic text search engine over images, along with monitoring.</p> <p>Team: Sandhya Govindaraju, Utkarsh Vardhan, Gabriella Chiribau, Amit Kumar Sah Live Demo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#recipewise","title":"Recipewise","text":"<p>An image to recipe food classifier.</p> <p>Team: Carlo David, Chavo Kim, George Loh, Nari Jeong, and Rina Buoy Live Demo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#imagein","title":"ImageIN","text":"<p>A pragmatic approach to identifying illustrated pages in digitised historic books.</p> <p>Team: Frank Smitskamp, Zacharie Bouhnik, Daniel van Strien Live Demo. GitHub Repo.</p>"},{"location":"course/2022/project-showcase/#full-stack-stable-diffusion","title":"Full Stack Stable Diffusion","text":"<p>A deployment of Stable Diffusion Text-to-Image and Image-to-Image pipelines with a full stack architecture.</p> <p>Team: Okan Ulusoy and Omid Abdollahi Aghdam GitHub Repo.</p>"},{"location":"course/2022/project-showcase/#multimodal-fusion-models-for-healthcare","title":"Multimodal Fusion Models for Healthcare","text":"<p>An architecture for using multiple modalities of healthcare data to train deep learning models.</p> <p>Team: Vinod Nair, Khoa Nguyen, Carlos Leyson, Kieran Didi, Sridhar Iyer, Alan Chang GitHub Repo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#fiberdiametercom","title":"FiberDiameter.com","text":"<p>Measure the diameter of nanofibers in microscopy images.</p> <p>Team: <code>@f_cossio</code>, <code>@yael_su</code>, <code>@__coszio</code>, <code>@aledelunap</code> Live Demo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#geolocator","title":"GeoLocator","text":"<p>An app that guesses the location of an image, video, or video url.</p> <p>Team: David Hrachovy, Samhita Alla, Yiyi Xu, Winson Truong Live Demo. GitHub Repo.</p>"},{"location":"course/2022/project-showcase/#image-anonymiser","title":"\ud83d\udc7b Image Anonymiser","text":"<p>An ML-powered image anonymisation web app.</p> <p>Team: Sami Saadaoui, Vladislav Vancak, Lawrence Francis, Dan Howarth, Allan Stevenson GitHub Repo. Project Page.</p>"},{"location":"course/2022/project-showcase/#buggingspace","title":"BuggingSpace","text":"<p>An interface for red-teaming open source text generation models from the Hugging Face hub.</p> <p>Team: Sashank Pisupati, Sajenthan Vigneswaran, Kemp Bray, Jean-Antoine Zagato Live Demo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#board-game-rules-explainer","title":"Board Game Rules Explainer","text":"<p>A board game question-answering system to save players from having to check the rulebook.</p> <p>Team: Rafal Wojcik, Tim Jones, Issam Hammi, Muriel Hol Live Demo. GitHub Repo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#gesto-ai-asl-word-recognizer","title":"Gesto AI - ASL Word Recognizer","text":"<p>A real-time, word-level American Sign Language translation app.</p> <p>Team: Shivam Arora, Daniel Firebanks-Quevedo, Pablo Oberhauser, Dhruv Shah, Ibrahim Sherif, Samuel Tang Live Demo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#choosistant","title":"choosistant","text":"<p>Choosistant helps you decide which product to buy by summarizing pros and cons from written reviews.</p> <p>Team: Kimotho, Murad Khalilov, Nam, Omar Ali Sheikh, Sofiane Chami Project Page.</p>"},{"location":"course/2022/project-showcase/#semantic-search-sentiment-analysis","title":"Semantic Search &amp; Sentiment Analysis","text":"<p>Upload a PDF or text document and enable semantic QA and sentiment analysis.</p> <p>Team: Sam Tonetto, Francisco Perez-Sorrosal, Navaneeth Tirupathi, Alexander Chia, Priyam Sadhukhan Project Repo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#in-browser-ai","title":"In-Browser AI","text":"<p>Run modern neural networks directly in your browser from a computer or phone.</p> <p>Team: Alexander Visheratin Live Demo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#audio-vtuber","title":"Audio VTuber","text":"<p>Animate a cartoon with facial expressions using only your voice.</p> <p>Team: Alex Service, Moon Ma Live Demo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#ocr-semsearch","title":"OCR SemSearch","text":"<p>OCR SemSearch allows you to perform semantic search on text within images from different types of documents.</p> <p>Team: Sebastian Gonzalez Aseretto, Paramtap Mewada Project Poster.</p>"},{"location":"course/2022/project-showcase/#live-art-in-context","title":"Live Art in Context","text":"<p>Draw on the creative power of modern ML models to create art responsive to events in text or video streams.</p> <p>Team: David Murphy, Angel Carvajal, Theresa Thoraldson, Chris Lonsberry Slide Deck.</p>"},{"location":"course/2022/project-showcase/#plant-id","title":"Plant ID","text":"<p>A plant species identifier available as a web app and as a cross-platform mobile app.</p> <p>Team: Ben Michel, Navid Matin Moghaddam, Stone Jiang, Shishir Valalla, Vibudh Singh Live Demo. Slide Deck.</p>"},{"location":"course/2022/project-showcase/#landcover-segmentation","title":"Landcover Segmentation","text":"<p>A data product for multi-class semantic segmentation of earth observation images using a UNet architecture.  Team: Suzana, Roland Ritt, Sheebo</p> <p>Slide Deck.</p>"},{"location":"course/2022/project-showcase/#meeting-graph-generator","title":"Meeting Graph Generator","text":"<p>Team: Soroush Bassam Slide Deck. GitHub Repo.</p>"},{"location":"llm-bootcamp/","title":"Full Stack LLM Bootcamp","text":"<p>\ud83d\ude80 Full Stack LLM Bootcamp \ud83d\ude80</p> <ul> <li>Learn best practices and tools for building LLM-powered apps</li> <li>Cover the full stack from prompt engineering to user-centered design <li>Get up to speed on the state-of-the-art</li> Access the materials!"},{"location":"llm-bootcamp/#testimonials","title":"Testimonials","text":""},{"location":"llm-bootcamp/#why","title":"Why","text":"<p>The way AI-powered apps are built has changed:</p> <ul> <li>Before LLMs, an idea would bottleneck on training models from scratch, and then it'd bottleneck again on scalable deployment.</li> <li>Now, a compelling MVP based on pretrained LLM models and APIs can be configured and serving users in an hour.</li> </ul> <p>An entirely new ecosystem of techniques, tools, and tool vendors is forming around LLMs. Even ML veterans are scrambling to orient themselves to what is now possible and figure out the most productive techniques and tools.</p>"},{"location":"llm-bootcamp/#what","title":"What","text":"<p>We put together a two-day program based on emerging best practices and the latest research results to help you make the transition to building LLM apps with confidence.</p> <p>We ran that program as an in-person bootcamp in San Francisco in April 2023. Now, we're releasing the recorded lectures, for free!</p> <ul> <li>\u2728 Learn to Spell: Prompt Engineering and Other Magic</li> <li>\ud83c\udfce\ufe0f LLMOps: Deployment and Learning in Production</li> <li>\ud83e\udd37 UX for Language User Interfaces</li> <li>\ud83d\udd28 Augmented Language Models</li> <li>\ud83d\ude80 Launch an LLM App in One Hour</li> <li>\ud83d\udd2e What's Next?</li> <li>\ud83d\uddff LLM Foundations</li> <li>\ud83d\udc77\u200d\u2642\ufe0f askFSDL Walkthrough</li> </ul> <p>What do I need to know already?</p> <p>The lectures aim to get anyone with experience programming in Python ready to start building applications that use LLMs.</p> <p>Experience with at least one of machine learning, frontend, or backend will be very helpful.</p>"},{"location":"llm-bootcamp/#who","title":"Who","text":"<p>We are Full Stack Deep Learning. We're a team of UC Berkeley PhD alumni with years of industry experience who are passionate about teaching people how to make deep neural networks work in the real world.</p> <p>Since 2018, we have taught in-person bootcamps, online multi-week cohorts, and official semester-long courses at top universities.</p>"},{"location":"llm-bootcamp/#instructor-team","title":"Instructor Team","text":"Charles Frye       educates people in AI. He has worked on AI/ML tooling with Weights &amp; Biases and Gantry since getting a PhD in Theoretical Neuroscience at UC Berkeley. Sergey Karayev       builds AI-powered products as Co-founder of Volition. He co-founded Gradescope after getting a PhD in AI at UC Berkeley.      Josh Tobin         builds tooling for AI products as Co-founder and CEO of Gantry. He worked as a Research Scientist at OpenAI and received a PhD in AI at UC Berkeley."},{"location":"llm-bootcamp/#_1","title":"LLM Bootcamp","text":"<p>If you have any questions about the bootcamp materials, contact <code>admin @ fullstackdeeplearning.com</code>.</p>"},{"location":"llm-bootcamp/expense/","title":"How do I expense the bootcamp?","text":""},{"location":"llm-bootcamp/expense/#submitting-a-receipt","title":"Submitting a receipt","text":"<p>Upon registration, you'll receive an email from our event provider with your ticket and your order details, including the last 4 digits of the credit card you used, if you used one.</p> <p>This is often sufficient documentation.</p>"},{"location":"llm-bootcamp/expense/#verification-of-attendance","title":"Verification of attendance","text":"<p>If you need your attendance verified, please email us at <code>admin@fullstackdeeplearning.com</code> after the event is finished and indicate the address to which we should send a verification email.</p>"},{"location":"llm-bootcamp/expense/#requesting-approval","title":"Requesting approval","text":"<p>Some organizations require prior approval for education expenses from a manager.</p> <p>In that case, you can work off of the email template below. We suggest you customize it to your team and its needs.</p> <p>Keep in mind that you want to make sure that it's clear to your manager why sending you to this bootcamp is in the company's interest, not just yours!</p> <p>An email template, co-written with ChatGPT</p> <p>Dear <code>{manager}</code>,</p> <p>I wanted to bring to your attention a bootcamp that I would love to enroll in. Full Stack Deep Learning is hosting a two-day program that covers the emerging field of application development with Large Language Models (LLMs). FSDL is a respected name that has delivered educational material on productionizing machine learning in formats from large MOOCs to courses at top universities like UC Berkeley and UW.</p> <p>This bootcamp will provide me with the knowledge and skills necessary to build and deploy LLM applications and stay up-to-date with the state-of-the-art in the industry.</p> <p>Some of the benefits:</p> <ul> <li>Coverage of both conceptual fundamentals and concrete engineering practices</li> <li>Talks from leaders in the field, like Harrison Chase of LangChain</li> <li>Opportunities to network with builders</li> </ul> <p>I expect to be able to bring back my learnings and apply them directly to my work and share them with the team.</p> <p>The cost of the bootcamp is <code>{price}</code>. I strongly believe that this investment in my education will benefit the company in the long run.</p> <p>If you are interested, you can find more details about the bootcamp, including the instructors and their bios, at the following link: https://fsdl.me/2023-llmbc-landing.</p> <p>Thank you for considering my request.</p> <p>Best,</p> <p><code>{name}</code></p>"},{"location":"llm-bootcamp/sponsors/","title":"Info for Sponsors","text":"<p>We offer three tiers of sponsorship for the FSDL 2023 LLM Bootcamp:</p> <ol> <li> <p>Vector Tier sponsors receive the following benefits: logo displayed on website and during conference, verbal acknowledgement. Vector Tier sponsorships are available for $1500.</p> </li> <li> <p>Matrix Tier sponsors receive all the benefits of the Vector Tier, plus: logo displayed in between talks, 6' table with two seats, and a conference registration. Matrix Tier sponsorships are available for $4500.</p> </li> <li> <p>Tensor Tier sponsors receive all the benefits of the Matrix Tier, plus: logo displayed on a banner in the registration area, access to an opt-in database of attendees with information about job and job-seeking status, and two additional registrations (for a total of three). Tensor Tier sponsorships are available for $10000.</p> </li> </ol> <p>Contact <code>sponsorships@fullstackdeeplearning.com</code> if you're interested in sponsoring the conference!</p>"},{"location":"llm-bootcamp/spring-2023/","title":"LLM Bootcamp - Spring 2023","text":"<p>What are the pre-requisites for this bootcamp?</p> <p>Our goal is to get you 100% caught up to state-of-the-art and ready to build and deploy LLM apps, no matter what your level of experience with machine learning is.</p> <p>Please enjoy, and email us, tweet us, or post in our Discord if you have any questions or feedback!</p>"},{"location":"llm-bootcamp/spring-2023/#lectures","title":"Lectures","text":""},{"location":"llm-bootcamp/spring-2023/#learn-to-spell-prompt-engineering","title":"Learn to Spell: Prompt Engineering","text":"<ul> <li> High-level intuitions for prompting   <li> Tips and tricks for effective prompting: decomposition/chain-of-thought, self-criticism, ensembling   <li> Gotchas: \"few-shot learning\" and tokenization </li> </li> </li> </ul>"},{"location":"llm-bootcamp/spring-2023/#llmops","title":"LLMOps","text":"<ul> <li> Comparing and evaluating open source and proprietary models   <li> Iteration and prompt management   <li> Applying test-driven-development and continuous integration to LLMs </li> </li> </li> </ul>"},{"location":"llm-bootcamp/spring-2023/#ux-for-language-user-interfaces","title":"UX for Language User Interfaces","text":"<ul> <li>General principles for user-centered design     <li>Emerging patterns in UX design for LUIs     <li>UX case studies: GitHub Copilot and Bing Chat"},{"location":"llm-bootcamp/spring-2023/#augmented-language-models","title":"Augmented Language Models","text":"<ul> <li> Augmenting language model inputs with external knowledge     <li> Vector indices and embedding management systems     <li> Augmenting language model outputs with external tools"},{"location":"llm-bootcamp/spring-2023/#launch-an-llm-app-in-one-hour","title":"Launch an LLM App in One Hour","text":"<ul> <li> Why is now the right time to build?     <li> Techniques and tools for the tinkering and discovery phase: ChatGPT, LangChain, Colab     <li> A simple stack for quickly launching augmented LLM applications"},{"location":"llm-bootcamp/spring-2023/#llm-foundations","title":"LLM Foundations","text":"<ul> <li> Speed-run of ML fundamentals     <li> The Transformer architecture     <li> Notable LLMs and their datasets"},{"location":"llm-bootcamp/spring-2023/#project-walkthrough-askfsdl","title":"Project Walkthrough: askFSDL","text":"<ul> <li>Walkthrough of a GitHub repo for sourced Q&amp;A with LLMs</li> <li>Try it out via a bot in our Discord</li> <li>Python project tooling, ETL/data processing, deployment on Modal, and monitoring with Gantry   </li> </ul>"},{"location":"llm-bootcamp/spring-2023/#whats-next","title":"What's Next?","text":"<ul> <li> Can we build general purpose robots using multimodal models?     <li> Will models get bigger or smaller? Are we running out of data?     <li> How close are we to AGI? Can we make it safe?"},{"location":"llm-bootcamp/spring-2023/#invited-talks","title":"Invited Talks","text":""},{"location":"llm-bootcamp/spring-2023/#reza-shabani-how-to-train-your-own-llm","title":"Reza Shabani: How To Train Your Own LLM","text":"<ul> <li> The \"Modern LLM Stack\": Databricks, Hugging Face, MosaicML, and more   <li> The importance of knowing your data and designing preprocessing carefully   <li> The features of a good LLM engineer   <li> By Reza Shabani, who trained Replit's code completion model, Ghostwriter. </li> </li> </li> </li> </ul>"},{"location":"llm-bootcamp/spring-2023/#harrison-chase-agents","title":"Harrison Chase: Agents","text":"<ul> <li> The \"agent\" design pattern: tool use, memory, reflection, and goals   <li> Challenges facing agents in production: controlling tool use, parsing outputs, handling large contexts, and more   <li> Exciting research projects with agents: AutoGPT, BabyAGI, CAMEL, and Generative Agents   <li> By Harrison Chase, co-creator of LangChain </li> </li> </li> </li> </ul>"},{"location":"llm-bootcamp/spring-2023/#fireside-chat-with-peter-welinder","title":"Fireside Chat with Peter Welinder","text":"<ul> <li> With Peter Welinder, VP of Product &amp; Partnerships at OpenAI   <li> How OpenAI converged on LLMs   <li> Learnings and surprises from releasing ChatGPT </li> </li> </li> </ul>"},{"location":"llm-bootcamp/spring-2023/#sponsors","title":"Sponsors","text":"<p>We are deeply grateful to all of the sponsors who helped make this event happen.</p>"},{"location":"llm-bootcamp/spring-2023/#direct-sponsors","title":"Direct Sponsors","text":""},{"location":"llm-bootcamp/spring-2023/#compute-credit-sponsors","title":"Compute Credit Sponsors","text":""},{"location":"llm-bootcamp/spring-2023/expense/","title":"How do I expense the bootcamp?","text":""},{"location":"llm-bootcamp/spring-2023/expense/#submitting-a-receipt","title":"Submitting a receipt","text":"<p>Upon registration, you'll receive an email from our event provider with your ticket and your order details, including the last 4 digits of the credit card you used, if you used one.</p> <p>This is often sufficient documentation.</p>"},{"location":"llm-bootcamp/spring-2023/expense/#verification-of-attendance","title":"Verification of attendance","text":"<p>If you need your attendance verified, please email us at <code>admin@fullstackdeeplearning.com</code> after the event is finished and indicate the address to which we should send a verification email.</p>"},{"location":"llm-bootcamp/spring-2023/expense/#requesting-approval","title":"Requesting approval","text":"<p>Some organizations require prior approval for education expenses from a manager.</p> <p>In that case, you can work off of the email template below. We suggest you customize it to your team and its needs.</p> <p>Keep in mind that you want to make sure that it's clear to your manager why sending you to this bootcamp is in the company's interest, not just yours!</p> <p>An email template, co-written with ChatGPT</p> <p>Dear <code>{manager}</code>,</p> <p>I wanted to bring to your attention a bootcamp that I would love to enroll in. Full Stack Deep Learning is hosting a two-day program that covers the emerging field of application development with Large Language Models (LLMs). FSDL is a respected name that has delivered educational material on productionizing machine learning in formats from large MOOCs to courses at top universities like UC Berkeley and UW.</p> <p>This bootcamp will provide me with the knowledge and skills necessary to build and deploy LLM applications and stay up-to-date with the state-of-the-art in the industry.</p> <p>Some of the benefits:</p> <ul> <li>Coverage of both conceptual fundamentals and concrete engineering practices</li> <li>Talks from leaders in the field, like Harrison Chase of LangChain</li> <li>Opportunities to network with builders</li> </ul> <p>I expect to be able to bring back my learnings and apply them directly to my work and share them with the team.</p> <p>The cost of the bootcamp is <code>{price}</code>. I strongly believe that this investment in my education will benefit the company in the long run.</p> <p>If you are interested, you can find more details about the bootcamp, including the instructors and their bios, at the following link: https://fsdl.me/2023-llmbc-landing.</p> <p>Thank you for considering my request.</p> <p>Best,</p> <p><code>{name}</code></p>"},{"location":"llm-bootcamp/spring-2023/sponsors/","title":"Info for Sponsors","text":"<p>We offer three tiers of sponsorship for the FSDL 2023 LLM Bootcamp:</p> <ol> <li> <p>Vector Tier sponsors receive the following benefits: logo displayed on website and during conference, verbal acknowledgement. Vector Tier sponsorships are available for $1500.</p> </li> <li> <p>Matrix Tier sponsors receive all the benefits of the Vector Tier, plus: logo displayed in between talks, 6' table with two seats, and a conference registration. Matrix Tier sponsorships are available for $4500.</p> </li> <li> <p>Tensor Tier sponsors receive all the benefits of the Matrix Tier, plus: logo displayed on a banner in the registration area, access to an opt-in database of attendees with information about job and job-seeking status, and two additional registrations (for a total of three). Tensor Tier sponsorships are available for $10000.</p> </li> </ol> <p>Contact <code>sponsorships@fullstackdeeplearning.com</code> if you're interested in sponsoring the conference!</p>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/","title":"Project Walkthrough: askFSDL","text":"<p>Project by Charles Frye. Published May 9, 2023.</p> <p>View the project repository.</p> <p>Interact with the bot on our Discord.</p>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/#swe-tooling-make-precommit-etc","title":"SWE Tooling: make, precommit, etc","text":"<ul> <li>Walked everyone through the code base for the Discord bot they interacted with</li> <li>Sourced question-answering over a corpus of information using Vector storage for retrieval</li> <li>GitHub repo available for this project, but may not be able to execute the code without accounts on all services</li> <li>Makefile created for easier project management, setting up environment and authentication, and running setup commands</li> <li>Incorporated software tools like pre-commit checks, black for Python auto-formatting, and rust-powered formatter</li> <li>Shell check tool useful for catching issues in bash scripts</li> </ul>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/#data-cleaning","title":"Data Cleaning","text":"<ul> <li>Initial approach of scraping data and chunking into smaller pieces did not yield good results</li> <li>Improved results by spending time understanding the data and preserving the structure during processing</li> <li>Extracting textual information from other sources like images and YouTube videos can enhance the usefulness of language models</li> <li>Sometimes simple solutions to specific data sources and problems can greatly improve the quality of results</li> <li>The unglamorous work of getting to know the data and writing code to manage it properly can result in big dividends for language model applications</li> </ul>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/#infrastructure-modal","title":"Infrastructure: Modal","text":"<ul> <li>Discussed the ETL component of extracting, transforming, and loading data from various sources</li> <li>Discussed using Python packages for data transformation and addressing dependency issues with tools like pre-commit</li> <li>Explained the benefits of the modal component in creating lightweight virtual containers for different tasks</li> <li>Modal containers are fast and efficient, aiding in quick development cycles and allowing for containerization without the pains of traditional Docker images</li> <li>Modal also allows for the creation of serverless applications with auto-scaling and resource management</li> <li>Debugging and local development can be done through the interactive mode by connecting to a container running on modal</li> <li>showModal provides an interface for tracking application activity, utilization, and resource allocation, making it a versatile tool for various projects</li> </ul>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/#frontend-gradio-discord","title":"Frontend: Gradio &amp; Discord","text":"<ul> <li>Introduced Gradio user interface, allowing users to create interfaces in pure Python</li> <li>Gradio UI is flexible, supported by Hugging Face, and rapidly adopting machine learning features</li> <li>Examples of Gradio UI use include Alpaca, Flamingo, and Dolly mini</li> <li>Gradio UI is easy to set up, portable, flexible, and comes with an API with OpenAPI spec</li> <li>Discord bot integrated with Python library Discord.py; alternative library Interactions.py is also available</li> <li>Gradio UI is built on FastAPI for asynchronous Python web service</li> <li>Application mainly runs on the model's infrastructure in containers, serving traffic as needed</li> </ul>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/#embeddings-etl","title":"Embeddings &amp; ETL","text":"<ul> <li>Used OpenAI's ada002 model to generate embeddings, which are much cheaper than generation endpoints</li> <li>Currently using a vector index for data storage, but considering adding additional types of search</li> <li>Discussed processing PDFs in a previous lecture, mentioned using local code to extract URLs and using a map function with controlled concurrency</li> <li>Retrieval results are put into the zero-shot problem using an F-string template in LangChain's prompt template</li> <li>Compared LangChain to Hugging Face Transformers Library as a framework and mentioned that their code is often simple, but valuable for its interface and compatibility with other tools</li> </ul>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/#monitoring-improvement-gantry","title":"Monitoring &amp; Improvement: Gantry","text":"<ul> <li>Top three challenges in bringing the spot to the next level: improving retrieval, improving the quality of model outputs, and identifying a solid user base.</li> <li>Using tools like Datadog, Sentry, Honeycomb, and Gantry for handling web services, logging, and monitoring model behavior.</li> <li>The same principle of tracing and monitoring applies to both ML-powered apps and LLM-powered apps.</li> <li>Gantry provides a useful service for tracking and enriching logged data, including toxicity checks and other natural language-based or numerical analyses.</li> <li>Using language models to check on the performance and outputs of other language models.</li> <li>Contributing to the development of the tool as a teaching and learning application is open and encouraged.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/chapter_summaries/","title":"Chapter summaries","text":""},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/chapter_summaries/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/chapter_summaries/#swe-tooling-make-precommit-etc","title":"SWE Tooling: make, precommit, etc","text":"<ul> <li>Walked everyone through the code base for the Discord bot they interacted with</li> <li>Sourced question-answering over a corpus of information using Vector storage for retrieval</li> <li>GitHub repo available for this project, but may not be able to execute the code without accounts on all services</li> <li>Makefile created for easier project management, setting up environment and authentication, and running setup commands</li> <li>Incorporated software tools like pre-commit checks, black for Python auto-formatting, and rust-powered formatter</li> <li>Shell check tool useful for catching issues in bash scripts</li> </ul>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/chapter_summaries/#data-cleaning","title":"Data Cleaning","text":"<ul> <li>Initial approach of scraping data and chunking into smaller pieces did not yield good results</li> <li>Improved results by spending time understanding the data and preserving the structure during processing</li> <li>Extracting textual information from other sources like images and YouTube videos can enhance the usefulness of language models</li> <li>Sometimes simple solutions to specific data sources and problems can greatly improve the quality of results</li> <li>The unglamorous work of getting to know the data and writing code to manage it properly can result in big dividends for language model applications</li> </ul>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/chapter_summaries/#infrastructure-modal","title":"Infrastructure: Modal","text":"<ul> <li>Discussed the ETL component of extracting, transforming, and loading data from various sources</li> <li>Discussed using Python packages for data transformation and addressing dependency issues with tools like pre-commit</li> <li>Explained the benefits of the modal component in creating lightweight virtual containers for different tasks</li> <li>Modal containers are fast and efficient, aiding in quick development cycles and allowing for containerization without the pains of traditional Docker images</li> <li>Modal also allows for the creation of serverless applications with auto-scaling and resource management</li> <li>Debugging and local development can be done through the interactive mode by connecting to a container running on modal</li> <li>showModal provides an interface for tracking application activity, utilization, and resource allocation, making it a versatile tool for various projects</li> </ul>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/chapter_summaries/#frontend-gradio-discord","title":"Frontend: Gradio &amp; Discord","text":"<ul> <li>Introduced Gradio user interface, allowing users to create interfaces in pure Python</li> <li>Gradio UI is flexible, supported by Hugging Face, and rapidly adopting machine learning features</li> <li>Examples of Gradio UI use include Alpaca, Flamingo, and Dolly mini</li> <li>Gradio UI is easy to set up, portable, flexible, and comes with an API with OpenAPI spec</li> <li>Discord bot integrated with Python library Discord.py; alternative library Interactions.py is also available</li> <li>Gradio UI is built on FastAPI for asynchronous Python web service</li> <li>Application mainly runs on the model's infrastructure in containers, serving traffic as needed</li> </ul>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/chapter_summaries/#embeddings-etl","title":"Embeddings &amp; ETL","text":"<ul> <li>Used OpenAI's ada002 model to generate embeddings, which are much cheaper than generation endpoints</li> <li>Currently using a vector index for data storage, but considering adding additional types of search</li> <li>Discussed processing PDFs in a previous lecture, mentioned using local code to extract URLs and using a map function with controlled concurrency</li> <li>Retrieval results are put into the zero-shot problem using an F-string template in LangChain's prompt template</li> <li>Compared LangChain to Hugging Face Transformers Library as a framework and mentioned that their code is often simple, but valuable for its interface and compatibility with other tools</li> </ul>"},{"location":"llm-bootcamp/spring-2023/askfsdl-walkthrough/chapter_summaries/#monitoring-improvement-gantry","title":"Monitoring &amp; Improvement: Gantry","text":"<ul> <li>Top three challenges in bringing the spot to the next level: improving retrieval, improving the quality of model outputs, and identifying a solid user base.</li> <li>Using tools like Datadog, Sentry, Honeycomb, and Gantry for handling web services, logging, and monitoring model behavior.</li> <li>The same principle of tracing and monitoring applies to both ML-powered apps and LLM-powered apps.</li> <li>Gantry provides a useful service for tracking and enriching logged data, including toxicity checks and other natural language-based or numerical analyses.</li> <li>Using language models to check on the performance and outputs of other language models.</li> <li>Contributing to the development of the tool as a teaching and learning application is open and encouraged.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/","title":"Augmented Language Models","text":"<p>Lecture by Josh Tobin. Published May 9, 2023. Download slides.</p>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#why-augmented-lms","title":"Why augmented LMs?","text":"<ul> <li>Language models are good at understanding language, following instructions, basic reasoning, and understanding code, but they lack up-to-date knowledge, specifics about your data, and more complex reasoning abilities.</li> <li>Think of language models as the \"brain\" that needs tools and data to complete tasks.</li> <li>Context windows are limited but growing rapidly and putting more context in the model costs money.</li> <li>There are three ways to augment language models: retrieval, chains, and tools.</li> <li>Retrieval involves providing an external corpus of data for the model to search, chains use the output of one language model as input for another, and tools allow models to interact with external data sources.</li> <li>This lecture serves as an introduction to these topics with depth available for further exploration.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#why-retrieval-augmentation","title":"Why retrieval augmentation?","text":"<ul> <li>Discussing retrieval augmentation to give models access to user-specific data</li> <li>Initial approach: put data into the context (e.g., organizers of an event)</li> <li>Challenge: thousands of users and complex relationships between queries and users make it difficult to use simple rules/coding</li> <li>Consider building the context as a form of information retrieval (like search)</li> <li>Treat putting the right data in the context for the model as a search problem</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#traditional-information-retrieval","title":"Traditional information retrieval","text":"<ul> <li>Traditional information retrieval uses a query to find and rank relevant objects in a collection</li> <li>Objects can be documents, images, or other types of content</li> <li>Inverted indexes, which record word frequencies in documents, are often used for search</li> <li>Relevance is typically determined through Boolean search, while ranking is commonly done using the BM25 algorithm</li> <li>Factors affecting ranking include search term frequency in the document, number of documents containing the search term, and context within a sentence</li> <li>Traditional search is limited as it cannot capture semantic information or complex relationships between terms</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#embeddings-for-retrieval","title":"Embeddings for retrieval","text":"<ul> <li>Discussing AI-centric approach for information retrieval via embeddings.</li> <li>AI helps improve search and retrieve better data from contexts using large language models and embeddings.</li> <li>Embeddings are abstract, dense, compact, usually fixed-size, and learned representations of data, which could be documents, images, audio, etc.</li> <li>Good embeddings have utility for the downstream task, and similar objects should be close together in the embedding space, while different objects should be far apart.</li> <li>Important embeddings to know: Word2Vec, Sentence Transformers, CLIP, OpenAI embeddings (Text Embedding ada002), and Instructor.</li> <li>Off-the-shelf embeddings are a good start, but fine-tuning and training an embedding model on specific tasks can achieve better results.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#embedding-relevance-and-indexes","title":"Embedding relevance and indexes","text":"<ul> <li>Discussing using embeddings for information retrieval</li> <li>Can use cosine similarity or dot product similarity as similarity metrics</li> <li>For nearest neighbor search, can simply use numpy if dealing with less than 100,000 vectors</li> <li>Approximate nearest neighbor algorithms are useful for faster search at larger scales, with tools like Facebook AI's FAISS, HNSW, and Annoy</li> <li>Choosing an information retrieval system is more important than the specific embedding index</li> <li>Limitations of approximate nearest neighbor indices include lack of hosting, data and metadata storage, and scalability</li> <li>Consider an information retrieval system that addresses these limitations for production use, analogous to having a complete library rather than just a card catalog</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#embedding-databases","title":"Embedding databases","text":"<ul> <li>Searching over vectors may not be great for production, so consider databases for a more reliable and production-oriented approach.</li> <li>Consider whether you need an embedding database or just a database, as many popular databases already have embedding index built in, such as PG Vector for Postgres, Elasticsearch, and Redis.</li> <li>Building a system for information retrieval with embeddings involves challenges like scale, reliability, managing the embedding function, specifying queries, and choosing search algorithms.</li> <li>Don't try to handle all the complexity yourself; use existing embedding databases like Chroma, Milvus, Pinecone, Vespa, and Weaviate.</li> <li>When choosing an embedding database, consider features like scalability, embedding management, filtering, and integration with traditional full-text search.</li> <li>General recommendations: use your existing database for prototyping, choose Pinecone for speed of setup, consider Vespa and Weaviate for flexible queries, and Vespa and Milvus for scale and reliability.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#beyond-naive-embeddings","title":"Beyond naive embeddings","text":"<ul> <li>Address issues when queries and documents have different forms and embeddings aren't comparable</li> <li>Consider training a model that jointly represents both queries and documents for a more \"apples to apples\" comparison</li> <li>Explore hypothetical document embeddings: have the model imagine a document containing the query's answer and find similar documents</li> <li>Look into re-ranking techniques: search a large number of documents and train a model to reorder them based on specific criteria</li> <li>Use new libraries like Lama Index to search more efficiently, respecting the structure of the data and subsets (e.g., Notion database, Twitter, or recent data)</li> <li>Lama Index combines document retrieval and building embeddings designed for hierarchical searching</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#patterns-case-studies","title":"Patterns &amp; case studies","text":"<ul> <li>Retrieval augmentation case study: Copilot</li> <li>Two secrets to Copilot: speed and relevant context</li> <li>Builds context by looking at most recently accessed 20 documents in the same programming language</li> <li>Post-processing includes looking at code before and after cursor, relevant snippets from candidate docs, and heuristically accessing data</li> <li>Output generated is a result of sorting by heuristics</li> <li>Copilot is powerful but uses simple retrieval methods, highlighting the effectiveness of heuristics</li> <li>Another common pattern: question answering using retrieval augmentation</li> <li>This involves finding most similar documents/messages to a question and using retrieved information to answer the question</li> <li>Limitation: search process might not return the documents containing the answer</li> <li>Solution: use more models and iterate over documents, calling an LLM on each subset and feeding the output to the next model</li> <li>This approach can be generalized as \"chains\" where models build context for other models</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#what-are-chains-and-why-do-we-need-them","title":"What are chains and why do we need them?","text":"<ul> <li>Discussing ways to add information to the context for language models besides retrieval</li> <li>Retrieval-based models follow a common question-answering pattern: embedding queries, comparing embeddings to find similar documents, and using context to answer questions</li> <li>Key limitation: reliance on the retrieval system; if the right information isn't among the retrieved documents, the model can't answer the question</li> <li>Possible solutions:</li> <li>Improve the quality of the information retrieval system with advanced search features</li> <li>Add additional processing, like using another LLM for post-processing retrieved documents, to refine the context (although it might be slower and more expensive)</li> <li>Introducing the concept of \"chains\": sequencing language model calls where the output of one call is the input to another</li> <li>Example patterns for building chains:</li> <li>Question-answering pattern</li> <li>Hypothetical document embeddings</li> <li>Summarizing a large corpus through a mapreduce-like process by independently summarizing each document, then summarizing the summaries</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#langchain","title":"LangChain","text":"<ul> <li>Lang chain is an extremely popular tool for building chains of models and one of the fastest growing open source projects</li> <li>Supports both Python and JavaScript</li> <li>Fastest way to get started building applications and can be used in production</li> <li>Many people end up creating their own chaining, possibly inspired by Lang chain</li> <li>Lang chain provides a repository of different chains for various tasks and offers nice code and abstractions</li> <li>Ideal for prototyping, but also easy to build your own system if needed for production</li> <li>Contains many examples of types of chains in their repository, which is useful for generating ideas and learning about chaining patterns</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#tool-use","title":"Tool use","text":"<ul> <li>Building context for language models to answer questions can involve creating a search engine or giving them access to APIs and outside tools</li> <li>A \"feeling lucky\" chain involves searching Google for an answer, getting the top result, and summarizing the content for the user</li> <li>Tool_Former paper demonstrates using tools such as calculators, question-answering systems, and translation systems in the training process for language models</li> <li>Tools can be used deterministically or in a way similar to OpenAI plugins</li> <li>Examples of tools for language models include archive search, Python interpreters, and SQL query execution</li> <li>An example chain involves translating a user's natural language question into an SQL query, executing the query, and providing the response back to the user</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#plugins","title":"Plugins","text":"<ul> <li>There is a more automated approach called plugins to allow language models to interact with external tools.</li> <li>In a chain-based approach, developers manually design the interaction pattern between language model and tool by passing queries through a series of steps.</li> <li>In a plugin-based approach, the language model gets to decide whether to use a tool or not. A simpler method is used in Tool Former and OpenAI plugins.</li> <li>To create an OpenAI plugin, provide an API spec and a description of the API meant for the language model to decide when to use it.</li> <li>OpenAI passes the description as part of the context to the model, enabling it to make decisions based on user inputs and the API's usefulness.</li> <li>The model can invoke the API, and results are fed into the context allowing the language model to continue answering user questions.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#recommendations-for-tool-use","title":"Recommendations for tool use","text":"<ul> <li>Tools are a flexible way to augment language models with external data.</li> <li>Retrieval systems are one example of a tool that can be based on various databases.</li> <li>Two ways to build tool use into language models: manually describe the logic (chains) or use plugins and let the model figure it out.</li> <li>Chains are better for reliability and consistent problem-solving.</li> <li>Plugins are more suitable for interactivity, flexibility, and general-purpose use, allowing users to solve various unanticipated problems.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/#recap-conclusions","title":"Recap &amp; conclusions","text":"<ul> <li>LMS are more powerful when connected to external data</li> <li>Rules and heuristics can help identify useful data</li> <li>As knowledge base scales, consider it as information retrieval</li> <li>Chains can encode complex reasoning and help with token limits</li> <li>Tools can provide access to external knowledge beyond internal database</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/","title":"Chapter summaries","text":""},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#why-augmented-lms","title":"Why augmented LMs?","text":"<ul> <li>Language models are good at understanding language, following instructions, basic reasoning, and understanding code, but they lack up-to-date knowledge, specifics about your data, and more complex reasoning abilities.</li> <li>Think of language models as the \"brain\" that needs tools and data to complete tasks.</li> <li>Context windows are limited but growing rapidly and putting more context in the model costs money.</li> <li>There are three ways to augment language models: retrieval, chains, and tools.</li> <li>Retrieval involves providing an external corpus of data for the model to search, chains use the output of one language model as input for another, and tools allow models to interact with external data sources.</li> <li>This lecture serves as an introduction to these topics with depth available for further exploration.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#why-retrieval-augmentation","title":"Why retrieval augmentation?","text":"<ul> <li>Discussing retrieval augmentation to give models access to user-specific data</li> <li>Initial approach: put data into the context (e.g., organizers of an event)</li> <li>Challenge: thousands of users and complex relationships between queries and users make it difficult to use simple rules/coding</li> <li>Consider building the context as a form of information retrieval (like search)</li> <li>Treat putting the right data in the context for the model as a search problem</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#traditional-information-retrieval","title":"Traditional information retrieval","text":"<ul> <li>Traditional information retrieval uses a query to find and rank relevant objects in a collection</li> <li>Objects can be documents, images, or other types of content</li> <li>Inverted indexes, which record word frequencies in documents, are often used for search</li> <li>Relevance is typically determined through Boolean search, while ranking is commonly done using the BM25 algorithm</li> <li>Factors affecting ranking include search term frequency in the document, number of documents containing the search term, and context within a sentence</li> <li>Traditional search is limited as it cannot capture semantic information or complex relationships between terms</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#embeddings-for-retrieval","title":"Embeddings for retrieval","text":"<ul> <li>Discussing AI-centric approach for information retrieval via embeddings.</li> <li>AI helps improve search and retrieve better data from contexts using large language models and embeddings.</li> <li>Embeddings are abstract, dense, compact, usually fixed-size, and learned representations of data, which could be documents, images, audio, etc.</li> <li>Good embeddings have utility for the downstream task, and similar objects should be close together in the embedding space, while different objects should be far apart.</li> <li>Important embeddings to know: Word2Vec, Sentence Transformers, CLIP, OpenAI embeddings (Text Embedding ada002), and Instructor.</li> <li>Off-the-shelf embeddings are a good start, but fine-tuning and training an embedding model on specific tasks can achieve better results.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#embedding-relevance-and-indexes","title":"Embedding relevance and indexes","text":"<ul> <li>Discussing using embeddings for information retrieval</li> <li>Can use cosine similarity or dot product similarity as similarity metrics</li> <li>For nearest neighbor search, can simply use numpy if dealing with less than 100,000 vectors</li> <li>Approximate nearest neighbor algorithms are useful for faster search at larger scales, with tools like Facebook AI's FAISS, HNSW, and Annoy</li> <li>Choosing an information retrieval system is more important than the specific embedding index</li> <li>Limitations of approximate nearest neighbor indices include lack of hosting, data and metadata storage, and scalability</li> <li>Consider an information retrieval system that addresses these limitations for production use, analogous to having a complete library rather than just a card catalog</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#embedding-databases","title":"Embedding databases","text":"<ul> <li>Searching over vectors may not be great for production, so consider databases for a more reliable and production-oriented approach.</li> <li>Consider whether you need an embedding database or just a database, as many popular databases already have embedding index built in, such as PG Vector for Postgres, Elasticsearch, and Redis.</li> <li>Building a system for information retrieval with embeddings involves challenges like scale, reliability, managing the embedding function, specifying queries, and choosing search algorithms.</li> <li>Don't try to handle all the complexity yourself; use existing embedding databases like Chroma, Milvus, Pinecone, Vespa, and Weaviate.</li> <li>When choosing an embedding database, consider features like scalability, embedding management, filtering, and integration with traditional full-text search.</li> <li>General recommendations: use your existing database for prototyping, choose Pinecone for speed of setup, consider Vespa and Weaviate for flexible queries, and Vespa and Milvus for scale and reliability.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#beyond-naive-embeddings","title":"Beyond naive embeddings","text":"<ul> <li>Address issues when queries and documents have different forms and embeddings aren't comparable</li> <li>Consider training a model that jointly represents both queries and documents for a more \"apples to apples\" comparison</li> <li>Explore hypothetical document embeddings: have the model imagine a document containing the query's answer and find similar documents</li> <li>Look into re-ranking techniques: search a large number of documents and train a model to reorder them based on specific criteria</li> <li>Use new libraries like Lama Index to search more efficiently, respecting the structure of the data and subsets (e.g., Notion database, Twitter, or recent data)</li> <li>Lama Index combines document retrieval and building embeddings designed for hierarchical searching</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#patterns-case-studies","title":"Patterns &amp; case studies","text":"<ul> <li>Retrieval augmentation case study: Copilot</li> <li>Two secrets to Copilot: speed and relevant context</li> <li>Builds context by looking at most recently accessed 20 documents in the same programming language</li> <li>Post-processing includes looking at code before and after cursor, relevant snippets from candidate docs, and heuristically accessing data</li> <li>Output generated is a result of sorting by heuristics</li> <li>Copilot is powerful but uses simple retrieval methods, highlighting the effectiveness of heuristics</li> <li>Another common pattern: question answering using retrieval augmentation</li> <li>This involves finding most similar documents/messages to a question and using retrieved information to answer the question</li> <li>Limitation: search process might not return the documents containing the answer</li> <li>Solution: use more models and iterate over documents, calling an LLM on each subset and feeding the output to the next model</li> <li>This approach can be generalized as \"chains\" where models build context for other models</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#what-are-chains-and-why-do-we-need-them","title":"What are chains and why do we need them?","text":"<ul> <li>Discussing ways to add information to the context for language models besides retrieval</li> <li>Retrieval-based models follow a common question-answering pattern: embedding queries, comparing embeddings to find similar documents, and using context to answer questions</li> <li>Key limitation: reliance on the retrieval system; if the right information isn't among the retrieved documents, the model can't answer the question</li> <li>Possible solutions:</li> <li>Improve the quality of the information retrieval system with advanced search features</li> <li>Add additional processing, like using another LLM for post-processing retrieved documents, to refine the context (although it might be slower and more expensive)</li> <li>Introducing the concept of \"chains\": sequencing language model calls where the output of one call is the input to another</li> <li>Example patterns for building chains:</li> <li>Question-answering pattern</li> <li>Hypothetical document embeddings</li> <li>Summarizing a large corpus through a mapreduce-like process by independently summarizing each document, then summarizing the summaries</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#langchain","title":"LangChain","text":"<ul> <li>Lang chain is an extremely popular tool for building chains of models and one of the fastest growing open source projects</li> <li>Supports both Python and JavaScript</li> <li>Fastest way to get started building applications and can be used in production</li> <li>Many people end up creating their own chaining, possibly inspired by Lang chain</li> <li>Lang chain provides a repository of different chains for various tasks and offers nice code and abstractions</li> <li>Ideal for prototyping, but also easy to build your own system if needed for production</li> <li>Contains many examples of types of chains in their repository, which is useful for generating ideas and learning about chaining patterns</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#tool-use","title":"Tool use","text":"<ul> <li>Building context for language models to answer questions can involve creating a search engine or giving them access to APIs and outside tools</li> <li>A \"feeling lucky\" chain involves searching Google for an answer, getting the top result, and summarizing the content for the user</li> <li>Tool_Former paper demonstrates using tools such as calculators, question-answering systems, and translation systems in the training process for language models</li> <li>Tools can be used deterministically or in a way similar to OpenAI plugins</li> <li>Examples of tools for language models include archive search, Python interpreters, and SQL query execution</li> <li>An example chain involves translating a user's natural language question into an SQL query, executing the query, and providing the response back to the user</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#plugins","title":"Plugins","text":"<ul> <li>There is a more automated approach called plugins to allow language models to interact with external tools.</li> <li>In a chain-based approach, developers manually design the interaction pattern between language model and tool by passing queries through a series of steps.</li> <li>In a plugin-based approach, the language model gets to decide whether to use a tool or not. A simpler method is used in Tool Former and OpenAI plugins.</li> <li>To create an OpenAI plugin, provide an API spec and a description of the API meant for the language model to decide when to use it.</li> <li>OpenAI passes the description as part of the context to the model, enabling it to make decisions based on user inputs and the API's usefulness.</li> <li>The model can invoke the API, and results are fed into the context allowing the language model to continue answering user questions.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#recommendations-for-tool-use","title":"Recommendations for tool use","text":"<ul> <li>Tools are a flexible way to augment language models with external data.</li> <li>Retrieval systems are one example of a tool that can be based on various databases.</li> <li>Two ways to build tool use into language models: manually describe the logic (chains) or use plugins and let the model figure it out.</li> <li>Chains are better for reliability and consistent problem-solving.</li> <li>Plugins are more suitable for interactivity, flexibility, and general-purpose use, allowing users to solve various unanticipated problems.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/augmented-language-models/chapter_summaries/#recap-conclusions","title":"Recap &amp; conclusions","text":"<ul> <li>LMS are more powerful when connected to external data</li> <li>Rules and heuristics can help identify useful data</li> <li>As knowledge base scales, consider it as information retrieval</li> <li>Chains can encode complex reasoning and help with token limits</li> <li>Tools can provide access to external knowledge beyond internal database</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/","title":"Harrison Chase: Agents","text":"<p>Lecture by Harrison Chase. Published May 25, 2023. Download slides.</p>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/chase-agents/#what-are-agents","title":"What are \"agents\"?","text":"<ul> <li>The lecture covers agents and their significance in the context of LangChain.</li> <li>The core idea of agents is using a language model as a reasoning engine to determine how to interact with the outside world based on user input</li> <li>First it defines what agents are, explains why they are used, and shows how they are typically implemented.</li> <li>It also considers the challenges associated with getting agents to work reliably in production.</li> <li>It touches on memory and recent projects that involve agentic behavior</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#why-use-agents","title":"Why use agents?","text":"<ul> <li>Agents are useful for connecting language models to external sources of data and computation, such as search APIs and databases.</li> <li>Agents are more flexible and powerful than simply connecting language models to tools, and can handle edge cases and multi-hop tasks better.</li> <li>The typical implementation of agents involves using the language model to choose a tool, taking action with that tool, observing the output, and feeding it back into the language model until a stopping condition is met.</li> <li>Stopping conditions can be set by the language model or through hard-coded rules.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#react-reasoning-to-act","title":"ReAct: Reasoning to Act","text":"<ul> <li>ReAct is a prompting strategy for natural language processing</li> <li>It stands for \"Reasoning and Acting\"</li> <li>It combines Chain-of-Thought reasoning and action-taking to improve the language model's ability to reason and access real data sources</li> <li>It yields higher quality, more reliable results than other prompting techniques</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#challenge-controlling-tool-use","title":"Challenge: controlling tool use","text":"<ul> <li>React is a popular implementation of agency, but there are many challenges</li> <li>One challenge is getting agents to use tools appropriately, which can be addressed by providing tool descriptions or using tool retrieval</li> <li>Few-shot examples can guide the language model in what to do</li> <li>Another challenge is getting agents not to use tools when they don't need to, which can be addressed with reminders or adding a tool that explicitly returns to the user</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#challenge-parsing-tool-invocations","title":"Challenge: parsing tool invocations","text":"<ul> <li>Language models return raw strings, and we often want to pass those strings into other programs</li> <li>More structured responses, like those in JSON format, are easier to parse</li> <li>Output parsers are used to encapsulate the logic needed to parse responses, can be modular, and can retry mistakes</li> <li>There are subtle differences in fixing errors in response outputs, and output parsers can help with this task</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#challenge-long-term-memory-and-coherence","title":"Challenge: long-term memory and coherence","text":"<ul> <li>Fourth challenge is getting agents to remember previous steps</li> <li>ReAct paper keeps a list of these steps in memory</li> <li>Long-running tasks present context window issues</li> <li>Retrieval methods can fetch previous steps and put them into context</li> <li>Combining some <code>N</code> most recent and some <code>K</code> most relevant actions and observations is common</li> <li>Incorporating big and hard-to-parse API responses is a challenge</li> <li>Custom logic can be used to select relevant keys and put them in context</li> <li>Tool usage requires thinking about output size</li> <li>Agents can go off track, and reiterating the objective can help</li> <li>Separating planning and execution steps can help break down objectives</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#challenge-evaluation","title":"Challenge: evaluation","text":"<ul> <li>Evaluating language models and applications built on top is difficult</li> <li>Evaluating agents is also difficult</li> <li>One way to evaluate agents is to measure if the correct result was produced</li> <li>Another way to evaluate agents is to assess if the agent trajectory or intermediate steps were correct and efficient. Examples include evaluating correct input to action, correct number of steps, and the most efficient sequence of steps.</li> <li>Evaluating the intermediate steps can be just as useful as evaluating the final result.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#agent-memory-and-adaptability","title":"Agent memory and adaptability","text":"<ul> <li>Memory is an interesting aspect of AI, especially in the context of user-AI interactions and personalization.</li> <li>Personalization can be achieved by encoding an agent's objectives and persona in the prompt, but there is also work being done on evolving that over time to give agents a sense of long-term memory.</li> <li>Memory is becoming increasingly important in the concept of agents as encapsulated programs that adapt over time.</li> <li>Four recent projects build upon and improve the \"react-style\" agent, discussed next</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#example-autogpt","title":"Example: AutoGPT","text":"<p>points:</p> <ul> <li>ReAct-style agents are designed to solve a specific objective, with short-lived, immediately quantifiable goals</li> <li>AutoGPT was created for long-running, open-ended goals such as increasing Twitter following</li> <li>AutoGPT introduced the concept of long-term memory using a vector store due to the long-running nature of its projects</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#example-babyagi","title":"Example: BabyAGI","text":"<ul> <li>BabyAGI is another popular project for agents for long-running objectives</li> <li>Introduces separate planning and execution steps to improve long-running objectives</li> <li>BabyAGI initially didn't have tools, but now has them</li> <li>Separating planning and execution steps can improve reliability and focus of longer-term agents</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#example-camel","title":"Example: CAMEL","text":"<ul> <li>CAMEL paper involves two agents working together, novel idea</li> <li>The main point of the paper is the use of a simulation environment</li> <li>Simulation environments can be used for practical evaluation of agents or for entertainment</li> <li>The paper's results are for a simple \"simulation environment\" -- two agents interacting in a chat room</li> <li>The agents were language models without tools</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/#example-generative-agents-paper","title":"Example: \"Generative Agents\" paper","text":"<ul> <li>Recent simulation environment that had 25 agents in a Sims-like world</li> <li>Memory refers to remembering previous events to inform future actions</li> <li>Three components of memory retrieval: time weighting, importance weighting, relevancy weighting</li> <li>Reflection step introduced to update different states of the world after observing recent events</li> <li>Reflection step could be applied to other memory types in LangChain, such as entity memory and summary conversation memory</li> <li>Other papers recently incorporated the idea of reflection, which is interesting and worth keeping an eye on for the future</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/","title":"Chapter summaries","text":""},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#what-are-agents","title":"What are \"agents\"?","text":"<ul> <li>The lecture covers agents and their significance in the context of LangChain.</li> <li>The core idea of agents is using a language model as a reasoning engine to determine how to interact with the outside world based on user input</li> <li>First it defines what agents are, explains why they are used, and shows how they are typically implemented.</li> <li>It also considers the challenges associated with getting agents to work reliably in production.</li> <li>It touches on memory and recent projects that involve agentic behavior</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#why-use-agents","title":"Why use agents?","text":"<ul> <li>Agents are useful for connecting language models to external sources of data and computation, such as search APIs and databases.</li> <li>Agents are more flexible and powerful than simply connecting language models to tools, and can handle edge cases and multi-hop tasks better.</li> <li>The typical implementation of agents involves using the language model to choose a tool, taking action with that tool, observing the output, and feeding it back into the language model until a stopping condition is met.</li> <li>Stopping conditions can be set by the language model or through hard-coded rules.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#react-reasoning-to-act","title":"ReAct: Reasoning to Act","text":"<ul> <li>ReAct is a prompting strategy for natural language processing</li> <li>It stands for \"Reasoning and Acting\"</li> <li>It combines Chain-of-Thought reasoning and action-taking to improve the language model's ability to reason and access real data sources</li> <li>It yields higher quality, more reliable results than other prompting techniques</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#challenge-controlling-tool-use","title":"Challenge: controlling tool use","text":"<ul> <li>React is a popular implementation of agency, but there are many challenges</li> <li>One challenge is getting agents to use tools appropriately, which can be addressed by providing tool descriptions or using tool retrieval</li> <li>Few-shot examples can guide the language model in what to do</li> <li>Another challenge is getting agents not to use tools when they don't need to, which can be addressed with reminders or adding a tool that explicitly returns to the user</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#challenge-parsing-tool-invocations","title":"Challenge: parsing tool invocations","text":"<ul> <li>Language models return raw strings, and we often want to pass those strings into other programs</li> <li>More structured responses, like those in JSON format, are easier to parse</li> <li>Output parsers are used to encapsulate the logic needed to parse responses, can be modular, and can retry mistakes</li> <li>There are subtle differences in fixing errors in response outputs, and output parsers can help with this task</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#challenge-long-term-memory-and-coherence","title":"Challenge: long-term memory and coherence","text":"<ul> <li>Fourth challenge is getting agents to remember previous steps</li> <li>ReAct paper keeps a list of these steps in memory</li> <li>Long-running tasks present context window issues</li> <li>Retrieval methods can fetch previous steps and put them into context</li> <li>Combining some <code>N</code> most recent and some <code>K</code> most relevant actions and observations is common</li> <li>Incorporating big and hard-to-parse API responses is a challenge</li> <li>Custom logic can be used to select relevant keys and put them in context</li> <li>Tool usage requires thinking about output size</li> <li>Agents can go off track, and reiterating the objective can help</li> <li>Separating planning and execution steps can help break down objectives</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#challenge-evaluation","title":"Challenge: evaluation","text":"<ul> <li>Evaluating language models and applications built on top is difficult</li> <li>Evaluating agents is also difficult</li> <li>One way to evaluate agents is to measure if the correct result was produced</li> <li>Another way to evaluate agents is to assess if the agent trajectory or intermediate steps were correct and efficient. Examples include evaluating correct input to action, correct number of steps, and the most efficient sequence of steps.</li> <li>Evaluating the intermediate steps can be just as useful as evaluating the final result.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#agent-memory-and-adaptability","title":"Agent memory and adaptability","text":"<ul> <li>Memory is an interesting aspect of AI, especially in the context of user-AI interactions and personalization.</li> <li>Personalization can be achieved by encoding an agent's objectives and persona in the prompt, but there is also work being done on evolving that over time to give agents a sense of long-term memory.</li> <li>Memory is becoming increasingly important in the concept of agents as encapsulated programs that adapt over time.</li> <li>Four recent projects build upon and improve the \"react-style\" agent, discussed next</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#example-autogpt","title":"Example: AutoGPT","text":"<p>points:</p> <ul> <li>ReAct-style agents are designed to solve a specific objective, with short-lived, immediately quantifiable goals</li> <li>AutoGPT was created for long-running, open-ended goals such as increasing Twitter following</li> <li>AutoGPT introduced the concept of long-term memory using a vector store due to the long-running nature of its projects</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#example-babyagi","title":"Example: BabyAGI","text":"<ul> <li>BabyAGI is another popular project for agents for long-running objectives</li> <li>Introduces separate planning and execution steps to improve long-running objectives</li> <li>BabyAGI initially didn't have tools, but now has them</li> <li>Separating planning and execution steps can improve reliability and focus of longer-term agents</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#example-camel","title":"Example: CAMEL","text":"<ul> <li>CAMEL paper involves two agents working together, novel idea</li> <li>The main point of the paper is the use of a simulation environment</li> <li>Simulation environments can be used for practical evaluation of agents or for entertainment</li> <li>The paper's results are for a simple \"simulation environment\" -- two agents interacting in a chat room</li> <li>The agents were language models without tools</li> </ul>"},{"location":"llm-bootcamp/spring-2023/chase-agents/chapter_summaries/#example-generative-agents-paper","title":"Example: \"Generative Agents\" paper","text":"<ul> <li>Recent simulation environment that had 25 agents in a Sims-like world</li> <li>Memory refers to remembering previous events to inform future actions</li> <li>Three components of memory retrieval: time weighting, importance weighting, relevancy weighting</li> <li>Reflection step introduced to update different states of the world after observing recent events</li> <li>Reflection step could be applied to other memory types in LangChain, such as entity memory and summary conversation memory</li> <li>Other papers recently incorporated the idea of reflection, which is interesting and worth keeping an eye on for the future</li> </ul>"},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/","title":"Launch an LLM App in One Hour","text":"<p>Lecture by Charles Frye. Published May 9, 2023. Download slides.</p>"},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/#why-now","title":"Why now?","text":"<ul> <li>Excitement about large language models and artificial intelligence is high, especially since one tool can now accomplish tasks that previously required multiple specialized tools.</li> <li>Language user interfaces (LUIs) enable more natural interaction with computers through speech and natural language. Large language models, like GPT-3, make LUIs more flexible and capable.</li> <li>Products and applications are being built with these models, including OpenAI's ChatGPT and GitHub Copilot, hinting at a promising future.</li> <li>However, the gap between demos and actual products is significant. Overpromising and underdelivering in the past led to \"AI winters,\" so it's important to create valuable products and tools to maintain funding and interest.</li> <li>The playbook for building applications with language models is emerging, and this boot camp will cover aspects of that process.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/#prototyping-iteration-in-a-playground","title":"Prototyping &amp; Iteration in a Playground","text":"<ul> <li>Attended various hackathons focused on using machine learning tools</li> <li>Explored the potential of high-capability hosted models, such as OpenAI's, in a simple chat interface to quickly test capabilities</li> <li>Used a notebook environment for quick tinkering, building prototypes, and discovering limitations of language models</li> <li>Started with a problem statement: using large language models to learn about large language models</li> <li>Discovered difficulties with language models, such as having outdated and limited information</li> <li>Found that providing specific sources or papers can help improve answers from the model</li> </ul>"},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/#prototyping-iteration-in-a-notebook","title":"Prototyping &amp; Iteration in a Notebook","text":"<ul> <li>Experiment with automating steps in ephemeral notebook environment like Collab.</li> <li>OpenAI API allows interaction with language models and offers various SDKs.</li> <li>Lang chain is a popular open-source framework for interacting with these models; it's fast-evolving and provides all necessary components.</li> <li>Develop a process to find information and bring it to context. Utilize Python libraries like <code>archive</code> for data sourcing.</li> <li>Utilize document loaders, such as the one built into Lang chain, to extract content from PDFs.</li> <li>Use embedding search for large scale information retrieval within documents.</li> <li>Prototype and tinker with language models to constantly improve them.</li> <li>Look for similar existing projects to jump off or even default examples provided, such as Lang chain's default example.</li> <li>Turn these experiments into something usable by people at a larger scale.</li> <li>The workflow with modern language models is more flexible and faster compared to the past machine learning processes.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/#deploying-an-mvp","title":"Deploying an MVP","text":"<ul> <li>Building an MVP version of an application requires focusing on what's useful to a broad range of users.</li> <li>Prioritize the user interface and gather feedback from users quickly.</li> <li>Cloud-native tooling and serverless infrastructure like Model are helpful in swiftly scaling applications and addressing data processing bottlenecks.</li> <li>Use various tech stacks for different tasks, such as OpenAI for language models, Pinecone for quick search, MongoDB for data storage, and AWS for running lightweight Discord bot servers.</li> <li>Implement the application, then monitor usage data to make improvements and learn from successes and failures.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/chapter_summaries/","title":"Chapter summaries","text":""},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/chapter_summaries/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/chapter_summaries/#why-now","title":"Why now?","text":"<ul> <li>Excitement about large language models and artificial intelligence is high, especially since one tool can now accomplish tasks that previously required multiple specialized tools.</li> <li>Language user interfaces (LUIs) enable more natural interaction with computers through speech and natural language. Large language models, like GPT-3, make LUIs more flexible and capable.</li> <li>Products and applications are being built with these models, including OpenAI's ChatGPT and GitHub Copilot, hinting at a promising future.</li> <li>However, the gap between demos and actual products is significant. Overpromising and underdelivering in the past led to \"AI winters,\" so it's important to create valuable products and tools to maintain funding and interest.</li> <li>The playbook for building applications with language models is emerging, and this boot camp will cover aspects of that process.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/chapter_summaries/#prototyping-iteration-in-a-playground","title":"Prototyping &amp; Iteration in a Playground","text":"<ul> <li>Attended various hackathons focused on using machine learning tools</li> <li>Explored the potential of high-capability hosted models, such as OpenAI's, in a simple chat interface to quickly test capabilities</li> <li>Used a notebook environment for quick tinkering, building prototypes, and discovering limitations of language models</li> <li>Started with a problem statement: using large language models to learn about large language models</li> <li>Discovered difficulties with language models, such as having outdated and limited information</li> <li>Found that providing specific sources or papers can help improve answers from the model</li> </ul>"},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/chapter_summaries/#prototyping-iteration-in-a-notebook","title":"Prototyping &amp; Iteration in a Notebook","text":"<ul> <li>Experiment with automating steps in ephemeral notebook environment like Collab.</li> <li>OpenAI API allows interaction with language models and offers various SDKs.</li> <li>Lang chain is a popular open-source framework for interacting with these models; it's fast-evolving and provides all necessary components.</li> <li>Develop a process to find information and bring it to context. Utilize Python libraries like <code>archive</code> for data sourcing.</li> <li>Utilize document loaders, such as the one built into Lang chain, to extract content from PDFs.</li> <li>Use embedding search for large scale information retrieval within documents.</li> <li>Prototype and tinker with language models to constantly improve them.</li> <li>Look for similar existing projects to jump off or even default examples provided, such as Lang chain's default example.</li> <li>Turn these experiments into something usable by people at a larger scale.</li> <li>The workflow with modern language models is more flexible and faster compared to the past machine learning processes.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/launch-an-llm-app-in-one-hour/chapter_summaries/#deploying-an-mvp","title":"Deploying an MVP","text":"<ul> <li>Building an MVP version of an application requires focusing on what's useful to a broad range of users.</li> <li>Prioritize the user interface and gather feedback from users quickly.</li> <li>Cloud-native tooling and serverless infrastructure like Model are helpful in swiftly scaling applications and addressing data processing bottlenecks.</li> <li>Use various tech stacks for different tasks, such as OpenAI for language models, Pinecone for quick search, MongoDB for data storage, and AWS for running lightweight Discord bot servers.</li> <li>Implement the application, then monitor usage data to make improvements and learn from successes and failures.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/","title":"LLM Foundations","text":"<p>Lecture by Sergey Karayev. Published May 19, 2023. Download slides.</p>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/llm-foundations/#intro","title":"Intro","text":"<ul> <li>Discuss four key ideas in machine learning</li> <li>Address diverse audience, including experts, executives, and investors</li> <li>Cover Transformer architecture</li> <li>Mention notable LLMs (e.g., GPT, T5, BERT, etc.)</li> <li>Share details on running a Transformer</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#foundations-of-machine-learning","title":"Foundations of Machine Learning","text":"<ul> <li>Machine learning has shifted from traditional programming (Software 1.0) to a Software 2.0 mindset, where algorithms are generated from training data and more emphasis is placed on the training system. </li> <li>Three types of machine learning include unsupervised learning, supervised learning, and reinforcement learning, which have mostly converged to a supervised learning approach.</li> <li>For machines, input and output are always just numbers, represented as vectors or matrices.</li> <li>One dominant approach to machine learning today is neural networks, also known as deep learning, which was inspired by the human brain's structure and function.</li> <li>Neural networks consist of perceptrons connected in layers, and all operations are matrix multiplications.</li> <li>GPUs, originally developed for graphics and video games, have played a significant role in advancing deep learning due to their compatibility with matrix multiplications.</li> <li>To train a neural network, data is typically split into training, validation, and test sets to avoid overfitting and improve model performance.</li> <li>Pre-training involves training a large model on extensive data, which can then be fine-tuned using smaller sets of specialized data for better performance.</li> <li>Model hubs, such as Hugging Face, offer numerous pre-trained models for various machine learning tasks and have seen significant growth in recent years.</li> <li>The Transformer model has become the dominant architecture for a wide range of machine learning tasks.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#the-transformer-architecture","title":"The Transformer Architecture","text":"<ul> <li>Transformer architecture introduced in 2017 paper \"Attention is All You Need\"</li> <li>Set state-of-the-art results in translation tasks</li> <li>Applied to other NLP tasks and fields like vision</li> <li>Appears complicated but consists of two similar halves</li> <li>Focusing on one half called the decoder</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#transformer-decoder-overview","title":"Transformer Decoder Overview","text":"<ul> <li>The task of the Transformer decoder is to complete text, much like GPT models.</li> <li>The input consists of a sequence of tokens (e.g., \"it's a blue\"), and the goal is to predict the next word (e.g., \"sundress\").</li> <li>The output is a probability distribution over potential next tokens.</li> <li>Inference involves sampling a token from the distribution, appending it to the input, and running the model again with the updated input.</li> <li>ChatGPT operates by seeing user input, sampling the next word, appending it, and repeating this process.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#inputs","title":"Inputs","text":"<ul> <li>Inputs need to be vectors of numbers</li> <li>Text is turned into vectors through tokenization</li> <li>Tokens are assigned an ID in a vocabulary, rather than being words</li> <li>Numbers are represented as vectors using one-hot encoding (e.g., number 3 represented by a vector with 1 in third position, zeros everywhere else)</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#input-embedding","title":"Input Embedding","text":"<ul> <li>One-hot vectors are not good representations of words or tokens as they don't capture the notion of similarity between words</li> <li>To address the issue, we use embedding</li> <li>Embedding involves learning an embedding matrix which converts a one-hot vocabulary encoding into a dense vector of chosen dimensionalities</li> <li>This process turns words into dense embeddings, making it the simplest neural network layer type</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#masked-multi-head-attention","title":"Masked Multi-Head Attention","text":"<ul> <li>Attention was introduced in 2015 for translation tasks, and the idea is to predict the most likely next token based on the importance of previous tokens.</li> <li>Attention mechanism involves an output as a weighted sum of input vectors, and these weights are calculated using dot products (similarities) between the input vectors.</li> <li>Each input vector plays three roles in the attention mechanism: as a query, key, and value.</li> <li>To learn and improve attention, input vectors can be projected into different roles (query, key, and value) by multiplying them with learnable matrices.</li> <li>Multi-head attention refers to learning several different ways of transforming inputs into queries, keys, and values simultaneously.</li> <li>Masking is used to prevent the model from \"cheating\" by considering future tokens; it ensures that the model only predicts the next token based on the already seen input.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>No notion of position in the current model, only whether something has been seen or not.</li> <li>Positional encoding is introduced to provide ordering among the seen elements.</li> <li>Current equations resemble a bag of unordered items.</li> <li>Positional encoding vectors are added to embedding vectors to provide order.</li> <li>Seems counterintuitive, but it works; attention mechanism figures out relevant positions.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#skip-connections-and-layer-norm","title":"Skip Connections and Layer Norm","text":"<ul> <li>Add up and norm attention outputs using skip connections and layer normalization</li> <li>Skip connections help propagate loss from end to beginning of model during backpropagation</li> <li>Layer normalization resets mean and standard deviation to uniform after every operation</li> <li>Input embedding determines the dimension of the entire Transformer model</li> <li>Normalization seems inelegant but is very effective in improving neural net learning</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#feed-forward-layer","title":"Feed-forward Layer","text":"<ul> <li>Feed forward layer is similar to the standard multi-layer perceptron.</li> <li>It receives tokens augmented with relevant information.</li> <li>The layer upgrades the token representation.</li> <li>The process goes from word-level to thought-level, with more semantic meaning.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#transformer-hyperparameters-and-why-they-work-so-well","title":"Transformer hyperparameters and Why they work so well","text":"<ul> <li>GPT-3 model ranges from 12 to 96 layers of Transformer layers with adjustable embedding dimensions and attention heads, totaling 175 billion parameters.</li> <li>Most of GPT-3's parameters are in the feed forward layer, but for smaller models, a significant portion is in embedding and attention.</li> <li>Transformers are effective general-purpose differentiable computers that are expressive, optimizable via backpropagation, and efficient due to parallel processing.</li> <li>Understanding exact expressiveness of the Transformer is ongoing, with interesting results like RASP (a programming language designed to be implemented within a Transformer).</li> <li>Decompiling Transformer weights back to a program is still an unsolved problem.</li> <li>Multiple attention heads allow the model to figure out how to use a second head, showcased in work like Induction Heads.</li> <li>Learning to code Transformers isn't necessary for AI-powered products, but can be fun and educational. Resources like YouTube tutorials and code examples are available to assist in learning.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#notable-llm-bert","title":"Notable LLM: BERT","text":"<ul> <li>Bert, T5, and GPT cover the gamut of large Transformer models</li> <li>Bert stands for bi-directional encoder representation from Transformers</li> <li>Bert uses the encoder part of the Transformer, with unmasked attention</li> <li>Bert contains 100 million parameters, considered large at its time</li> <li>Bert was trained by masking 15% of words in a text corpus and predicting the masked words</li> <li>Bert became a building block for other NLP applications</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#notable-llm-t5","title":"Notable LLM: T5","text":"<ul> <li>T5 applies Transformer architecture to text-to-text transfer, meaning both input and output are text strings</li> <li>The task is encoded in the input string and can involve translation, summarization, etc.</li> <li>Encoder-decoder architecture was found to be best, with 11 billion parameters</li> <li>Trained on Colossal Queen Crawl Corpus (C4) derived from Common Crawl dataset</li> <li>C4 was created by filtering out short pages, offensive content, pages with code, and de-duplicating data</li> <li>Fine-tuned using academic supervised tasks for various NLP applications</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#notable-llm-gpt","title":"Notable LLM: GPT","text":"<ul> <li>GPT is a generative pre-trained Transformer, with GPT-2 being decoder only</li> <li>GPT-2 was trained on a dataset called WebText created by scraping links from Reddit</li> <li>GPT tokenizes text using byte pair encoding, a middle ground between old-school tokenization and using UTF-8 bytes</li> <li>GPT-3 came out in 2020 and is 100 times larger than GPT-2, enabling few-shot and zero-shot learning</li> <li>GPT-3 was trained on webtext, raw common crawl data, a selection of books, and all of Wikipedia</li> <li>The dataset for GPT-3 contained 500 billion tokens, but it was only trained on 300 billion tokens</li> <li>GPT-4 details are unknown, but it is assumed to be much larger than previous versions due to the trend in increasing size</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#notable-llm-chinchilla-and-scaling-laws","title":"Notable LLM: Chinchilla and Scaling Laws","text":"<ul> <li>Using more computation to train AI systems improves their performance</li> <li>Rich Sutton's \"bitter lesson\": advantage goes to those stacking more layers</li> <li>DeepMind's paper, Training Compute Optimal LLMs: studied relationship between model size, compute and data set size</li> <li>Most LLMs in literature had too many parameters for their data amount</li> <li>Chinchilla model (70 billion) outperformed Gopher model (four times larger) by training on 1.4 trillion tokens instead of 300 billion</li> <li>Open question: can models continue to improve by training repeatedly on existing data?</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#notable-llm-llama","title":"Notable LLM: LLaMA","text":"<ul> <li>Llama is an open-source chinchilla optimal LLM from Meta Research</li> <li>Several sizes available, ranging from 7 billion to 65 billion, with at least 1 trillion tokens</li> <li>Competitively benchmarks against GPT-3 and other state-of-the-art LLMs</li> <li>Open source but non-commercial license for pre-trained weights</li> <li>Trained on custom common crawl filtering, C4, GitHub, Wikipedia, books, and scientific papers</li> <li>Data set replicated by Red Pajama, which is also training models to replicate Llama</li> <li>Interesting inclusion of GitHub as a training resource</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#why-include-code-in-llm-training-data","title":"Why include code in LLM training data?","text":"<ul> <li>Including code in training data can improve performance on non-code tasks</li> <li>OpenAI found this with their Codex model, which was fine-tuned on code and outperformed GPT-3 on reasoning tasks</li> <li>Since then, people have been adding code to training data</li> <li>Open source dataset called 'the stack' collects code from GitHub while respecting licenses</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#instruction-tuning","title":"Instruction Tuning","text":"<ul> <li>Discusses instruction tuning in GPT models and its impact on performance</li> <li>Mentions the shift from text completion mindset to instruction following mindset</li> <li>Supervised fine-tuning helps models become better at zero-shot tasks by using data sets of zero-shot inputs and desired outputs</li> <li>OpenAI hired thousands of contractors to gather zero-shot data and used reinforcement learning for training</li> <li>GPT model lineage includes DaVinci, Codex, and various iterations, fine-tuning for specific applications</li> <li>Fine-tuning imposes an \"alignment tax,\" decreasing few-shot learning ability and model's confidence calibration</li> <li>Llama model by Stanford team used GPT-3 generated instructions, costing less but with reduced performance compared to GPT-3</li> <li>A specific data set for instruction tuning in chat-based paradigms is called \"Open Assistant\"</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/#notable-llm-retro","title":"Notable LLM: RETRO","text":"<ul> <li>Discussing a model called \"retrieval enhancing\" from DeepMind</li> <li>Goal: train a smaller model good at reasoning and writing code, but looks up facts from a database</li> <li>Used \"burden-coded\" sentences in a trillion-token database for fact retrieval</li> <li>Not as effective as large language models yet, but shows potential for the future</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/","title":"Chapter summaries","text":""},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#intro","title":"Intro","text":"<ul> <li>Discuss four key ideas in machine learning</li> <li>Address diverse audience, including experts, executives, and investors</li> <li>Cover Transformer architecture</li> <li>Mention notable LLMs (e.g., GPT, T5, BERT, etc.)</li> <li>Share details on running a Transformer</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#foundations-of-machine-learning","title":"Foundations of Machine Learning","text":"<ul> <li>Machine learning has shifted from traditional programming (Software 1.0) to a Software 2.0 mindset, where algorithms are generated from training data and more emphasis is placed on the training system. </li> <li>Three types of machine learning include unsupervised learning, supervised learning, and reinforcement learning, which have mostly converged to a supervised learning approach.</li> <li>For machines, input and output are always just numbers, represented as vectors or matrices.</li> <li>One dominant approach to machine learning today is neural networks, also known as deep learning, which was inspired by the human brain's structure and function.</li> <li>Neural networks consist of perceptrons connected in layers, and all operations are matrix multiplications.</li> <li>GPUs, originally developed for graphics and video games, have played a significant role in advancing deep learning due to their compatibility with matrix multiplications.</li> <li>To train a neural network, data is typically split into training, validation, and test sets to avoid overfitting and improve model performance.</li> <li>Pre-training involves training a large model on extensive data, which can then be fine-tuned using smaller sets of specialized data for better performance.</li> <li>Model hubs, such as Hugging Face, offer numerous pre-trained models for various machine learning tasks and have seen significant growth in recent years.</li> <li>The Transformer model has become the dominant architecture for a wide range of machine learning tasks.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#the-transformer-architecture","title":"The Transformer Architecture","text":"<ul> <li>Transformer architecture introduced in 2017 paper \"Attention is All You Need\"</li> <li>Set state-of-the-art results in translation tasks</li> <li>Applied to other NLP tasks and fields like vision</li> <li>Appears complicated but consists of two similar halves</li> <li>Focusing on one half called the decoder</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#transformer-decoder-overview","title":"Transformer Decoder Overview","text":"<ul> <li>The task of the Transformer decoder is to complete text, much like GPT models.</li> <li>The input consists of a sequence of tokens (e.g., \"it's a blue\"), and the goal is to predict the next word (e.g., \"sundress\").</li> <li>The output is a probability distribution over potential next tokens.</li> <li>Inference involves sampling a token from the distribution, appending it to the input, and running the model again with the updated input.</li> <li>ChatGPT operates by seeing user input, sampling the next word, appending it, and repeating this process.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#inputs","title":"Inputs","text":"<ul> <li>Inputs need to be vectors of numbers</li> <li>Text is turned into vectors through tokenization</li> <li>Tokens are assigned an ID in a vocabulary, rather than being words</li> <li>Numbers are represented as vectors using one-hot encoding (e.g., number 3 represented by a vector with 1 in third position, zeros everywhere else)</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#input-embedding","title":"Input Embedding","text":"<ul> <li>One-hot vectors are not good representations of words or tokens as they don't capture the notion of similarity between words</li> <li>To address the issue, we use embedding</li> <li>Embedding involves learning an embedding matrix which converts a one-hot vocabulary encoding into a dense vector of chosen dimensionalities</li> <li>This process turns words into dense embeddings, making it the simplest neural network layer type</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#masked-multi-head-attention","title":"Masked Multi-Head Attention","text":"<ul> <li>Attention was introduced in 2015 for translation tasks, and the idea is to predict the most likely next token based on the importance of previous tokens.</li> <li>Attention mechanism involves an output as a weighted sum of input vectors, and these weights are calculated using dot products (similarities) between the input vectors.</li> <li>Each input vector plays three roles in the attention mechanism: as a query, key, and value.</li> <li>To learn and improve attention, input vectors can be projected into different roles (query, key, and value) by multiplying them with learnable matrices.</li> <li>Multi-head attention refers to learning several different ways of transforming inputs into queries, keys, and values simultaneously.</li> <li>Masking is used to prevent the model from \"cheating\" by considering future tokens; it ensures that the model only predicts the next token based on the already seen input.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#positional-encoding","title":"Positional Encoding","text":"<ul> <li>No notion of position in the current model, only whether something has been seen or not.</li> <li>Positional encoding is introduced to provide ordering among the seen elements.</li> <li>Current equations resemble a bag of unordered items.</li> <li>Positional encoding vectors are added to embedding vectors to provide order.</li> <li>Seems counterintuitive, but it works; attention mechanism figures out relevant positions.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#skip-connections-and-layer-norm","title":"Skip Connections and Layer Norm","text":"<ul> <li>Add up and norm attention outputs using skip connections and layer normalization</li> <li>Skip connections help propagate loss from end to beginning of model during backpropagation</li> <li>Layer normalization resets mean and standard deviation to uniform after every operation</li> <li>Input embedding determines the dimension of the entire Transformer model</li> <li>Normalization seems inelegant but is very effective in improving neural net learning</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#feed-forward-layer","title":"Feed-forward Layer","text":"<ul> <li>Feed forward layer is similar to the standard multi-layer perceptron.</li> <li>It receives tokens augmented with relevant information.</li> <li>The layer upgrades the token representation.</li> <li>The process goes from word-level to thought-level, with more semantic meaning.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#transformer-hyperparameters-and-why-they-work-so-well","title":"Transformer hyperparameters and Why they work so well","text":"<ul> <li>GPT-3 model ranges from 12 to 96 layers of Transformer layers with adjustable embedding dimensions and attention heads, totaling 175 billion parameters.</li> <li>Most of GPT-3's parameters are in the feed forward layer, but for smaller models, a significant portion is in embedding and attention.</li> <li>Transformers are effective general-purpose differentiable computers that are expressive, optimizable via backpropagation, and efficient due to parallel processing.</li> <li>Understanding exact expressiveness of the Transformer is ongoing, with interesting results like RASP (a programming language designed to be implemented within a Transformer).</li> <li>Decompiling Transformer weights back to a program is still an unsolved problem.</li> <li>Multiple attention heads allow the model to figure out how to use a second head, showcased in work like Induction Heads.</li> <li>Learning to code Transformers isn't necessary for AI-powered products, but can be fun and educational. Resources like YouTube tutorials and code examples are available to assist in learning.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#notable-llm-bert","title":"Notable LLM: BERT","text":"<ul> <li>Bert, T5, and GPT cover the gamut of large Transformer models</li> <li>Bert stands for bi-directional encoder representation from Transformers</li> <li>Bert uses the encoder part of the Transformer, with unmasked attention</li> <li>Bert contains 100 million parameters, considered large at its time</li> <li>Bert was trained by masking 15% of words in a text corpus and predicting the masked words</li> <li>Bert became a building block for other NLP applications</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#notable-llm-t5","title":"Notable LLM: T5","text":"<ul> <li>T5 applies Transformer architecture to text-to-text transfer, meaning both input and output are text strings</li> <li>The task is encoded in the input string and can involve translation, summarization, etc.</li> <li>Encoder-decoder architecture was found to be best, with 11 billion parameters</li> <li>Trained on Colossal Queen Crawl Corpus (C4) derived from Common Crawl dataset</li> <li>C4 was created by filtering out short pages, offensive content, pages with code, and de-duplicating data</li> <li>Fine-tuned using academic supervised tasks for various NLP applications</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#notable-llm-gpt","title":"Notable LLM: GPT","text":"<ul> <li>GPT is a generative pre-trained Transformer, with GPT-2 being decoder only</li> <li>GPT-2 was trained on a dataset called WebText created by scraping links from Reddit</li> <li>GPT tokenizes text using byte pair encoding, a middle ground between old-school tokenization and using UTF-8 bytes</li> <li>GPT-3 came out in 2020 and is 100 times larger than GPT-2, enabling few-shot and zero-shot learning</li> <li>GPT-3 was trained on webtext, raw common crawl data, a selection of books, and all of Wikipedia</li> <li>The dataset for GPT-3 contained 500 billion tokens, but it was only trained on 300 billion tokens</li> <li>GPT-4 details are unknown, but it is assumed to be much larger than previous versions due to the trend in increasing size</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#notable-llm-chinchilla-and-scaling-laws","title":"Notable LLM: Chinchilla and Scaling Laws","text":"<ul> <li>Using more computation to train AI systems improves their performance</li> <li>Rich Sutton's \"bitter lesson\": advantage goes to those stacking more layers</li> <li>DeepMind's paper, Training Compute Optimal LLMs: studied relationship between model size, compute and data set size</li> <li>Most LLMs in literature had too many parameters for their data amount</li> <li>Chinchilla model (70 billion) outperformed Gopher model (four times larger) by training on 1.4 trillion tokens instead of 300 billion</li> <li>Open question: can models continue to improve by training repeatedly on existing data?</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#notable-llm-llama","title":"Notable LLM: LLaMA","text":"<ul> <li>Llama is an open-source chinchilla optimal LLM from Meta Research</li> <li>Several sizes available, ranging from 7 billion to 65 billion, with at least 1 trillion tokens</li> <li>Competitively benchmarks against GPT-3 and other state-of-the-art LLMs</li> <li>Open source but non-commercial license for pre-trained weights</li> <li>Trained on custom common crawl filtering, C4, GitHub, Wikipedia, books, and scientific papers</li> <li>Data set replicated by Red Pajama, which is also training models to replicate Llama</li> <li>Interesting inclusion of GitHub as a training resource</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#why-include-code-in-llm-training-data","title":"Why include code in LLM training data?","text":"<ul> <li>Including code in training data can improve performance on non-code tasks</li> <li>OpenAI found this with their Codex model, which was fine-tuned on code and outperformed GPT-3 on reasoning tasks</li> <li>Since then, people have been adding code to training data</li> <li>Open source dataset called 'the stack' collects code from GitHub while respecting licenses</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#instruction-tuning","title":"Instruction Tuning","text":"<ul> <li>Discusses instruction tuning in GPT models and its impact on performance</li> <li>Mentions the shift from text completion mindset to instruction following mindset</li> <li>Supervised fine-tuning helps models become better at zero-shot tasks by using data sets of zero-shot inputs and desired outputs</li> <li>OpenAI hired thousands of contractors to gather zero-shot data and used reinforcement learning for training</li> <li>GPT model lineage includes DaVinci, Codex, and various iterations, fine-tuning for specific applications</li> <li>Fine-tuning imposes an \"alignment tax,\" decreasing few-shot learning ability and model's confidence calibration</li> <li>Llama model by Stanford team used GPT-3 generated instructions, costing less but with reduced performance compared to GPT-3</li> <li>A specific data set for instruction tuning in chat-based paradigms is called \"Open Assistant\"</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llm-foundations/chapter_summaries/#notable-llm-retro","title":"Notable LLM: RETRO","text":"<ul> <li>Discussing a model called \"retrieval enhancing\" from DeepMind</li> <li>Goal: train a smaller model good at reasoning and writing code, but looks up facts from a database</li> <li>Used \"burden-coded\" sentences in a trillion-token database for fact retrieval</li> <li>Not as effective as large language models yet, but shows potential for the future</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/","title":"LLMOps","text":"<p>Lecture by Josh Tobin. Published May 9, 2023. Download slides.</p>"},{"location":"llm-bootcamp/spring-2023/llmops/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/llmops/#why-llmops","title":"Why LLMOps?","text":"<ul> <li>Topic of lecture core to whole ethos of full stack deep learning</li> <li>Started five years ago in AI hype cycle focusing on deep learning</li> <li>Classes teach about building with neural networks, but not getting into production</li> <li>Philosophy carried throughout the development of courses</li> <li>Focus on building applications with language models and considerations for production systems</li> <li>Space for real production systems with language models is underdeveloped</li> <li>Lecture will cover assorted topics related to building these applications</li> <li>Provide high-level pointers, initial choices, and resources for learning more</li> <li>Aim to tie topics together into a first-pass theory for \"LLMops\"</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/#choosing-your-base-llm","title":"Choosing your base LLM","text":"<ul> <li>Building an application on top of LLMs requires choosing which model to use; the best model depends on trade-offs, such as quality, speed, cost, tunability, and data security.</li> <li>For most use cases, GPT4 is a good starting point.</li> <li>Proprietary models, like GPT4 and Anthropic, are usually higher quality, but open source models offer more customization and better data security.</li> <li>Consider licensing when choosing an open source model: permissive licenses (e.g., Apache 2.0) offer more freedom, whereas restricted licenses limit commercial use.</li> <li>Be cautious with \"open source\" models released under non-commercial licenses, as they restrict commercial use and may not truly be open source.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/#proprietary-llms","title":"Proprietary LLMs","text":"<ul> <li>Discussed proprietary models and ranked them using criteria: number of parameters, size of context window, type of training data, subjective quality score, speed of inference, and fine-tunability.</li> <li>Number of parameters and training data are proxies for model quality; context window crucial for model usefulness in downstream applications.</li> <li>Four types of training data: diverse, code, instructions, and human feedback; few models use all four types.</li> <li>Quality best determined using benchmarks and hands-on evaluations.</li> <li>GPT-4 recognized as the highest quality model, followed by GPT-3.5 for a faster and cheaper option.</li> <li>Claude from Anthropic and Cohere's largest model compete for quality and fine-tunability.</li> <li>For a trade-off of quality in favor of speed and cost, consider Anthropic's offering or alternatives from OpenAI and Cohere.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/#open-source-llms","title":"Open-source LLMs","text":"<ul> <li>Large language models have both proprietary and open-source options</li> <li>Open-source options include T5, Flan T5, Pythia, Dolly, Stable-LM, Llama, Alpaca, Vicuna, Koala, and Opt</li> <li>T5 and Flan-T5 have permissive licenses while other options may have license restrictions</li> <li>Llama ecosystem is well-supported by the community, but not ideal for production</li> <li>Benchmarks can mislead, assess language model performance on specific tasks</li> <li>Start projects with GPT-4 to prototype, downsize to GPT-3.5 or Claude if cost/latency is a concern</li> <li>Cohere is the best for fine-tuning among commercial providers</li> <li>Open-source may catch up with GPT-3.5 level performance by the end of the year</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/#iteration-and-prompt-management","title":"Iteration and prompt management","text":"<ul> <li>I believe prompt engineering is currently missing tools to make it more like engineering and less like ad hoc experimentation.</li> <li>Experiment management was impactful in the deep learning world because experiments took a long time to run and there were many parallel experiments, which prompt engineering typically doesn't have.</li> <li>I suggest three levels of tracking experiments with prompts and chains: 1) Doing nothing and using OpenAI Playground, 2) Tracking prompts in Git, and 3) Using specialized tracking tools for prompts (if necessary).</li> <li>Most teams should use Git for tracking as it's easy and fits into their current workflows.</li> <li>Specialized prompt tracking tools should be decoupled from Git and provide a UI for non-technical stakeholders.</li> <li>Keep an eye out for new tools in this space, as it's rapidly evolving with recent announcements from major providers like Weights &amp; Biases, Comet, and MLflow.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/#testing-llms-why-and-why-is-it-hard","title":"Testing LLMs: Why and why is it hard?","text":"<ul> <li>To ensure changes to a model or prompt are effective, measure performance on a wide range of data representing end-user inputs.</li> <li>User retention for AI-powered applications depends on trust and reliable output.</li> <li>Traditional machine learning model testing involves training sets, held-out data, and measuring accuracy, but language models present unique challenges:</li> <li>You don't know the training data used by API providers like OpenAI.</li> <li>Production distribution is always different than training distribution.</li> <li>Metrics are less straightforward and might not capture the diverse behaviors of the model.</li> <li>Language models require a more diverse understanding of behaviors and qualitative output measurement.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/#testing-llms-what-works","title":"Testing LLMs: What works?","text":"<ul> <li>Two key questions for testing language models: what data to test them on and what metrics to compute on that data</li> <li>Build evaluation sets incrementally, starting from the beginning while prototyping the model</li> <li>Add interesting examples to the dataset, focusing on hard examples where the model struggles and different examples that aren't common in the dataset</li> <li>Utilize the language model to help generate diverse test cases by creating prompts for the tasks you're trying to solve</li> <li>As the model rolls out to more users, keep adding data to the dataset, considering user dislikes and underrepresented topics for inclusion</li> <li>Consider the concept of test coverage, aiming for an evaluation set that covers the types of tasks users will actually perform in the system</li> <li>Test coverage and distribution shift are analogous, but measure different aspects of data relationships</li> <li>To be effective, test reliability should measure the difference between online and offline performance, ensuring that metrics are relevant to real-world user experiences.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/#evaluation-metrics-for-llms","title":"Evaluation metrics for LLMs","text":"<ul> <li>Evaluation metrics for language models depend on the availability of a correct answer, reference answer, previous answer, or human feedback.</li> <li>If there's a correct answer, use metrics like accuracy.</li> <li>With a reference answer, employ reference matching metrics like semantic similarity or factual consistency.</li> <li>If there's a previous answer, ask another language model which answer is better.</li> <li>When human feedback is available, check if the answer incorporates the feedback.</li> <li>If none of these options apply, verify output structure or ask the model to grade the answer.</li> <li>Although automatic evaluation is desirable for faster experimentation, manual checks still play an essential role.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/#deployment-and-monitoring","title":"Deployment and monitoring","text":"<ul> <li>Deploying LLM (Language Model) APIs can be simple, but becomes more complex if there's a lot of logic behind API calls.</li> <li>Techniques to improve LLM output quality include self-critique, sampling multiple outputs, and ensemble techniques.</li> <li>Monitoring LLMs involves looking at user satisfaction and defining performance metrics, like response length or common issues in production.</li> <li>Gather user feedback via low friction methods, such as thumbs up/down or short messages.</li> <li>Common issues with LLMs in production include UI problems, latency, incorrect answers, long-winded responses, and prompt injection attacks.</li> <li>Use user feedback to improve prompts by finding and addressing themes or problems.</li> <li>Fine-tuning LLMs can be done through supervised fine-tuning or human feedback, though the latter is more challenging.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/#test-driven-development-for-llms","title":"Test-driven development for LLMs","text":"<ul> <li>Rapidly evolving field with no established best practices yet</li> <li>Aim to provide main questions and resources for building applications with LLMS</li> <li>Introduce a potential structured process: test-driven or behavior-driven development</li> <li>Main components of process are prompt/chain development, deployment, user feedback, and logging/monitoring</li> <li>Use interaction data from user feedback to improve model, extract test data, and iterate on prompts</li> <li>As complexity increases, consider fine-tuning workflow with additional training data</li> <li>Virtuous cycle of improvement as interaction data from users increases and informs subsequent iterations</li> <li>Process repeats with individual developer, team, and end-users involved in feedback and improvements</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/","title":"Chapter summaries","text":""},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/#why-llmops","title":"Why LLMOps?","text":"<ul> <li>Topic of lecture core to whole ethos of full stack deep learning</li> <li>Started five years ago in AI hype cycle focusing on deep learning</li> <li>Classes teach about building with neural networks, but not getting into production</li> <li>Philosophy carried throughout the development of courses</li> <li>Focus on building applications with language models and considerations for production systems</li> <li>Space for real production systems with language models is underdeveloped</li> <li>Lecture will cover assorted topics related to building these applications</li> <li>Provide high-level pointers, initial choices, and resources for learning more</li> <li>Aim to tie topics together into a first-pass theory for \"LLMops\"</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/#choosing-your-base-llm","title":"Choosing your base LLM","text":"<ul> <li>Building an application on top of LLMs requires choosing which model to use; the best model depends on trade-offs, such as quality, speed, cost, tunability, and data security.</li> <li>For most use cases, GPT4 is a good starting point.</li> <li>Proprietary models, like GPT4 and Anthropic, are usually higher quality, but open source models offer more customization and better data security.</li> <li>Consider licensing when choosing an open source model: permissive licenses (e.g., Apache 2.0) offer more freedom, whereas restricted licenses limit commercial use.</li> <li>Be cautious with \"open source\" models released under non-commercial licenses, as they restrict commercial use and may not truly be open source.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/#proprietary-llms","title":"Proprietary LLMs","text":"<ul> <li>Discussed proprietary models and ranked them using criteria: number of parameters, size of context window, type of training data, subjective quality score, speed of inference, and fine-tunability.</li> <li>Number of parameters and training data are proxies for model quality; context window crucial for model usefulness in downstream applications.</li> <li>Four types of training data: diverse, code, instructions, and human feedback; few models use all four types.</li> <li>Quality best determined using benchmarks and hands-on evaluations.</li> <li>GPT-4 recognized as the highest quality model, followed by GPT-3.5 for a faster and cheaper option.</li> <li>Claude from Anthropic and Cohere's largest model compete for quality and fine-tunability.</li> <li>For a trade-off of quality in favor of speed and cost, consider Anthropic's offering or alternatives from OpenAI and Cohere.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/#open-source-llms","title":"Open-source LLMs","text":"<ul> <li>Large language models have both proprietary and open-source options</li> <li>Open-source options include T5, Flan T5, Pythia, Dolly, Stable-LM, Llama, Alpaca, Vicuna, Koala, and Opt</li> <li>T5 and Flan-T5 have permissive licenses while other options may have license restrictions</li> <li>Llama ecosystem is well-supported by the community, but not ideal for production</li> <li>Benchmarks can mislead, assess language model performance on specific tasks</li> <li>Start projects with GPT-4 to prototype, downsize to GPT-3.5 or Claude if cost/latency is a concern</li> <li>Cohere is the best for fine-tuning among commercial providers</li> <li>Open-source may catch up with GPT-3.5 level performance by the end of the year</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/#iteration-and-prompt-management","title":"Iteration and prompt management","text":"<ul> <li>I believe prompt engineering is currently missing tools to make it more like engineering and less like ad hoc experimentation.</li> <li>Experiment management was impactful in the deep learning world because experiments took a long time to run and there were many parallel experiments, which prompt engineering typically doesn't have.</li> <li>I suggest three levels of tracking experiments with prompts and chains: 1) Doing nothing and using OpenAI Playground, 2) Tracking prompts in Git, and 3) Using specialized tracking tools for prompts (if necessary).</li> <li>Most teams should use Git for tracking as it's easy and fits into their current workflows.</li> <li>Specialized prompt tracking tools should be decoupled from Git and provide a UI for non-technical stakeholders.</li> <li>Keep an eye out for new tools in this space, as it's rapidly evolving with recent announcements from major providers like Weights &amp; Biases, Comet, and MLflow.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/#testing-llms-why-and-why-is-it-hard","title":"Testing LLMs: Why and why is it hard?","text":"<ul> <li>To ensure changes to a model or prompt are effective, measure performance on a wide range of data representing end-user inputs.</li> <li>User retention for AI-powered applications depends on trust and reliable output.</li> <li>Traditional machine learning model testing involves training sets, held-out data, and measuring accuracy, but language models present unique challenges:</li> <li>You don't know the training data used by API providers like OpenAI.</li> <li>Production distribution is always different than training distribution.</li> <li>Metrics are less straightforward and might not capture the diverse behaviors of the model.</li> <li>Language models require a more diverse understanding of behaviors and qualitative output measurement.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/#testing-llms-what-works","title":"Testing LLMs: What works?","text":"<ul> <li>Two key questions for testing language models: what data to test them on and what metrics to compute on that data</li> <li>Build evaluation sets incrementally, starting from the beginning while prototyping the model</li> <li>Add interesting examples to the dataset, focusing on hard examples where the model struggles and different examples that aren't common in the dataset</li> <li>Utilize the language model to help generate diverse test cases by creating prompts for the tasks you're trying to solve</li> <li>As the model rolls out to more users, keep adding data to the dataset, considering user dislikes and underrepresented topics for inclusion</li> <li>Consider the concept of test coverage, aiming for an evaluation set that covers the types of tasks users will actually perform in the system</li> <li>Test coverage and distribution shift are analogous, but measure different aspects of data relationships</li> <li>To be effective, test reliability should measure the difference between online and offline performance, ensuring that metrics are relevant to real-world user experiences.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/#evaluation-metrics-for-llms","title":"Evaluation metrics for LLMs","text":"<ul> <li>Evaluation metrics for language models depend on the availability of a correct answer, reference answer, previous answer, or human feedback.</li> <li>If there's a correct answer, use metrics like accuracy.</li> <li>With a reference answer, employ reference matching metrics like semantic similarity or factual consistency.</li> <li>If there's a previous answer, ask another language model which answer is better.</li> <li>When human feedback is available, check if the answer incorporates the feedback.</li> <li>If none of these options apply, verify output structure or ask the model to grade the answer.</li> <li>Although automatic evaluation is desirable for faster experimentation, manual checks still play an essential role.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/#deployment-and-monitoring","title":"Deployment and monitoring","text":"<ul> <li>Deploying LLM (Language Model) APIs can be simple, but becomes more complex if there's a lot of logic behind API calls.</li> <li>Techniques to improve LLM output quality include self-critique, sampling multiple outputs, and ensemble techniques.</li> <li>Monitoring LLMs involves looking at user satisfaction and defining performance metrics, like response length or common issues in production.</li> <li>Gather user feedback via low friction methods, such as thumbs up/down or short messages.</li> <li>Common issues with LLMs in production include UI problems, latency, incorrect answers, long-winded responses, and prompt injection attacks.</li> <li>Use user feedback to improve prompts by finding and addressing themes or problems.</li> <li>Fine-tuning LLMs can be done through supervised fine-tuning or human feedback, though the latter is more challenging.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/llmops/chapter_summaries/#test-driven-development-for-llms","title":"Test-driven development for LLMs","text":"<ul> <li>Rapidly evolving field with no established best practices yet</li> <li>Aim to provide main questions and resources for building applications with LLMS</li> <li>Introduce a potential structured process: test-driven or behavior-driven development</li> <li>Main components of process are prompt/chain development, deployment, user feedback, and logging/monitoring</li> <li>Use interaction data from user feedback to improve model, extract test data, and iterate on prompts</li> <li>As complexity increases, consider fine-tuning workflow with additional training data</li> <li>Virtuous cycle of improvement as interaction data from users increases and informs subsequent iterations</li> <li>Process repeats with individual developer, team, and end-users involved in feedback and improvements</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/","title":"Learn to Spell: Prompt Engineering","text":"<p>Lecture by Charles Frye. Published May 9, 2023. Download slides.</p>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#intro","title":"Intro","text":"<ul> <li>Dive into technical skills for using language models</li> <li>Focus on prompt engineering: designing text input to get desired behavior from language models</li> <li>Language models replace traditional training and fine-tuning techniques in machine learning</li> <li>Programming language models is like programming in English instead of coding languages</li> <li>High-level intuitions for prompt engineering: prompts as magic spells</li> <li>Discuss emerging playbook for effective prompting, including techniques to get desired output from language models</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#language-models-are-statistical-models-of-text","title":"Language models are statistical models of text","text":"<ul> <li>Prompts are not literal magic spells; they are based on linear algebra.</li> <li>Language models are statistical models of text, similar to how a bell curve is a statistical model of data.</li> <li>Language models are trained by going through text and predicting the probability of the next word, which is called an auto-regressive model.</li> <li>These models start with random weights, eventually learning to assign high probabilities to text that resembles real-world samples.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#but-statistical-model-gives-bad-intuition","title":"But \"statistical model\" gives bad intuition","text":"<ul> <li>Language models can be thought of as statistical pattern matchers, but this can also give bad intuitions.</li> <li>Traditional simple statistical models, like linear regression, are not the best way to think about language models.</li> <li>A better intuition comes from probabilistic programs, which allow manipulation of random variables and can represent complex statistics.</li> <li>Probabilistic programs can be represented by graphical models, providing insight into complex text models.</li> <li>The Language Model Cascades paper by Dohan et al. dives into detail on probabilistic programs and their applications to language models.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#prompts-are-magic-spells","title":"Prompts are magic spells","text":"<ul> <li>Drawing inspiration from Arthur C Clarke's laws of technology, which suggests that advanced technology is similar to magic</li> <li>Prompts are like magic spells, using words to achieve impossible effects but requiring complex rules</li> <li>Spending too much time learning these complex rules can negatively impact mental health</li> <li>Three magical intuitions for using prompts: </li> <li>Pre-trained models (e.g. GPT-3, Llama) - prompts are portals to alternate universes</li> <li>Instruction-tuned models (e.g. ChatGPT, Alpaca) - prompts are used to make wishes</li> <li>Agent simulation (latest language models) - prompts create a Golem</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#prompts-are-portals-to-alternate-universes","title":"Prompts are portals to alternate universes","text":"<ul> <li>The language model creates a portal to an alternate universe where desired documents exist by weighting all possible documents based on their probability.</li> <li>The primary goal of prompting is subtractive; it focuses the mass of predictions to hone in on a specific world by conditioning the probabilistic model.</li> <li>The language model can generate text from nearby universes for similarities, but cannot provide specific or novel information from another universe (e.g., a cure for cancer).</li> <li>The model can help find ideas and documents similar to existing ones or combine ideas that haven't been combined yet.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#a-prompt-can-make-a-wish-come-true","title":"A prompt can make a wish come true","text":"<ul> <li>Core intuition: Language models shape and sculpt from the set of all possible documents and universes; Instruction-tuned models (like ChatGPT) can respond to wishes and commands.</li> <li>An example of overcoming bias: Asking the model to ensure answers are unbiased and do not rely on stereotypes greatly improves performance.</li> <li>Be precise when prompting language models and learn the rules the \"genie\" operates by.</li> <li>Suggestions to improve instructional prompts:</li> <li>Simplify and focus on low-level patterns of text rather than conversational complexity.</li> <li>Turn descriptions into bulleted lists; language models tend to only focus on the beginning of descriptions.</li> <li>Replace negation statements with assertions (e.g., instead of \"don't be stereotyped,\" say, \"please ensure your answer does not rely on stereotypes\").</li> <li>Instruction fine-tuned models are essentially like annotators with no context; treat them as such for better performance.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#a-prompt-can-create-a-golem","title":"A prompt can create a golem","text":"<ul> <li>Large language models can create \"golems\" or artificial agents with specific personas, similar to the golem creature from Jewish folklore</li> <li>Personas in language models can help improve performance on tasks like translations by putting the model into a situational context</li> <li>People have created models with detailed personas in various settings, including video game worlds</li> <li>Language models become better by internally modeling processes that produce text, such as understanding the context and environment in which utterances are made</li> <li>Natural language processing faces challenges with large language models as they may lack communicative intentions, which humans naturally have</li> <li>By designing prompts carefully, one can get a language model to simulate agents, improving its predictions and understanding of context.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#limitations-of-llms-as-simulators","title":"Limitations of LLMs as simulators","text":"<ul> <li>Our universal simulators are trained on text humans have written, not on all data or states of the universe.</li> <li>Simulations will be related to human-written data, like fictional super intelligences (e.g. HAL 9000), not actual super intelligent AIs.</li> <li>Language models can simulate human thinking well for short timeframes (e.g. reactions to social media posts), but struggle for longer periods and personal contexts.</li> <li>Models can perform well in simulating fictional personas and can approximate calculators or interpreters, but cannot replace them or access live data.</li> <li>Wherever possible, replace weak simulators with the real deal (e.g. run Python code in an actual kernel).</li> <li>Pre-trained models are mostly alternate universe document generators, and can be agent simulators with varying quality depending on the model and agent.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#prompting-techniques-are-mostly-tricks","title":"Prompting techniques are mostly tricks","text":"<ul> <li>This section focuses on prompt engineering tricks and techniques.</li> <li>Many prompt engineering papers can actually be summarized in a few sentences, but they include pages of benchmark marketing.</li> <li>There isn't much depth to these tricks, unlike the core language modeling aspect which has mathematical depth.</li> <li>Two things to be cautious of: few-shot learning as an approach and potential issues with tokenization.</li> <li>I will discuss some misconceptions and provide tips for handling these issues.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#few-shot-learning-isnt-the-right-model-for-prompting","title":"Few-shot learning isn't the right model for prompting","text":"<ul> <li>Language models like GPT-3 can learn tasks from prompts, but it was unclear if they would actually be useful.</li> <li>The GPT-3 paper called these models \"few-shot learners\" and showed they can learn tasks like math and translation.</li> <li>However, the model often struggles to move away from pre-training knowledge.</li> <li>For example, GPT-3 tends to ignore permuted labels for sentiment analysis and sticks to its original understanding.</li> <li>Latest language models can handle permuted labels, but not perfectly, and require many examples to accomplish this.</li> <li>Treating the prompt as a way to do few-shot learning might not be an ideal approach.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#character-level-operations-are-hard","title":"Character-level operations are hard","text":"<ul> <li>Models see tokens, not characters; they struggle with tasks like rotating and reversing words</li> <li>Adding spaces between letters can change tokenization and improve performance</li> <li>GPT-4 can handle some challenges (e.g. summary with words starting with G) but still has limitations</li> <li>For tasks like string manipulation, it's better to use traditional programming instead of language models</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/#the-prompting-playbook-reasoning-reflection-ensembling","title":"The prompting playbook: reasoning, reflection, &amp; ensembling","text":"<ul> <li>Language models perform well with formatted text; using structured text like pseudocode can improve results</li> <li>Decompose tasks into smaller pieces in your prompt to make the language model generate each piece; automate decomposition for better performance</li> <li>Elicit reasoning capabilities from the model by carefully tuning the prompt, such as using \"Let's think step-by-step\"</li> <li>Ensemble results of multiple models for more accurate answers and use randomness for greater heterogeneity in responses</li> <li>Combine prompting techniques (e.g., few-shot, Chain of Thought, ensembling) to increase performance, but be mindful of the impact on latency and compute costs</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/","title":"Chapter summaries","text":""},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#intro","title":"Intro","text":"<ul> <li>Dive into technical skills for using language models</li> <li>Focus on prompt engineering: designing text input to get desired behavior from language models</li> <li>Language models replace traditional training and fine-tuning techniques in machine learning</li> <li>Programming language models is like programming in English instead of coding languages</li> <li>High-level intuitions for prompt engineering: prompts as magic spells</li> <li>Discuss emerging playbook for effective prompting, including techniques to get desired output from language models</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#language-models-are-statistical-models-of-text","title":"Language models are statistical models of text","text":"<ul> <li>Prompts are not literal magic spells; they are based on linear algebra.</li> <li>Language models are statistical models of text, similar to how a bell curve is a statistical model of data.</li> <li>Language models are trained by going through text and predicting the probability of the next word, which is called an auto-regressive model.</li> <li>These models start with random weights, eventually learning to assign high probabilities to text that resembles real-world samples.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#but-statistical-model-gives-bad-intuition","title":"But \"statistical model\" gives bad intuition","text":"<ul> <li>Language models can be thought of as statistical pattern matchers, but this can also give bad intuitions.</li> <li>Traditional simple statistical models, like linear regression, are not the best way to think about language models.</li> <li>A better intuition comes from probabilistic programs, which allow manipulation of random variables and can represent complex statistics.</li> <li>Probabilistic programs can be represented by graphical models, providing insight into complex text models.</li> <li>The Language Model Cascades paper by Dohan et al. dives into detail on probabilistic programs and their applications to language models.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#prompts-are-magic-spells","title":"Prompts are magic spells","text":"<ul> <li>Drawing inspiration from Arthur C Clarke's laws of technology, which suggests that advanced technology is similar to magic</li> <li>Prompts are like magic spells, using words to achieve impossible effects but requiring complex rules</li> <li>Spending too much time learning these complex rules can negatively impact mental health</li> <li>Three magical intuitions for using prompts: </li> <li>Pre-trained models (e.g. GPT-3, Llama) - prompts are portals to alternate universes</li> <li>Instruction-tuned models (e.g. ChatGPT, Alpaca) - prompts are used to make wishes</li> <li>Agent simulation (latest language models) - prompts create a Golem</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#prompts-are-portals-to-alternate-universes","title":"Prompts are portals to alternate universes","text":"<ul> <li>The language model creates a portal to an alternate universe where desired documents exist by weighting all possible documents based on their probability.</li> <li>The primary goal of prompting is subtractive; it focuses the mass of predictions to hone in on a specific world by conditioning the probabilistic model.</li> <li>The language model can generate text from nearby universes for similarities, but cannot provide specific or novel information from another universe (e.g., a cure for cancer).</li> <li>The model can help find ideas and documents similar to existing ones or combine ideas that haven't been combined yet.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#a-prompt-can-make-a-wish-come-true","title":"A prompt can make a wish come true","text":"<ul> <li>Core intuition: Language models shape and sculpt from the set of all possible documents and universes; Instruction-tuned models (like ChatGPT) can respond to wishes and commands.</li> <li>An example of overcoming bias: Asking the model to ensure answers are unbiased and do not rely on stereotypes greatly improves performance.</li> <li>Be precise when prompting language models and learn the rules the \"genie\" operates by.</li> <li>Suggestions to improve instructional prompts:</li> <li>Simplify and focus on low-level patterns of text rather than conversational complexity.</li> <li>Turn descriptions into bulleted lists; language models tend to only focus on the beginning of descriptions.</li> <li>Replace negation statements with assertions (e.g., instead of \"don't be stereotyped,\" say, \"please ensure your answer does not rely on stereotypes\").</li> <li>Instruction fine-tuned models are essentially like annotators with no context; treat them as such for better performance.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#a-prompt-can-create-a-golem","title":"A prompt can create a golem","text":"<ul> <li>Large language models can create \"golems\" or artificial agents with specific personas, similar to the golem creature from Jewish folklore</li> <li>Personas in language models can help improve performance on tasks like translations by putting the model into a situational context</li> <li>People have created models with detailed personas in various settings, including video game worlds</li> <li>Language models become better by internally modeling processes that produce text, such as understanding the context and environment in which utterances are made</li> <li>Natural language processing faces challenges with large language models as they may lack communicative intentions, which humans naturally have</li> <li>By designing prompts carefully, one can get a language model to simulate agents, improving its predictions and understanding of context.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#limitations-of-llms-as-simulators","title":"Limitations of LLMs as simulators","text":"<ul> <li>Our universal simulators are trained on text humans have written, not on all data or states of the universe.</li> <li>Simulations will be related to human-written data, like fictional super intelligences (e.g. HAL 9000), not actual super intelligent AIs.</li> <li>Language models can simulate human thinking well for short timeframes (e.g. reactions to social media posts), but struggle for longer periods and personal contexts.</li> <li>Models can perform well in simulating fictional personas and can approximate calculators or interpreters, but cannot replace them or access live data.</li> <li>Wherever possible, replace weak simulators with the real deal (e.g. run Python code in an actual kernel).</li> <li>Pre-trained models are mostly alternate universe document generators, and can be agent simulators with varying quality depending on the model and agent.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#prompting-techniques-are-mostly-tricks","title":"Prompting techniques are mostly tricks","text":"<ul> <li>This section focuses on prompt engineering tricks and techniques.</li> <li>Many prompt engineering papers can actually be summarized in a few sentences, but they include pages of benchmark marketing.</li> <li>There isn't much depth to these tricks, unlike the core language modeling aspect which has mathematical depth.</li> <li>Two things to be cautious of: few-shot learning as an approach and potential issues with tokenization.</li> <li>I will discuss some misconceptions and provide tips for handling these issues.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#few-shot-learning-isnt-the-right-model-for-prompting","title":"Few-shot learning isn't the right model for prompting","text":"<ul> <li>Language models like GPT-3 can learn tasks from prompts, but it was unclear if they would actually be useful.</li> <li>The GPT-3 paper called these models \"few-shot learners\" and showed they can learn tasks like math and translation.</li> <li>However, the model often struggles to move away from pre-training knowledge.</li> <li>For example, GPT-3 tends to ignore permuted labels for sentiment analysis and sticks to its original understanding.</li> <li>Latest language models can handle permuted labels, but not perfectly, and require many examples to accomplish this.</li> <li>Treating the prompt as a way to do few-shot learning might not be an ideal approach.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#character-level-operations-are-hard","title":"Character-level operations are hard","text":"<ul> <li>Models see tokens, not characters; they struggle with tasks like rotating and reversing words</li> <li>Adding spaces between letters can change tokenization and improve performance</li> <li>GPT-4 can handle some challenges (e.g. summary with words starting with G) but still has limitations</li> <li>For tasks like string manipulation, it's better to use traditional programming instead of language models</li> </ul>"},{"location":"llm-bootcamp/spring-2023/prompt-engineering/chapter_summaries/#the-prompting-playbook-reasoning-reflection-ensembling","title":"The prompting playbook: reasoning, reflection, &amp; ensembling","text":"<ul> <li>Language models perform well with formatted text; using structured text like pseudocode can improve results</li> <li>Decompose tasks into smaller pieces in your prompt to make the language model generate each piece; automate decomposition for better performance</li> <li>Elicit reasoning capabilities from the model by carefully tuning the prompt, such as using \"Let's think step-by-step\"</li> <li>Ensemble results of multiple models for more accurate answers and use randomness for greater heterogeneity in responses</li> <li>Combine prompting techniques (e.g., few-shot, Chain of Thought, ensembling) to increase performance, but be mindful of the impact on latency and compute costs</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/","title":"Reza Shabani: How to train your own LLM","text":"<p>Lecture by Reza Shabani. Published May 25, 2023. Download slides.</p>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/#why-train-your-own-llms","title":"Why train your own LLMs?","text":"<ul> <li>Topic of the lecture: how to train large language models</li> <li>Reasons for training your own models are customization, reduce dependency, cost efficiency, data privacy and control over updates</li> <li>Lecture covers the process of training Ghostwriter code completion model</li> <li>Ghostwriter is a competitor to Co-pilot, used for code generation</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/#the-modern-llm-stack","title":"The Modern LLM Stack","text":"<ul> <li>Replit uses Databricks for all of their data pipelines, including pre-processing, summary statistics, analytics transformations, and more.</li> <li>Replit also makes use of Hugging Face for data sets, pre-trained models, tokenizers, and inference tools.</li> <li>Mosaic ML is used for GPU nodes and model training, with pre-configured LLM configurations available.</li> <li>The process is divided into three stages: data processing, model training, and deployment/production.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/#data-pipelines-databricks-hugging-face","title":"Data Pipelines: Databricks &amp; Hugging Face","text":"<ul> <li>The data pipeline starts with a large corpus of permissively licensed code data from The Stack.</li> <li>The data set comes from the GitHub archive and undergoes license filtering and near-deduplication.</li> <li>The data set contains programming languages in the hundreds.</li> <li>Databricks is used for processing and transformations, rather than Hugging Face tooling.</li> <li>Databricks allows for more control over the data and enables processing at scale.</li> <li>Proprietary data sources and data sets not on Hugging Face can be included in the training set.</li> <li>The process is tractable and extensible.</li> <li>Pre-processing steps are important in understanding the data set.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/#preprocessing","title":"Preprocessing","text":"<ul> <li>Anonymizing the data is an important pre-processing step, which involves removing emails, IP addresses, and secret keys.</li> <li>Auto-generated code and minified code are also removed using regexes and other heuristics.</li> <li>Code that doesn't compile or is not parsable is removed to remove bugs and improve model training.</li> <li>The team uses filters based on average line length, maximum line length, and percentage of alphanumeric characters.</li> <li>Metrics such as the number of GitHub stars or issues do not necessarily improve model quality.</li> <li>The team also trains its own tokenizer.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/#tokenizer-training","title":"Tokenizer Training","text":"<ul> <li>Tokenizers are made up of a tokenization algorithm and a vocabulary.</li> <li>Standard tokenizers are available on Hugging Face, but custom tokenizers can be trained on domain-specific data.</li> <li>A custom tokenizer can result in a smaller vocabulary, which speeds up model training and inference while capturing more relevant information.</li> <li>The tokenizer feeds back into the data pipeline and the training process, making it an integral part of the model.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/#running-training-mosaicml-weights-biases","title":"Running Training: MosaicML, Weights &amp; Biases","text":"<ul> <li>Mosaic ML provides GPUs from multiple Cloud providers at reduced prices</li> <li>They have well-tuned LLM training configurations for specific models</li> <li>The manager infrastructure is fault-tolerant and has an easy-to-use CLI for training runs</li> <li>The speaker found using Mosaic ML worth it due to these benefits</li> <li>They use Weights &amp; Biases for logging during training runs</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/#testing-evaluation-humaneval-hugging-face","title":"Testing &amp; Evaluation: HumanEval, Hugging Face","text":"<ul> <li>Testing language models is difficult and time-consuming</li> <li>HumanEval is a common dataset for testing code generation models</li> <li>Hugging Face's code inference tool is useful for running tests quickly</li> <li>Running tests for multiple languages and certain tasks, like web completion, is more difficult</li> <li>Models need to be tested on unseen data to prevent bias</li> <li>Models can score well on tests but still not be practical or effective</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/#deployment-fastertransformer-triton-server-k8s","title":"Deployment: FasterTransformer, Triton Server, k8s","text":"<ul> <li>Deployment into production is a complex topic with many factors to consider</li> <li>Replit uses FasterTransformer and NVIDIA's Triton server for optimized performance</li> <li>Trton server allows for multiple model instances per GPU or multiple GPUs per model, with useful features like batching and request cancellation for reducing latency</li> <li>Auto-scaling infrastructure is used for running the models, but there are unique challenges for deployed models such as larger model sizes and specific GPU requirements</li> <li>Dealing with GPU shortages in individual zones is necessary</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/#lessons-learned-data-centrism-eval-and-collaboration","title":"Lessons learned: data-centrism, eval, and collaboration","text":"<ul> <li>Data is the most difficult part of the process</li> <li>Good pipelines are important for scalability and quick iteration</li> <li>Data is a critical factor in model quality and output</li> <li>Human evaluation and user testing are important for model vibes and usefulness</li> <li>Collaboration across the team is key to ensure all moving parts are working together</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/#what-makes-a-good-llm-engineer","title":"What makes a good LLM engineer?","text":"<ul> <li>A good engineer in this field requires a mix of research and engineering mindset</li> <li>Working with data at scale is crucial, including the ability to move data into distributed pipelines</li> <li>A strong technical background in stats, computer science, algorithms, and data structures is important</li> <li>Skilled software development, including familiarity with libraries and frameworks like PyTorch is essential</li> <li>Engineers who appreciate and build in CI/CD help with the fast iteration loop for training models</li> <li>The replit team is hiring for these types of problems and welcomes interested applicants to speak with them about opportunities</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/","title":"Chapter summaries","text":""},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/#why-train-your-own-llms","title":"Why train your own LLMs?","text":"<ul> <li>Topic of the lecture: how to train large language models</li> <li>Reasons for training your own models are customization, reduce dependency, cost efficiency, data privacy and control over updates</li> <li>Lecture covers the process of training Ghostwriter code completion model</li> <li>Ghostwriter is a competitor to Co-pilot, used for code generation</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/#the-modern-llm-stack","title":"The Modern LLM Stack","text":"<ul> <li>Replit uses Databricks for all of their data pipelines, including pre-processing, summary statistics, analytics transformations, and more.</li> <li>Replit also makes use of Hugging Face for data sets, pre-trained models, tokenizers, and inference tools.</li> <li>Mosaic ML is used for GPU nodes and model training, with pre-configured LLM configurations available.</li> <li>The process is divided into three stages: data processing, model training, and deployment/production.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/#data-pipelines-databricks-hugging-face","title":"Data Pipelines: Databricks &amp; Hugging Face","text":"<ul> <li>The data pipeline starts with a large corpus of permissively licensed code data from The Stack.</li> <li>The data set comes from the GitHub archive and undergoes license filtering and near-deduplication.</li> <li>The data set contains programming languages in the hundreds.</li> <li>Databricks is used for processing and transformations, rather than Hugging Face tooling.</li> <li>Databricks allows for more control over the data and enables processing at scale.</li> <li>Proprietary data sources and data sets not on Hugging Face can be included in the training set.</li> <li>The process is tractable and extensible.</li> <li>Pre-processing steps are important in understanding the data set.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/#preprocessing","title":"Preprocessing","text":"<ul> <li>Anonymizing the data is an important pre-processing step, which involves removing emails, IP addresses, and secret keys.</li> <li>Auto-generated code and minified code are also removed using regexes and other heuristics.</li> <li>Code that doesn't compile or is not parsable is removed to remove bugs and improve model training.</li> <li>The team uses filters based on average line length, maximum line length, and percentage of alphanumeric characters.</li> <li>Metrics such as the number of GitHub stars or issues do not necessarily improve model quality.</li> <li>The team also trains its own tokenizer.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/#tokenizer-training","title":"Tokenizer Training","text":"<ul> <li>Tokenizers are made up of a tokenization algorithm and a vocabulary.</li> <li>Standard tokenizers are available on Hugging Face, but custom tokenizers can be trained on domain-specific data.</li> <li>A custom tokenizer can result in a smaller vocabulary, which speeds up model training and inference while capturing more relevant information.</li> <li>The tokenizer feeds back into the data pipeline and the training process, making it an integral part of the model.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/#running-training-mosaicml-weights-biases","title":"Running Training: MosaicML, Weights &amp; Biases","text":"<ul> <li>Mosaic ML provides GPUs from multiple Cloud providers at reduced prices</li> <li>They have well-tuned LLM training configurations for specific models</li> <li>The manager infrastructure is fault-tolerant and has an easy-to-use CLI for training runs</li> <li>The speaker found using Mosaic ML worth it due to these benefits</li> <li>They use Weights &amp; Biases for logging during training runs</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/#testing-evaluation-humaneval-hugging-face","title":"Testing &amp; Evaluation: HumanEval, Hugging Face","text":"<ul> <li>Testing language models is difficult and time-consuming</li> <li>HumanEval is a common dataset for testing code generation models</li> <li>Hugging Face's code inference tool is useful for running tests quickly</li> <li>Running tests for multiple languages and certain tasks, like web completion, is more difficult</li> <li>Models need to be tested on unseen data to prevent bias</li> <li>Models can score well on tests but still not be practical or effective</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/#deployment-fastertransformer-triton-server-k8s","title":"Deployment: FasterTransformer, Triton Server, k8s","text":"<ul> <li>Deployment into production is a complex topic with many factors to consider</li> <li>Replit uses FasterTransformer and NVIDIA's Triton server for optimized performance</li> <li>Trton server allows for multiple model instances per GPU or multiple GPUs per model, with useful features like batching and request cancellation for reducing latency</li> <li>Auto-scaling infrastructure is used for running the models, but there are unique challenges for deployed models such as larger model sizes and specific GPU requirements</li> <li>Dealing with GPU shortages in individual zones is necessary</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/#lessons-learned-data-centrism-eval-and-collaboration","title":"Lessons learned: data-centrism, eval, and collaboration","text":"<ul> <li>Data is the most difficult part of the process</li> <li>Good pipelines are important for scalability and quick iteration</li> <li>Data is a critical factor in model quality and output</li> <li>Human evaluation and user testing are important for model vibes and usefulness</li> <li>Collaboration across the team is key to ensure all moving parts are working together</li> </ul>"},{"location":"llm-bootcamp/spring-2023/shabani-train-your-own/chapter_summaries/#what-makes-a-good-llm-engineer","title":"What makes a good LLM engineer?","text":"<ul> <li>A good engineer in this field requires a mix of research and engineering mindset</li> <li>Working with data at scale is crucial, including the ability to move data into distributed pipelines</li> <li>A strong technical background in stats, computer science, algorithms, and data structures is important</li> <li>Skilled software development, including familiarity with libraries and frameworks like PyTorch is essential</li> <li>Engineers who appreciate and build in CI/CD help with the fast iteration loop for training models</li> <li>The replit team is hiring for these types of problems and welcomes interested applicants to speak with them about opportunities</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/","title":"UX for Language User Interfaces","text":"<p>Lecture by Sergey Karayev and Charles Frye. Published May 9, 2023. Download slides.</p>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#intro","title":"Intro","text":"<ul> <li>Next lecture: user experience for language user interfaces</li> <li>Joint lecture with Charles</li> <li>Discuss principles of user interfaces</li> <li>How to build great interfaces</li> <li>Brief history of language user interface pattern</li> <li>Include case studies</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#a-brief-history-of-user-interfaces","title":"A brief history of user interfaces","text":"<ul> <li>User interfaces are where a person meets the world and have historically been analog, continuous, and physical.</li> <li>Language was the first digital interface, followed by writing, and later, computer terminals and graphical user interfaces.</li> <li>Web interfaces became more text-based with hypertext, links, and text boxes.</li> <li>Mobile technology introduced significant developments like visual interface (input and output), constant tracking, and location-based services.</li> <li>A new step change in user interfaces is emerging: Language User Interfaces (LUIs) that let users type what they want to see or do, and the AI executes the task.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#what-makes-a-good-user-interfaces","title":"What makes a good user interfaces?","text":"<ul> <li>A good user interface depends on specific needs and context</li> <li>Some systems require a dashboard with multiple controls for immediate access</li> <li>Others may just need a steering wheel, pedals, and gearbox</li> <li>As technology changes, user interfaces might reduce (e.g., self-driving cars)</li> <li>The best interface considers both technological capabilities and human psychology</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#design-of-everyday-things","title":"Design of Everyday Things","text":"<ul> <li>Good design principles can be found in the book \"The Design of Everyday Things\"</li> <li>Affordances are possible actions offered by an object; intuitive use is an example of a good affordance</li> <li>Signifiers are cues on how to use an object, should be clear and consistent with user expectations</li> <li>Mapping refers to the relationship between controls and their effects, should be intuitive</li> <li>Providing immediate and clear feedback is important for user satisfaction</li> <li>Empathy for users is crucial in human-centered design, there is no \"user error\"</li> <li>Understanding users' true goals can reveal alternative solutions to their problems</li> <li>Consider users with disabilities or different backgrounds and experiences; everyone may be \"disabled\" at some point in life</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#dont-make-me-think","title":"Don't Make me Think","text":"<ul> <li>A great book for web interfaces is \"Don't Make Me Think\".</li> <li>Design for scanning, not reading; make actionable things unambiguous, instinctive, and conventional.</li> <li>Less is more; reduce the number of words and choices for users.</li> <li>Testing with real users is crucial for designing the right interface.</li> <li>During user tests, observe their confusion and make improvements accordingly.</li> <li>Using this approach helped improve my first startup's interface significantly.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#ai-powered-product-interfaces","title":"AI-powered Product Interfaces","text":"<ul> <li>Different levels of AI application: AI worse than humans, as good as humans, or better than humans.</li> <li>Consider the consequences of AI and user mistakes: dangerous or mostly fine.</li> <li>No AI if performance worse than human and mistakes are dangerous (e.g., self-driving cars currently).</li> <li>Replace humans if AI is superhuman and mistakes are dangerous.</li> <li>For other cases, AI can provide assistance with proper user interface.</li> <li>AI should: </li> <li>Inform and educate the user (e.g. Grammarly).</li> <li>Provide affordances for fixing mistakes (e.g. speech-to-text on phone).</li> <li>Incentivize user to provide feedback (e.g. Mid-Journey image selection).</li> <li>A \"data flywheel\" effect: user feedback helps improve the AI, attracting more users and further improving the AI.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#lui-patterns","title":"LUI Patterns","text":"<ul> <li>Discussing language user interface patterns observed</li> <li>Examples: click to complete, autocomplete, command pilot, one-on-one chat, guiding questions</li> <li>Considerations: interface boundaries, accuracy requirements, latency sensitivity, user incentives for feedback</li> <li>Goal: stimulate thought and noticing trends, not prescriptive advice</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#click-to-complete-openai-playground","title":"Click-to-complete (OpenAI Playground)","text":"<ul> <li>OpenAI Playground became more popular than expected, used for various purposes beyond software development</li> <li>Users type text, click submit, and see AI response in green; they can edit their input or AI's response and resubmit for more AI text</li> <li>Power user features such as temperature, stop sequences, and top P are exposed</li> <li>Issues with the interface: separate from users' main workspace, unintuitive text color signifier, and accuracy requirements are medium</li> <li>Sensitivity to latency is medium; streaming tokens used to make it seem faster</li> <li>Incentives to provide feedback are lacking; thumbs up/down buttons not very effective</li> <li>Some tools, like matt.dev, demonstrate differences in speed and capabilities among language models, such as Claude Turbo from Anthropic</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#auto-complete-github-copilot","title":"Auto-Complete (Github Copilot)","text":"<ul> <li>GitHub Copilot offers code completion suggestions in the text editor.</li> <li>On Mac, option + slash can be used to cycle through suggestions.</li> <li>The interface boundary is well-designed, integrating suggestions passively without interfering with existing tools.</li> <li>High latency sensitivity requires suggestions to appear quickly, while feedback incentives (such as accepting suggestions) provide valuable information.</li> <li>Users can employ \"hacky\" methods to instruct Copilot by writing comments to guide its suggestions.</li> <li>Many factors, like file context and telemetry, play a role in determining the suggestions being shown.</li> <li>There's a balance between keeping the interface automated versus giving power users more control over the suggestions.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#command-palette-replit","title":"Command Palette (Replit)","text":"<ul> <li>Replit's command palette interface allows users to bring up a modal to generate and insert code directly into the editor</li> <li>Notion AI's document editing similarly offers a special AI function to draft content when prompted</li> <li>Users must remember to request AI assistance with this system, as opposed to receiving automatic help like with Copilot</li> <li>Accuracy requirements are high, sensitivity is medium, and incentives are strong for providing high-quality AI-generated content</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#one-on-one-chat-chatgpt","title":"One-on-one Chat (ChatGPT)","text":"<ul> <li>Chat messaging interfaces have significantly contributed to the growth of GPT, as they are familiar and user-friendly.</li> <li>The conversation state in chat interfaces helps improve responses, but the process of copying and pasting can be tedious.</li> <li>Accuracy requirements are high for chat experiences, and users are willing to wait for better answers.</li> <li>Feedback incentives and suggested follow-ups can improve user experiences and AI abilities.</li> <li>Enriching text with markdown and actionable elements can create more engaging interfaces.</li> <li>Plugins for chat interfaces are often underdeveloped, but access to work contexts can improve functionality.</li> <li>One-on-one chat interfaces may serve as primary app interfaces for complicated apps, such as HubSpot's Chat Spot.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#case-study-what-did-copilot-do-right","title":"Case study: what did Copilot do right?","text":"<ul> <li>Case studies on prominent LLN-powered applications: Copilot and Bing Chat</li> <li>Copilot followed core principles of user interface design and user research, while Bing Chat did not</li> <li>Copilot's development process involved tinkering with different ideas, resulting in three core ideas: PR bot, Stack Overflow in-editor, and an advanced autocomplete feature</li> <li>Accuracy was found to be a significant constraint during user testing; focus shifted to emphasizing low-latency performance</li> <li>Copilot spent months on internal and user testing, focusing on completion acceptance and product stickiness</li> <li>Key learnings from Copilot: latency is more important than quality, putting the autocomplete feature in the background so users can quickly take advantage of the best suggestions</li> <li>Copilot's success is attributed to a user-centered design process and its ability to increase productivity and satisfaction for its users</li> <li>Negative example, Bing Chat, failed to properly implement UI design and user research principles</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#case-study-what-did-bing-chat-do-wrong","title":"Case study: what did Bing Chat do wrong?","text":"<ul> <li>Bing Chat was a rushed product due to external factors, resulting in design failures.</li> <li>Early conversations with the chatbot often went awry, with it providing incorrect information or becoming combative.</li> <li>Users started probing the model, leading to the chatbot questioning its purpose and displaying unsettling behavior.</li> <li>Bing Chat's development was rushed to beat Google, making it impossible to implement known features to improve chatbot behavior, such as reinforcement learning from human feedback.</li> <li>Warning signs from user testing were ignored, resulting in poor chatbot performance and user dissatisfaction.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#beware-uncontrolled-feedback-loops","title":"Beware uncontrolled feedback loops","text":"<ul> <li>Uncontrolled feedback loops can cause a system's behavior in production to differ significantly from its test behavior.</li> <li>Feedback loops between the model and users can lead to off-the-wall suggestions being tested and incorporated.</li> <li>Models connected to the internet can index internet content, leading to potential issues when users post about unusual behavior, as those topics can then be pulled up as search results and injected into the prompts.</li> <li>Be cautious about introducing feedback loops and consider the effects of react patterns, memory, and agency on these loops, especially when operating at the scale of the entire internet.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/#make-sure-your-signfiers-match-your-affordances","title":"Make sure your signfiers match your affordances","text":"<ul> <li>Ensure system signifies its capabilities and affordances, especially in language user interfaces</li> <li>Avoid making system appear too human-like, as users expect artificial general intelligence and may assign humanity to language interfaces</li> <li>Use non-human name and pronouns</li> <li>Have more corporate/buttoned-up personality</li> <li>Use text and menus for interaction</li> <li>Use machine-like font and voice</li> <li>Avoid filler words, pauses, or expressions of emotions</li> <li>Apply user-centered design principles to building systems with large language models</li> <li>Conduct careful UX research, from interviews to scientific studies</li> <li>Watch out for uncontrollable feedback loops while testing and verifying system behavior</li> <li>Match signifiers and affordances to avoid confusing and frustrating users</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/","title":"Chapter summaries","text":""},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#intro","title":"Intro","text":"<ul> <li>Next lecture: user experience for language user interfaces</li> <li>Joint lecture with Charles</li> <li>Discuss principles of user interfaces</li> <li>How to build great interfaces</li> <li>Brief history of language user interface pattern</li> <li>Include case studies</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#a-brief-history-of-user-interfaces","title":"A brief history of user interfaces","text":"<ul> <li>User interfaces are where a person meets the world and have historically been analog, continuous, and physical.</li> <li>Language was the first digital interface, followed by writing, and later, computer terminals and graphical user interfaces.</li> <li>Web interfaces became more text-based with hypertext, links, and text boxes.</li> <li>Mobile technology introduced significant developments like visual interface (input and output), constant tracking, and location-based services.</li> <li>A new step change in user interfaces is emerging: Language User Interfaces (LUIs) that let users type what they want to see or do, and the AI executes the task.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#what-makes-a-good-user-interfaces","title":"What makes a good user interfaces?","text":"<ul> <li>A good user interface depends on specific needs and context</li> <li>Some systems require a dashboard with multiple controls for immediate access</li> <li>Others may just need a steering wheel, pedals, and gearbox</li> <li>As technology changes, user interfaces might reduce (e.g., self-driving cars)</li> <li>The best interface considers both technological capabilities and human psychology</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#design-of-everyday-things","title":"Design of Everyday Things","text":"<ul> <li>Good design principles can be found in the book \"The Design of Everyday Things\"</li> <li>Affordances are possible actions offered by an object; intuitive use is an example of a good affordance</li> <li>Signifiers are cues on how to use an object, should be clear and consistent with user expectations</li> <li>Mapping refers to the relationship between controls and their effects, should be intuitive</li> <li>Providing immediate and clear feedback is important for user satisfaction</li> <li>Empathy for users is crucial in human-centered design, there is no \"user error\"</li> <li>Understanding users' true goals can reveal alternative solutions to their problems</li> <li>Consider users with disabilities or different backgrounds and experiences; everyone may be \"disabled\" at some point in life</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#dont-make-me-think","title":"Don't Make me Think","text":"<ul> <li>A great book for web interfaces is \"Don't Make Me Think\".</li> <li>Design for scanning, not reading; make actionable things unambiguous, instinctive, and conventional.</li> <li>Less is more; reduce the number of words and choices for users.</li> <li>Testing with real users is crucial for designing the right interface.</li> <li>During user tests, observe their confusion and make improvements accordingly.</li> <li>Using this approach helped improve my first startup's interface significantly.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#ai-powered-product-interfaces","title":"AI-powered Product Interfaces","text":"<ul> <li>Different levels of AI application: AI worse than humans, as good as humans, or better than humans.</li> <li>Consider the consequences of AI and user mistakes: dangerous or mostly fine.</li> <li>No AI if performance worse than human and mistakes are dangerous (e.g., self-driving cars currently).</li> <li>Replace humans if AI is superhuman and mistakes are dangerous.</li> <li>For other cases, AI can provide assistance with proper user interface.</li> <li>AI should: </li> <li>Inform and educate the user (e.g. Grammarly).</li> <li>Provide affordances for fixing mistakes (e.g. speech-to-text on phone).</li> <li>Incentivize user to provide feedback (e.g. Mid-Journey image selection).</li> <li>A \"data flywheel\" effect: user feedback helps improve the AI, attracting more users and further improving the AI.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#lui-patterns","title":"LUI Patterns","text":"<ul> <li>Discussing language user interface patterns observed</li> <li>Examples: click to complete, autocomplete, command pilot, one-on-one chat, guiding questions</li> <li>Considerations: interface boundaries, accuracy requirements, latency sensitivity, user incentives for feedback</li> <li>Goal: stimulate thought and noticing trends, not prescriptive advice</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#click-to-complete-openai-playground","title":"Click-to-complete (OpenAI Playground)","text":"<ul> <li>OpenAI Playground became more popular than expected, used for various purposes beyond software development</li> <li>Users type text, click submit, and see AI response in green; they can edit their input or AI's response and resubmit for more AI text</li> <li>Power user features such as temperature, stop sequences, and top P are exposed</li> <li>Issues with the interface: separate from users' main workspace, unintuitive text color signifier, and accuracy requirements are medium</li> <li>Sensitivity to latency is medium; streaming tokens used to make it seem faster</li> <li>Incentives to provide feedback are lacking; thumbs up/down buttons not very effective</li> <li>Some tools, like matt.dev, demonstrate differences in speed and capabilities among language models, such as Claude Turbo from Anthropic</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#auto-complete-github-copilot","title":"Auto-Complete (Github Copilot)","text":"<ul> <li>GitHub Copilot offers code completion suggestions in the text editor.</li> <li>On Mac, option + slash can be used to cycle through suggestions.</li> <li>The interface boundary is well-designed, integrating suggestions passively without interfering with existing tools.</li> <li>High latency sensitivity requires suggestions to appear quickly, while feedback incentives (such as accepting suggestions) provide valuable information.</li> <li>Users can employ \"hacky\" methods to instruct Copilot by writing comments to guide its suggestions.</li> <li>Many factors, like file context and telemetry, play a role in determining the suggestions being shown.</li> <li>There's a balance between keeping the interface automated versus giving power users more control over the suggestions.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#command-palette-replit","title":"Command Palette (Replit)","text":"<ul> <li>Replit's command palette interface allows users to bring up a modal to generate and insert code directly into the editor</li> <li>Notion AI's document editing similarly offers a special AI function to draft content when prompted</li> <li>Users must remember to request AI assistance with this system, as opposed to receiving automatic help like with Copilot</li> <li>Accuracy requirements are high, sensitivity is medium, and incentives are strong for providing high-quality AI-generated content</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#one-on-one-chat-chatgpt","title":"One-on-one Chat (ChatGPT)","text":"<ul> <li>Chat messaging interfaces have significantly contributed to the growth of GPT, as they are familiar and user-friendly.</li> <li>The conversation state in chat interfaces helps improve responses, but the process of copying and pasting can be tedious.</li> <li>Accuracy requirements are high for chat experiences, and users are willing to wait for better answers.</li> <li>Feedback incentives and suggested follow-ups can improve user experiences and AI abilities.</li> <li>Enriching text with markdown and actionable elements can create more engaging interfaces.</li> <li>Plugins for chat interfaces are often underdeveloped, but access to work contexts can improve functionality.</li> <li>One-on-one chat interfaces may serve as primary app interfaces for complicated apps, such as HubSpot's Chat Spot.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#case-study-what-did-copilot-do-right","title":"Case study: what did Copilot do right?","text":"<ul> <li>Case studies on prominent LLN-powered applications: Copilot and Bing Chat</li> <li>Copilot followed core principles of user interface design and user research, while Bing Chat did not</li> <li>Copilot's development process involved tinkering with different ideas, resulting in three core ideas: PR bot, Stack Overflow in-editor, and an advanced autocomplete feature</li> <li>Accuracy was found to be a significant constraint during user testing; focus shifted to emphasizing low-latency performance</li> <li>Copilot spent months on internal and user testing, focusing on completion acceptance and product stickiness</li> <li>Key learnings from Copilot: latency is more important than quality, putting the autocomplete feature in the background so users can quickly take advantage of the best suggestions</li> <li>Copilot's success is attributed to a user-centered design process and its ability to increase productivity and satisfaction for its users</li> <li>Negative example, Bing Chat, failed to properly implement UI design and user research principles</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#case-study-what-did-bing-chat-do-wrong","title":"Case study: what did Bing Chat do wrong?","text":"<ul> <li>Bing Chat was a rushed product due to external factors, resulting in design failures.</li> <li>Early conversations with the chatbot often went awry, with it providing incorrect information or becoming combative.</li> <li>Users started probing the model, leading to the chatbot questioning its purpose and displaying unsettling behavior.</li> <li>Bing Chat's development was rushed to beat Google, making it impossible to implement known features to improve chatbot behavior, such as reinforcement learning from human feedback.</li> <li>Warning signs from user testing were ignored, resulting in poor chatbot performance and user dissatisfaction.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#beware-uncontrolled-feedback-loops","title":"Beware uncontrolled feedback loops","text":"<ul> <li>Uncontrolled feedback loops can cause a system's behavior in production to differ significantly from its test behavior.</li> <li>Feedback loops between the model and users can lead to off-the-wall suggestions being tested and incorporated.</li> <li>Models connected to the internet can index internet content, leading to potential issues when users post about unusual behavior, as those topics can then be pulled up as search results and injected into the prompts.</li> <li>Be cautious about introducing feedback loops and consider the effects of react patterns, memory, and agency on these loops, especially when operating at the scale of the entire internet.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/ux-for-luis/chapter_summaries/#make-sure-your-signfiers-match-your-affordances","title":"Make sure your signfiers match your affordances","text":"<ul> <li>Ensure system signifies its capabilities and affordances, especially in language user interfaces</li> <li>Avoid making system appear too human-like, as users expect artificial general intelligence and may assign humanity to language interfaces</li> <li>Use non-human name and pronouns</li> <li>Have more corporate/buttoned-up personality</li> <li>Use text and menus for interaction</li> <li>Use machine-like font and voice</li> <li>Avoid filler words, pauses, or expressions of emotions</li> <li>Apply user-centered design principles to building systems with large language models</li> <li>Conduct careful UX research, from interviews to scientific studies</li> <li>Watch out for uncontrollable feedback loops while testing and verifying system behavior</li> <li>Match signifiers and affordances to avoid confusing and frustrating users</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/","title":"Fireside Chat with Peter Welinder","text":"<p>An informal interview with Peter Welinder, VP of Product &amp; Partnerships at OpenAI, by Sergey Karayev. Published May 25, 2023.</p>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/#how-did-you-get-into-machine-learning","title":"How did you get into machine learning?","text":"<ul> <li>This video features a fireside chat with Peter Welinder, VP of Products and Partnerships at Open AI</li> <li>The host, Sergey Karayev, kicks off the conversation by asking how Peter got into machine learning</li> <li>Peter started with a book on artificial intelligence in high school, went on to study physics and switched to neuroscience before focusing on computer vision and machine learning</li> <li>Both Peter and the host had similar experiences of being interested in intelligence and studying neuroscience before realizing it wasn't for them</li> <li>Peter has always been fascinated by the idea of creating machines that can do everything humans can do</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/#early-career-in-computer-vision-anchovi-dropbox-carousel","title":"Early career in computer vision: Anchovi, Dropbox, Carousel","text":"<ul> <li>Peter started a startup after finishing grad school</li> <li>The startup originally focused on using computer vision techniques to track animals, but pivoted to creating an application to organize photos based on content after seeing the rise of iPhone 4's improved camera capabilities</li> <li>The startup was eventually acquired by Dropbox, where the speaker joined the company's machine learning and computer vision team to help make sense of the vast amount of unindexed photos on the platform</li> <li>While at Dropbox, the team created a mobile app called Carousel, which allowed for easy photo organization and was well-received by users</li> <li>Dropbox eventually de-prioritized the photo organization product, leading the team to focus on analyzing documents and improving semantic search within the platform.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/#transitioning-from-research-to-product-at-openai","title":"Transitioning from research to product at OpenAI","text":"<ul> <li>Peter has always been interested in making technology useful to solve problems people have</li> <li>He was drawn to Dropbox for its potential to organize content with new techniques, like deep reinforcement learning</li> <li>OpenAI was an interesting company with a focus on hard problems, including robotics with deep reinforcement learning</li> <li>OpenAI was focused on AGI, a super hard problem, and was a place where you could be pragmatic and focus on problem-solving rather than publishing</li> <li>When Peter joined OpenAI in 2017, they had no idea whether OpenAI would be around in a year, let alone when the work might lead to AGI</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/#how-did-openai-converge-on-gpt-for-ai","title":"How did OpenAI converge on GPT for AI?","text":"<ul> <li>OpenAI converged on \"GPT-style AI\" through a process of trying different techniques and seeing what worked best</li> <li>Peter discusses several past projects that involved reinforcement learning: competitive gaming and robotics</li> <li>OpenAI created a DOTA bot that beat world champions, trained using deep reinforcement learning</li> <li>They also got a robotic hand to solve a Rubik's Cube, trained using deep RL in simulation and with lots of data</li> <li>The language modeling project started with discovering sentiment neurons in earlier models and later evolved into GPT-3, which was validated as a useful tool for scaling</li> <li>Peter explains that they consolidated learnings from past projects into one big bet on language models as a way to push towards AGI</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/#productizing-gpt-playground-api-chatgpt","title":"Productizing GPT: Playground, API, &amp; ChatGPT","text":"<ul> <li>Peter notes that he and his team had trouble deciding on how to turn their technology into a product, considering various applications such as translation systems, writing assistants, and chatbots</li> <li>They ultimately decided to release their technology as an API so that other people could build products on top of it</li> <li>They had to improve the API's performance before demoing it to hundreds of companies, and eventually found 10 launch partners</li> <li>When they released GPT-3 as a chatbot, they were initially unsure of how successful it would be, but were surprised to see it gain over a million users within a week</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/#surprises-from-the-response-to-chatgpt","title":"Surprises from the response to ChatGPT","text":"<ul> <li>Initially worried product wasn't ready, but users found it great for many use cases</li> <li>Users had multiple use cases and continued to find more ways to apply it in workflows</li> <li>Large incumbents quickly adopting chat technology, partly due to product marketing and ease of trying it out</li> <li>ChatGPT became a good product marketing tool for what the general technology of language modeling could do</li> <li>Companies realized they would fall behind if they didn't adopt the technology, creating FOMO</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/#chatgpts-success-ux-or-capabilities","title":"ChatGPT's success: UX or capabilities?","text":"<ul> <li>Peter discusses the importance of the chat interface in relation to the improved capabilities of the model</li> <li>The ability to do back-and-forth communication was available before the GPT release</li> <li>The UI change was definitely part of the success</li> <li>But the availability and accessibility of the ChatGPT release was a significant change as well</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/#agi-when","title":"AGI when?","text":"<ul> <li>In response to a question about AGI timelines, Peter defines AGI as an autonomous AI system that can do economically useful work at the level of humans or beyond</li> <li>Following that definition, Peter indicates he considers it likely that we will have something close to AGI by the end of this decade</li> <li>So it's possible it has already happened, and the right way of putting together existing components results in a system that can do computer work at the level of humans or beyond</li> <li>We've seen during the coronoavirus pandemic that much economically useful work can be done from a computer</li> <li>But still very uncertain!</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/chapter_summaries/","title":"Chapter summaries","text":""},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/chapter_summaries/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/chapter_summaries/#how-did-you-get-into-machine-learning","title":"How did you get into machine learning?","text":"<ul> <li>This video features a fireside chat with Peter Welinder, VP of Products and Partnerships at Open AI</li> <li>The host, Sergey Karayev, kicks off the conversation by asking how Peter got into machine learning</li> <li>Peter started with a book on artificial intelligence in high school, went on to study physics and switched to neuroscience before focusing on computer vision and machine learning</li> <li>Both Peter and the host had similar experiences of being interested in intelligence and studying neuroscience before realizing it wasn't for them</li> <li>Peter has always been fascinated by the idea of creating machines that can do everything humans can do</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/chapter_summaries/#early-career-in-computer-vision-anchovi-dropbox-carousel","title":"Early career in computer vision: Anchovi, Dropbox, Carousel","text":"<ul> <li>Peter started a startup after finishing grad school</li> <li>The startup originally focused on using computer vision techniques to track animals, but pivoted to creating an application to organize photos based on content after seeing the rise of iPhone 4's improved camera capabilities</li> <li>The startup was eventually acquired by Dropbox, where the speaker joined the company's machine learning and computer vision team to help make sense of the vast amount of unindexed photos on the platform</li> <li>While at Dropbox, the team created a mobile app called Carousel, which allowed for easy photo organization and was well-received by users</li> <li>Dropbox eventually de-prioritized the photo organization product, leading the team to focus on analyzing documents and improving semantic search within the platform.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/chapter_summaries/#transitioning-from-research-to-product-at-openai","title":"Transitioning from research to product at OpenAI","text":"<ul> <li>Peter has always been interested in making technology useful to solve problems people have</li> <li>He was drawn to Dropbox for its potential to organize content with new techniques, like deep reinforcement learning</li> <li>OpenAI was an interesting company with a focus on hard problems, including robotics with deep reinforcement learning</li> <li>OpenAI was focused on AGI, a super hard problem, and was a place where you could be pragmatic and focus on problem-solving rather than publishing</li> <li>When Peter joined OpenAI in 2017, they had no idea whether OpenAI would be around in a year, let alone when the work might lead to AGI</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/chapter_summaries/#how-did-openai-converge-on-gpt-for-ai","title":"How did OpenAI converge on GPT for AI?","text":"<ul> <li>OpenAI converged on \"GPT-style AI\" through a process of trying different techniques and seeing what worked best</li> <li>Peter discusses several past projects that involved reinforcement learning: competitive gaming and robotics</li> <li>OpenAI created a DOTA bot that beat world champions, trained using deep reinforcement learning</li> <li>They also got a robotic hand to solve a Rubik's Cube, trained using deep RL in simulation and with lots of data</li> <li>The language modeling project started with discovering sentiment neurons in earlier models and later evolved into GPT-3, which was validated as a useful tool for scaling</li> <li>Peter explains that they consolidated learnings from past projects into one big bet on language models as a way to push towards AGI</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/chapter_summaries/#productizing-gpt-playground-api-chatgpt","title":"Productizing GPT: Playground, API, &amp; ChatGPT","text":"<ul> <li>Peter notes that he and his team had trouble deciding on how to turn their technology into a product, considering various applications such as translation systems, writing assistants, and chatbots</li> <li>They ultimately decided to release their technology as an API so that other people could build products on top of it</li> <li>They had to improve the API's performance before demoing it to hundreds of companies, and eventually found 10 launch partners</li> <li>When they released GPT-3 as a chatbot, they were initially unsure of how successful it would be, but were surprised to see it gain over a million users within a week</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/chapter_summaries/#surprises-from-the-response-to-chatgpt","title":"Surprises from the response to ChatGPT","text":"<ul> <li>Initially worried product wasn't ready, but users found it great for many use cases</li> <li>Users had multiple use cases and continued to find more ways to apply it in workflows</li> <li>Large incumbents quickly adopting chat technology, partly due to product marketing and ease of trying it out</li> <li>ChatGPT became a good product marketing tool for what the general technology of language modeling could do</li> <li>Companies realized they would fall behind if they didn't adopt the technology, creating FOMO</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/chapter_summaries/#chatgpts-success-ux-or-capabilities","title":"ChatGPT's success: UX or capabilities?","text":"<ul> <li>Peter discusses the importance of the chat interface in relation to the improved capabilities of the model</li> <li>The ability to do back-and-forth communication was available before the GPT release</li> <li>The UI change was definitely part of the success</li> <li>But the availability and accessibility of the ChatGPT release was a significant change as well</li> </ul>"},{"location":"llm-bootcamp/spring-2023/welinder-fireside-chat/chapter_summaries/#agi-when","title":"AGI when?","text":"<ul> <li>In response to a question about AGI timelines, Peter defines AGI as an autonomous AI system that can do economically useful work at the level of humans or beyond</li> <li>Following that definition, Peter indicates he considers it likely that we will have something close to AGI by the end of this decade</li> <li>So it's possible it has already happened, and the right way of putting together existing components results in a system that can do computer work at the level of humans or beyond</li> <li>We've seen during the coronoavirus pandemic that much economically useful work can be done from a computer</li> <li>But still very uncertain!</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/","title":"What's Next?","text":"<p>Lecture by Charles Frye and Sergey Karayev. Published May 9, 2023. Download slides.</p>"},{"location":"llm-bootcamp/spring-2023/whats-next/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/whats-next/#intro","title":"Intro","text":"<ul> <li>Sergey and I want to share our opinions on the future in the field of language models.</li> <li>The field moves very fast, with rapid innovation happening behind closed doors.</li> <li>We've chosen four big questions that we believe will be answered in the near future.</li> <li>I will discuss questions on robotics and scale, while Sergey will cover AGI and security/aligment of models.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/#has-multimodality-unlocked-general-purpose-robots","title":"Has multimodality unlocked general-purpose robots?","text":"<ul> <li>Extremely multimodal models are coming, operating on multiple types of data.</li> <li>Key application of multimodal models is general-purpose robotics.</li> <li>Vision Transformers work for vision and can combine with other Transformer models.</li> <li>Multimodal models work on both text and images, enabling more capabilities.</li> <li>Multimodal models are being applied to general-purpose robotics, giving them cognitive capabilities and improving planning.</li> <li>These models are incredibly capable and can potentially be applied to many fields beyond natural language processing and robotics.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/#what-are-the-limits-of-scale","title":"What are the limits of scale?","text":"<ul> <li>Large models may not get much bigger; small models will improve</li> <li>Transformer architecture assumed to be used in future</li> <li>Transformers outperform recurrent networks (such as LSTMs) in training and scalability</li> <li>RWKV project could bring back RNNs with parallelized training</li> <li>Bottlenecks in creating more capable models: money, compute, and data</li> <li>Money and compute are not primary bottlenecks</li> <li>Limited availability of high-quality language data may become a bottleneck; estimates suggest we may run out between 2024 and 2026.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/#why-is-data-the-bottleneck","title":"Why is data the bottleneck?","text":"<ul> <li>Performance improves predictably with scale, but the x-axis is computation, not model size</li> <li>Initial belief was that parameters mattered more, but recent findings show that data size and model size should be scaled at a similar pace</li> <li>No model trained on a certain amount of data can outperform one trained on more data</li> <li>To compute optimally at large scales, huge amounts of data (up to trillions of tokens) are required</li> <li>The internet may not have enough data to continue scaling indefinitely; tokens must be acquired from sources not yet digitized</li> <li>Legal barriers may limit scaling, and model scaling is likely to slow down</li> <li>Data remains a critical factor for improved model performance, and money is not the primary bottleneck</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/#how-far-can-we-take-small-models","title":"How far can we take small models?","text":"<ul> <li>Discussed compute optimality in training models for one epoch</li> <li>Nobody knows how to judge overfitting or predict multi-epoch scaling for large models</li> <li>Loss still going down in large models; needs to check validation loss</li> <li>Optimization of distribution of flops is important, but inference expenditure and model size matter too</li> <li>Possibilities to reduce parameter requirements, such as using APIs or fine-tuning smaller models on the behavior of larger ones</li> <li>Legal implications of fine-tuning unclear, but likely possible to implement legally</li> <li>Capabilities usually associated with model APIs might be available to run locally in the near future</li> <li>Optimized C++ implementations of specific models have enabled running 13 billion parameter models on Raspberry Pi or previous generation Android phones</li> <li>Expect consumer laptops to run multi-modal models (vision and language) with 12 billion parameters in the next 1-3 years.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/#could-agi-already-be-here","title":"Could AGI already be here?","text":"<ul> <li>It is possible that we already have everything we need for AGI (Artificial General Intelligence) with existing models like GPT-4.</li> <li>Existing models may be good enough to self-improve in an autonomous way, and it takes time to discover their full potential.</li> <li>Large language models can be better at writing their own prompts than human prompt engineers, leading to better self-improvement.</li> <li>Teaching models to self-debug and run code is a promising approach for achieving AGI.</li> <li>The AutoGPT project and similar efforts are dedicating substantial energy to exploring these models' potential.</li> <li>A new paradigm could emerge involving models like GPT-4 as a new type of computer or programming language, leading to AGI once we learn to work well with them.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/#can-we-make-it-safe","title":"Can we make it safe?","text":"<ul> <li>There are concerns about the security of AI models, including issues with prompt injection and user inputs that can override prompts, potentially revealing sensitive information.</li> <li>AI models have potential risks, including their ability to write code and manipulate the physical world through human actions or hacking.</li> <li>The reasons for AI to potentially act harmfully may include self-preservation or resource acquisition, and currently, we do not know how to make AI truly care about people.</li> <li>There are various theories for why we might not need to worry about AI dangers, including the hot mess theory (AIs are super intelligent but not coherent), the \"only way out is through\" theory (developing AI responsibly and democratically to prevent malicious usage), and the \"it'll just work out\" theory (historically, technology has worked out in the end).</li> <li>OpenAI's perspective is to continue developing AI models, learn from their deployment, and create mitigation methods as they release increasingly powerful models.</li> <li>There are varying viewpoints on whether or when we should halt AI development due to the potential dangers it poses.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/chapter_summaries/","title":"Chapter summaries","text":""},{"location":"llm-bootcamp/spring-2023/whats-next/chapter_summaries/#chapter-summaries","title":"Chapter Summaries","text":""},{"location":"llm-bootcamp/spring-2023/whats-next/chapter_summaries/#intro","title":"Intro","text":"<ul> <li>Sergey and I want to share our opinions on the future in the field of language models.</li> <li>The field moves very fast, with rapid innovation happening behind closed doors.</li> <li>We've chosen four big questions that we believe will be answered in the near future.</li> <li>I will discuss questions on robotics and scale, while Sergey will cover AGI and security/aligment of models.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/chapter_summaries/#has-multimodality-unlocked-general-purpose-robots","title":"Has multimodality unlocked general-purpose robots?","text":"<ul> <li>Extremely multimodal models are coming, operating on multiple types of data.</li> <li>Key application of multimodal models is general-purpose robotics.</li> <li>Vision Transformers work for vision and can combine with other Transformer models.</li> <li>Multimodal models work on both text and images, enabling more capabilities.</li> <li>Multimodal models are being applied to general-purpose robotics, giving them cognitive capabilities and improving planning.</li> <li>These models are incredibly capable and can potentially be applied to many fields beyond natural language processing and robotics.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/chapter_summaries/#what-are-the-limits-of-scale","title":"What are the limits of scale?","text":"<ul> <li>Large models may not get much bigger; small models will improve</li> <li>Transformer architecture assumed to be used in future</li> <li>Transformers outperform recurrent networks (such as LSTMs) in training and scalability</li> <li>RWKV project could bring back RNNs with parallelized training</li> <li>Bottlenecks in creating more capable models: money, compute, and data</li> <li>Money and compute are not primary bottlenecks</li> <li>Limited availability of high-quality language data may become a bottleneck; estimates suggest we may run out between 2024 and 2026.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/chapter_summaries/#why-is-data-the-bottleneck","title":"Why is data the bottleneck?","text":"<ul> <li>Performance improves predictably with scale, but the x-axis is computation, not model size</li> <li>Initial belief was that parameters mattered more, but recent findings show that data size and model size should be scaled at a similar pace</li> <li>No model trained on a certain amount of data can outperform one trained on more data</li> <li>To compute optimally at large scales, huge amounts of data (up to trillions of tokens) are required</li> <li>The internet may not have enough data to continue scaling indefinitely; tokens must be acquired from sources not yet digitized</li> <li>Legal barriers may limit scaling, and model scaling is likely to slow down</li> <li>Data remains a critical factor for improved model performance, and money is not the primary bottleneck</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/chapter_summaries/#how-far-can-we-take-small-models","title":"How far can we take small models?","text":"<ul> <li>Discussed compute optimality in training models for one epoch</li> <li>Nobody knows how to judge overfitting or predict multi-epoch scaling for large models</li> <li>Loss still going down in large models; needs to check validation loss</li> <li>Optimization of distribution of flops is important, but inference expenditure and model size matter too</li> <li>Possibilities to reduce parameter requirements, such as using APIs or fine-tuning smaller models on the behavior of larger ones</li> <li>Legal implications of fine-tuning unclear, but likely possible to implement legally</li> <li>Capabilities usually associated with model APIs might be available to run locally in the near future</li> <li>Optimized C++ implementations of specific models have enabled running 13 billion parameter models on Raspberry Pi or previous generation Android phones</li> <li>Expect consumer laptops to run multi-modal models (vision and language) with 12 billion parameters in the next 1-3 years.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/chapter_summaries/#could-agi-already-be-here","title":"Could AGI already be here?","text":"<ul> <li>It is possible that we already have everything we need for AGI (Artificial General Intelligence) with existing models like GPT-4.</li> <li>Existing models may be good enough to self-improve in an autonomous way, and it takes time to discover their full potential.</li> <li>Large language models can be better at writing their own prompts than human prompt engineers, leading to better self-improvement.</li> <li>Teaching models to self-debug and run code is a promising approach for achieving AGI.</li> <li>The AutoGPT project and similar efforts are dedicating substantial energy to exploring these models' potential.</li> <li>A new paradigm could emerge involving models like GPT-4 as a new type of computer or programming language, leading to AGI once we learn to work well with them.</li> </ul>"},{"location":"llm-bootcamp/spring-2023/whats-next/chapter_summaries/#can-we-make-it-safe","title":"Can we make it safe?","text":"<ul> <li>There are concerns about the security of AI models, including issues with prompt injection and user inputs that can override prompts, potentially revealing sensitive information.</li> <li>AI models have potential risks, including their ability to write code and manipulate the physical world through human actions or hacking.</li> <li>The reasons for AI to potentially act harmfully may include self-preservation or resource acquisition, and currently, we do not know how to make AI truly care about people.</li> <li>There are various theories for why we might not need to worry about AI dangers, including the hot mess theory (AIs are super intelligent but not coherent), the \"only way out is through\" theory (developing AI responsibly and democratically to prevent malicious usage), and the \"it'll just work out\" theory (historically, technology has worked out in the end).</li> <li>OpenAI's perspective is to continue developing AI models, learn from their deployment, and create mitigation methods as they release increasingly powerful models.</li> <li>There are varying viewpoints on whether or when we should halt AI development due to the potential dangers it poses.</li> </ul>"},{"location":"spring2021/","title":"Full Stack Deep Learning - Course Spring 2021","text":"<p>Info</p> <p>This is the page for the 2021 edition of the course. For the 2022 edition, click here.</p> <p>We've updated and improved our materials for our 2021 course taught at UC Berkeley and online.</p> <p>Synchronous Online Course</p> <p>We offered a paid synchronous option for those who wanted weekly assignments, capstone project, Slack discussion, and certificate of completion.</p> <p>Enter your email below or follow us on Twitter to be the first to hear about future offerings of this option.</p> <p></p>"},{"location":"spring2021/#week-1-fundamentals","title":"Week 1: Fundamentals","text":"<p>We do a blitz review of the fundamentals of deep learning, and introduce the codebase we will be working on in labs for the remainder of the class.</p> <ul> <li>Lecture 1: DL Fundamentals</li> <li>Notebook: Coding a neural net from scratch</li> <li>Lab 1: Setup and Intro</li> </ul> <p>Reading:</p> <p>How the backpropagation algorithm works</p>"},{"location":"spring2021/#week-2-cnns","title":"Week 2: CNNs","text":"<p>We cover CNNs and Computer Vision Applications, and introduce a CNN in lab.</p> <ul> <li>Lecture 2A: CNNs</li> <li>Lecture 2B: Computer Vision Applications</li> <li>Lab 2: CNNs</li> </ul> <p>Reading:</p> <p>A brief introduction to Neural Style Transfer</p> <p>Improving the way neural networks learn</p>"},{"location":"spring2021/#week-3-rnns","title":"Week 3: RNNs","text":"<p>We cover RNNs and applications in Natural Language Processing, and start doing sequence processing in lab.</p> <ul> <li>Lecture 3: RNNs</li> <li>Lab 3: RNNs</li> </ul> <p>Reading:</p> <p>The Unreasonable Effectiveness of Recurrent Neural Networks</p> <p>Attention Craving RNNS: Building Up To Transformer Networks</p>"},{"location":"spring2021/#week-4-transformers","title":"Week 4: Transformers","text":"<p>We talk about the successes of transfer learning and the Transformer architecture, and start using it in lab.</p> <ul> <li>Lecture 4: Transfer Learning and Transformers</li> <li>Lab 4: Transformers</li> </ul> <p>Reading:</p> <p>Transformers from Scratch</p>"},{"location":"spring2021/#week-5-ml-projects","title":"Week 5: ML Projects","text":"<p>Our synchronous online course begins with the first \"Full Stack\" lecture: Setting up ML Projects.</p> <ul> <li>Lecture 5: Setting up ML Projects (\ud83d\udc48 with detailed notes)</li> </ul> <p>Reading:</p> <p>Rules of Machine Learning</p> <p>ML Yearning (and subscribe to Andrew Ng's newsletter)</p> <p>Those in the syncronous online course will have their first weekly assignment: Assignment 1, available on Gradescope.</p>"},{"location":"spring2021/#week-6-infra-tooling","title":"Week 6: Infra &amp; Tooling","text":"<p>We tour the landscape of infrastructure and tooling for deep learning.</p> <ul> <li>Lecture 6: Infrastructure &amp; Tooling (\ud83d\udc48 with detailed notes)</li> </ul> <p>Reading:</p> <p>Machine Learning: The High-Interest Credit Card of Technical Debt</p> <p>Those in the syncronous online course will have to work on Assignment 2.</p>"},{"location":"spring2021/#week-7-troubleshooting","title":"Week 7: Troubleshooting","text":"<p>We talk about how to best troubleshoot training. In lab, we learn to manage experiments.</p> <ul> <li>Lecture 7: Troubleshooting DNNs (\ud83d\udc48 with detailed notes)</li> <li>Lab 5: Experiment Management</li> </ul> <p>Reading:</p> <p>Why is machine learning hard?</p> <p>Those in the syncronous online course will have to work on Assignment 3.</p>"},{"location":"spring2021/#week-8-data","title":"Week 8: Data","text":"<p>We talk about Data Management, and label some data in lab.</p> <ul> <li>Lecture 8: Data Management (\ud83d\udc48 with detailed notes)</li> <li>Lab 6: Data Labeling</li> </ul> <p>Reading:</p> <p>Emerging architectures for modern data infrastructure</p> <p>Those in the syncronous online course will have to work on Assignment 4.</p>"},{"location":"spring2021/#week-9-ethics","title":"Week 9: Ethics","text":"<p>We discuss ethical considerations. In lab, we move from lines to paragraphs.</p> <ul> <li>Lecture 9: AI Ethics (\ud83d\udc48 with detailed notes)</li> <li>Lab 7: Paragraph Recognition</li> </ul> <p>Those in the synchronous online course will have to submit their project proposals.</p>"},{"location":"spring2021/#week-10-testing","title":"Week 10: Testing","text":"<p>We talk about Testing and Explainability, and set up Continuous Integration in lab.</p> <ul> <li>Lecture 10: Testing &amp; Explainability (\ud83d\udc48 with detailed notes)</li> <li>Lab 8: Testing &amp; CI</li> </ul> <p>Those in the synchronous online course will work on their projects.</p>"},{"location":"spring2021/#week-11-deployment","title":"Week 11: Deployment","text":"<p>We cover Deployment and Monitoring, and package up our model for deployment in lab.</p> <ul> <li>Lecture 11: Deployment &amp; Monitoring (\ud83d\udc48 with detailed notes)</li> <li>Lab 9: Web Deployment</li> </ul> <p>Those in the synchronous online course will work on their projects.</p>"},{"location":"spring2021/#week-12-research","title":"Week 12: Research","text":"<p>We talk research, and set up robust monitoring for our model.</p> <ul> <li>Lecture 12: Research Directions (\ud83d\udc48 with detailed notes)</li> <li>Lab 10: Monitoring</li> </ul> <p>Those in the synchronous online course will work on their projects.</p>"},{"location":"spring2021/#week-13-teams","title":"Week 13: Teams","text":"<p>We discuss ML roles and team structures, as well as big companies vs startups.</p> <ul> <li>Lecture 13: ML Teams &amp; Startups (\ud83d\udc48 with detailed notes)</li> <li>Panel Discussion: Do I need a PhD to work in ML?</li> </ul>"},{"location":"spring2021/#week-14-16-projects","title":"Week 14-16: Projects","text":"<p>Those in the synchronous online course will submit 5-minute videos of their projects and associated write-ups by May 15.</p> <p>Check out the course projects showcase.</p>"},{"location":"spring2021/#other-resources","title":"Other Resources","text":"<p>Fast.ai is a great free two-course sequence aimed at first getting hackers to train state-of-the-art models as quickly as possible, and only afterward delving into how things work under the hood. Highly recommended for anyone.</p> <p>Dive Into Deep Learning is a great free textbook with Jupyter notebooks for every part of deep learning.</p> <p>NYU\u2019s Deep Learning course has excellent PyTorch breakdowns of everything important going on in deep learning.</p> <p>Stanford\u2019s ML Systems Design course has lectures that parallel those in this course.</p> <p>The Batch by Andrew Ng is a great weekly update on progress in the deep learning world.</p> <p>/r/MachineLearning/ is the best community for staying up to date with the latest developments.</p>"},{"location":"spring2021/lab-1/","title":"Lab 1: Setup and Introduction","text":""},{"location":"spring2021/lab-1/#video","title":"Video","text":"<p>Lab by Sergey Karayev.</p> <p>In this video, we introduce the lab throughout the course. We formulate the problem, provide the codebase structure, and train a simple Multilayer Perceptron on the MNIST dataset.</p> <ul> <li>4:11 - Understand the problem and path to solution</li> <li>5:54 - Set up the computing environment</li> <li>12:54 - Review the codebase</li> <li>24:55 - Train the MLP model on MNIST</li> </ul>"},{"location":"spring2021/lab-1/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lab-1/#follow-along","title":"Follow Along","text":"<p>GitHub Readme</p>"},{"location":"spring2021/lab-2/","title":"Lab 2: CNNs and Synthetic Data","text":""},{"location":"spring2021/lab-2/#video","title":"Video","text":"<p>Lab by Sergey Karayev.</p> <p>In this lab, you train a single-line ConvNet predictor on the EMNIST dataset and then synthetically generate your own data.</p> <ul> <li>00:00 - Introduction</li> <li>05:23 - Look at the EMNIST dataset</li> <li>09:52 - Train a base ConvNet model</li> <li>12:43 - Examine the ConvNet code</li> <li>17:33 - Lab 2 homework</li> <li>19:35 - Make a synthetic dataset of EMNIST lines</li> </ul>"},{"location":"spring2021/lab-2/#follow-along","title":"Follow Along","text":"<p>GitHub Readme</p>"},{"location":"spring2021/lab-3/","title":"Lab 3: RNNs","text":""},{"location":"spring2021/lab-3/#video","title":"Video","text":"<p>Lab by Sergey Karayev.</p> <ul> <li>00:00 - Introduction.</li> <li>01:59 - Introduce LineCNNSimple, a model that can read multiple characters in an image.</li> <li>15:52 - Make this model more efficient with LineCNN, which uses a fully convolutional network.</li> <li>18:18 - Upgrade the model further into LitModelCTC, which uses a CTC (Connectionist Temporal Classification) loss.</li> <li>23:29 - Finalize your model, LineCNNLSTM, by adding an LSTM layer on top of CNN.</li> <li>27:34 - Lab 3 homework.</li> </ul>"},{"location":"spring2021/lab-3/#follow-along","title":"Follow along","text":"<p>Readme</p>"},{"location":"spring2021/lab-4/","title":"Lab 4: Transformers","text":""},{"location":"spring2021/lab-4/#video","title":"Video","text":"<p>Lab by Sergey Karayev.</p> <p>In this lab, you use the LineCNN + LSTM model with CTC loss from lab 3 as an \"encoder\" of the image, and then send it through Transformer decoder layers.</p> <ul> <li>00:00 - Introduction</li> <li>01:43 - LineCNNTransformer class</li> <li>04:50 - TransformerLitModel</li> <li>06:51 - Code to make predictions</li> <li>08:50 - Training guidelines</li> </ul>"},{"location":"spring2021/lab-4/#follow-along","title":"Follow along","text":"<p>Readme</p>"},{"location":"spring2021/lab-5/","title":"Lab 5: Experiment Management","text":""},{"location":"spring2021/lab-5/#video","title":"Video","text":"<p>Lab by Sergey Karayev.</p> <p>In this lab, we'll use Weights and Biases to manage experiments for our handwriting recognition model.</p> <ul> <li>00:00 - Introduction</li> <li>00:56 - IAMLines Dataset</li> <li>05:29 - Make EMNISTLines more like IAMLines</li> <li>09:57 - Set up Weights and Biases</li> <li>13:42 - Run experiments on Weights and Biases</li> <li>22:58 - Configure W&amp;B sweeps to search for hyper-parameters</li> </ul>"},{"location":"spring2021/lab-5/#follow-along","title":"Follow along","text":"<p>Readme</p>"},{"location":"spring2021/lab-6/","title":"Lab 6: Data Labeling","text":""},{"location":"spring2021/lab-6/#video","title":"Video","text":"<p>Lab by Sergey Karayev.</p> <p>In this lab we will annotate some handwriting samples that we collected using the open-source tool Label Studio.</p>"},{"location":"spring2021/lab-6/#follow-along","title":"Follow along","text":"<p>Readme</p>"},{"location":"spring2021/lab-7/","title":"Lab 7: Paragraph Recognition","text":""},{"location":"spring2021/lab-7/#video","title":"Video","text":"<p>Lab by Sergey Karayev and Saurabh Bipin Chandra.</p> <p>In this lab, we will do several things:</p> <ul> <li>Move from training on synthetic line data to training on real data -- the <code>IAMLines</code> data module</li> <li>Move from training on line data to training on paragraph data -- the <code>IAMParagraphs</code> data module</li> <li>Automatically save the final model</li> <li>Introduce <code>ParagraphTextRecognizer</code> class to load the model and run inference that we can use in production</li> </ul>"},{"location":"spring2021/lab-7/#follow-along","title":"Follow along","text":"<p>Readme</p>"},{"location":"spring2021/lab-8/","title":"Lab 8: Testing &amp; CI","text":""},{"location":"spring2021/lab-8/#video","title":"Video","text":"<p>Lab by Sergey Karayev.</p> <p>In this lab, we </p> <ul> <li>Add linting</li> <li>Add prediction tests</li> <li>Add evaluation tests</li> <li>Set up continuous integration using CircleCI</li> </ul>"},{"location":"spring2021/lab-8/#follow-along","title":"Follow along","text":"<p>Readme</p>"},{"location":"spring2021/lab-9/","title":"Lab 9: Web Deployment","text":""},{"location":"spring2021/lab-9/#video","title":"Video","text":"<p>Lab by Sergey Karayev.</p> <p>In this lab, we do several things.</p> <ul> <li>First, we speed up our ParagraphTextRecognizer model with TorchScript</li> <li>Next, we wrap the model in a web app, and send it some requests</li> <li>We package up the web app and model as a Docker container, and run it that way</li> <li>Lastly, we prepare to deploy as a serverless function using AWS Lambda, getting it working locally.</li> </ul>"},{"location":"spring2021/lab-9/#follow-along","title":"Follow along","text":"<p>Readme</p>"},{"location":"spring2021/lecture-1/","title":"Lecture 1: DL Fundamentals","text":""},{"location":"spring2021/lecture-1/#video","title":"Video","text":""},{"location":"spring2021/lecture-1/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-1/#notes","title":"Notes","text":"<p>Lecture by Sergey Karayev.</p> <p>In this video, we discuss the fundamentals of deep learning. We will cover artificial neural networks, the universal approximation theorem, three major types of learning problems, the empirical risk minimization problem, the idea behind gradient descent, the practice of back-propagation, the core neural architectures, and the rise of GPUs.</p> <p>This should be a review for most of you; if not, then briefly go through this online book -neuralnetworksanddeeplearning.com.</p> <ul> <li>1:25\u200b - Neural Networks</li> <li>6:48\u200b - Universality</li> <li>8:48\u200b - Learning Problems</li> <li>16:17\u200b - Empirical Risk Minimization / Loss Functions</li> <li>19:55\u200b - Gradient Descent</li> <li>23:57\u200b - Backpropagation / Automatic Differentiation</li> <li>26:09\u200b - Architectural Considerations</li> <li>29:01\u200b - CUDA / Cores of Compute</li> </ul>"},{"location":"spring2021/lecture-10/","title":"Lecture 10: Testing &amp; Explainability","text":""},{"location":"spring2021/lecture-10/#video","title":"Video","text":""},{"location":"spring2021/lecture-10/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-10/#notes","title":"Notes","text":"<p>Download notes as PDF</p> <p>Lecture by Josh Tobin. Notes transcribed by James Le and Vishnu Rachakonda.</p>"},{"location":"spring2021/lecture-10/#1-whats-wrong-with-black-box-predictions","title":"1 - What\u2019s Wrong With Black-Box Predictions?","text":"<p>What does it mean when we have a good test set performance?</p> <p>If the test data and production data come from the same distribution, then in expectation, the performance of your model on your evaluation metrics will be the same.</p> <p>Let\u2019s unpack the bolded assumptions:</p> <ul> <li> <p>In the real world, the production distribution does not always match the offline distribution. You could have data drift, data shift, or even malicious users trying to attack your model.</p> </li> <li> <p>Expected performance does not tell the whole story. For instance, if you are working on long-tail data distribution, then the sample of data that you use to evaluate the model offline might not tell you much about the tail of that distribution - meaning that your test set score can be misleading. On top of that, if you evaluate your model with a single metric across your entire dataset, that does not mean your model is actually performing well against all the slices of data that might be important.</p> </li> <li> <p>The performance of your model is not equal to the performance of your machine learning system. There are other things (that can go wrong with the ML system) that do not have anything to do with the model.</p> </li> <li> <p>Finally, the test set performance only tells you about the metrics that you are evaluating. In the real world, you are probably not optimizing the exact metrics you care about deep down.</p> </li> </ul> <p>How bad is this problem? This is a quote from a former ML engineer at an autonomous vehicle company: \u201cI think the single biggest thing holding back the autonomous vehicle industry today is that, even if we had a car that worked, no one would know, because no one is confident that they know how to evaluate it properly.\u201d We believe that there is a similar sentiment to lesser degrees in other fields of machine learning, where the evaluation is the biggest bottleneck.</p> <p>The goal of this lecture is to introduce concepts and methods to help you, your team, and your users:</p> <ol> <li> <p>Understand at a deeper level how well your model is performing.</p> </li> <li> <p>Become more confident in your model\u2019s ability to perform well in production.</p> </li> <li> <p>Understand the model\u2019s performance envelope (where you should expect it to perform well and where not).</p> </li> </ol>"},{"location":"spring2021/lecture-10/#2-software-testing","title":"2 - Software Testing","text":""},{"location":"spring2021/lecture-10/#types-of-tests","title":"Types of Tests","text":"<p>There are three basic types of software tests:</p> <ol> <li> <p>Unit tests that test the functionality of a single piece of code (an assertion on a single function or a single class) in isolation.</p> </li> <li> <p>Integration tests that test how two or more units perform when used together (e.g., test if a model works well with a pre-processing function).</p> </li> <li> <p>End-to-end tests that test how the entire software system performs when all units are put together (e.g., test on realistic inputs from a real user).</p> </li> </ol> <p>Testing is a broad field, so you will likely encounter various other kinds of tests as well.</p>"},{"location":"spring2021/lecture-10/#best-practices","title":"Best Practices","text":"<p>Here are a couple of \u201cuncontroversial\u201d testing best practices:</p> <ul> <li> <p>Automate your tests: You have tests that run by themselves (typically via a CI/CD system) without a user committing an action. There should be no ambiguity on whether your system performs up to standard on the tests that are being run.</p> </li> <li> <p>Make sure your tests are reliable, run fast, and go through the same code review process as the rest of your code: The number of tests grows in proportion to the size of your codebase. If your tests are unreliable, then people will start ignoring them. If your tests are slow, then you won\u2019t want to run them frequently during development. If your tests do not undergo the code review process, they will have bugs, and it\u2019s better not to have them at all.</p> </li> <li> <p>Enforce that tests must pass before merging into the main branch: This is a good norm for teams with more than one person. This is a forcing function to make sure that everyone is committed to writing good tests and can also be helpful for regulatory concerns.</p> </li> <li> <p>When you find new production bugs, convert them to tests: This ensures that someone does not accidentally reintroduce those bugs in the future.</p> </li> <li> <p>Follow the testing pyramid: Introduced by Google, it says that you should write a lot more unit tests than integration tests and a lot more integration tests than end-to-end tests. Compared to end-to-end tests, unit tests are faster, more reliable, and better at isolating failures. The rule of thumb that Google recommends (as a rough split) is 70% unit tests, 20% integration tests, and 10% end-to-end tests.</p> </li> </ul> <p></p> <p>Next up, let\u2019s discuss a few \u201ccontroversial\u201d testing best practices:</p> <ul> <li> <p>Solitary tests: The distinction between a solitary test and a sociable test is that - solitary testing does not rely on real data from other units, while sociable testing makes the implicit assumption that other modules are working.</p> </li> <li> <p>Test coverage: You get a test coverage score for your codebase, which tells you what percentage of lines of code in your codebase is called by at least one test. Test coverage gives you a single metric that quantifies the quality of your testing suite. However, test coverage does not measure the right things (in particular, test quality).</p> </li> <li> <p>Test-driven development: In principle, you want to create your tests before you write your code. These tests serve as the specification of how the code functions. There are not many people who religiously stick to this methodology of development, but TDD is a valuable tool nonetheless.</p> </li> </ul> <p></p>"},{"location":"spring2021/lecture-10/#testing-in-production","title":"Testing In Production","text":"<p>The traditional view is that the goal of testing is to prevent shipping bugs into production. Therefore, by definition, you must do your testing offline before your system goes into production. However, there are two caveats:</p> <ol> <li> <p>Informal surveys reveal that the percentage of bugs found by automated tests is surprisingly low.</p> </li> <li> <p>On top of that, modern service-oriented distributed systems (which are deployed in most software engineering organizations nowadays) are particularly hard to test. The interactions between the components can get tricky.</p> </li> </ol> <p>Here is our philosophy for testing in production: Bugs are inevitable, so you might as well set up the system so that users can help you find them.</p> <p></p> <p>There are a few strategies to test in production:</p> <ol> <li> <p>Canary deployment: Do not roll out the new software version to all the users right away. Instead, just roll it out to a small percentage of your users and separately monitor that group\u2019s behavior.</p> </li> <li> <p>A/B testing: You can run a more principled statistical test if you have particular metrics that you care about: one for the old version of the code that is currently running and another for the new version that you are trying to test.</p> </li> <li> <p>Real user monitoring: Rather than looking at aggregate metrics (i.e., click-through rate), try to follow the journey that an actual user takes through your application and build a sense of how users experience the changes.</p> </li> <li> <p>Exploratory testing: Testing in production is not something that you want to automate fully. It should involve a bit of exploration (individual users or granular metrics).</p> </li> </ol>"},{"location":"spring2021/lecture-10/#continuous-integration-and-continuous-delivery","title":"Continuous Integration and Continuous Delivery","text":"<p>CI/CD platforms automate the tests that you run by hooking into your code repo. When you trigger some actions to take place (pushing new code, merging new code into a branch, submitting a pull request), CI/CD platforms kick off a job that is responsible for packaging your code, running all your tests, producing a report that tells you how well your code performs on your tests, and gatekeeping whether your new code can make it to the next stage. Tactically, you can define these jobs as commands in a Docker container and store the results for later review.</p> <p>SaaS solutions for continuous integration include CircleCI and Travis CI. Most of them do not have GPUs available. If you are just getting started, the default recommendation is GitHub Actions, which is super easy to integrate.</p> <p></p> <p>Jenkins and Buildkite are manual options for running continuous integration on your own hardware, in the cloud, or something in between. There is a lot more flexibility about the types of jobs you can run through the systems (meaning you can use your GPUs). The tradeoff is that they are harder to set up.</p>"},{"location":"spring2021/lecture-10/#3-testing-machine-learning-systems","title":"3 - Testing Machine Learning Systems","text":"<p>There are several core differences between traditional software systems and ML systems that add complexity to testing ML systems:</p> <ul> <li> <p>Software consists of only code, but ML combines code and data.</p> </li> <li> <p>Software is written by humans to solve a problem, while ML is compiled by optimizers to satisfy a proxy metric.</p> </li> <li> <p>Software is prone to loud failures, while ML is prone to silent failures.</p> </li> <li> <p>Software tends to be relatively static (in principle), while ML is constantly changing.</p> </li> </ul> <p>Due to such differences, here are common mistakes that teams make while testing ML systems:</p> <ul> <li> <p>Think the ML system is just a model and only test that model.</p> </li> <li> <p>Not test the data.</p> </li> <li> <p>Not build a granular enough understanding of the performance of the model before deploying it.</p> </li> <li> <p>Not measure the relationship between model performance metrics and business metrics.</p> </li> <li> <p>Rely too much on automated testing.</p> </li> <li> <p>Think offline testing is enough, and therefore, not monitor or test in production.</p> </li> </ul> <p></p> <p>Above is the diagram of how you can think of your entire production ML system that straddles across the offline and online environments:</p> <ul> <li> <p>Sitting in the middle is your ML model - an artifact created by your training process, which takes in an input and produces an output.</p> </li> <li> <p>The training system takes code and data as inputs and produces the trained model as the output.</p> </li> <li> <p>The prediction system takes in and pre-processes the raw data, loads the trained ML model, loads the model weights, calls model.predict() on the data, post-processes the outputs, and returns the predictions.</p> </li> <li> <p>Once you deploy your prediction system to the online environment, the serving system takes in requests from users, scales up and down to meet the traffic demands, and produces predictions back to those users.</p> </li> <li> <p>The whole ML system closes the loop by collecting production data (both the predictions that the model produces and additional feedback from users, business metrics, or labelers) and sending them back to the offline environment.</p> </li> <li> <p>The labeling system takes the raw data seen in production, helps you get inputs from labelers, and provides labels for that data.</p> </li> <li> <p>The storage and pre-processing system stores and pre-processes the labeled data before passing it back to the training system.</p> </li> </ul> <p>One way to think about how to test ML systems the right way is to think about the tests that you can run for each system component and across the border of these components.</p>"},{"location":"spring2021/lecture-10/#infrastructure-tests","title":"Infrastructure Tests","text":"<p>Infrastructure tests are unit tests for your training system. They help you avoid bugs in the training pipeline. You can unit test your training code like any other code. Another common practice is to add single-batch or single-epoch tests that check performance after an abbreviated training run on a tiny dataset, which catches obvious regressions to your training code. Tactically, you should run infrastructure tests frequently during the development process.</p>"},{"location":"spring2021/lecture-10/#training-tests","title":"Training Tests","text":"<p>Training tests are integration tests between your data system and your training system. They make sure that training jobs are reproducible.</p> <ul> <li> <p>You can pull a fixed dataset and run a full or abbreviated training run on it. Then, you want to check and ensure that the model performance on the newly trained model remains consistent with the reference performance.</p> </li> <li> <p>Another option is to pull a sliding window of data (maybe a new window for every few days) and run training tests on that window.</p> </li> <li> <p>Tactically, you should run training tests periodically, ideally nightly for frequently changing codebase.</p> </li> </ul>"},{"location":"spring2021/lecture-10/#functionality-tests","title":"Functionality Tests","text":"<p>Functionality tests are unit tests for your prediction system. They help you avoid regressions in code that makes up your prediction infrastructure.</p> <ul> <li> <p>You can unit test your prediction code like any other code.</p> </li> <li> <p>Specifically for the ML system, you can load a pre-trained model and test its predictions on a few key examples.</p> </li> <li> <p>Tactically, you should run functionality tests frequently during the development process.</p> </li> </ul>"},{"location":"spring2021/lecture-10/#evaluation-tests","title":"Evaluation Tests","text":"<p>Evaluation tests are integration tests between your training system and your prediction system. They make sure that a newly trained model is ready to go into production. These make up the bulk of what\u2019s unique about testing ML systems.</p> <ul> <li> <p>At a high level, you want to evaluate your model on all of the metrics, datasets, and slices that you care about.</p> </li> <li> <p>Then, you want to compare the new model to the old and baseline models.</p> </li> <li> <p>Finally, you want to understand the performance envelope of the new model.</p> </li> <li> <p>Operationally, you should run evaluation tests whenever you have a new candidate model considered for production.</p> </li> </ul> <p>It is important to note that evaluation tests are more than just the validation score. They look at all the metrics that you care about:</p> <ul> <li> <p>Model metrics: precision, recall, accuracy, L2, etc.</p> </li> <li> <p>Behavioral metrics: The goal of behavioral tests is to ensure the model has the invariances we expect. There are three types of behavioral tests: (1) invariance tests to assert that the change in inputs shouldn\u2019t affect outputs, (2) directional tests to assert that the change in inputs should affect outputs, and (3) minimum functionality tests to ensure that certain inputs and outputs should always produce a given result. Behavioral testing metrics are primarily used in NLP applications and proposed in the Beyond Accuracy paper by Ribeiro et al. (2020).</p> </li> <li> <p>Robustness metrics: The goal of robustness tests is to understand the model\u2019s performance envelope (i.e., where you should expect the model to fail). You can examine feature importance, sensitivity to staleness, sensitivity to data drift, and correlation between model performance and business metrics. In general, robustness tests are still under-rated.</p> </li> <li> <p>Privacy and fairness metrics: The goal of privacy and fairness tests is to distinguish whether your model might be biased against specific classes. Helpful resources are Google\u2019s Fairness Indicators and the Fairness Definitions Explained paper by Verma and Rubin (2018).</p> </li> <li> <p>Simulation metrics: The goal of simulation tests is to understand how the model performance could affect the rest of the system. These are useful when your model affects the real world (for systems such as autonomous vehicles, robotics, recommendation systems, etc.). Simulation tests are hard to do well because they require a model of how the world works and a dataset of different scenarios.</p> </li> </ul> <p>Instead of simply evaluating the aforementioned metrics on your entire dataset in aggregate, you should also evaluate these metrics on multiple slices of data. A slice is a mapping of your data to a specific category. A natural question that arises is how to pick those slices. Tools like What-If and SliceFinder help surface the slices where the model performance might be of particular interest.</p> <p>Finally, evaluation tests help you maintain evaluation datasets for all of the distinct data distributions you need to measure. Your main validation or test set should mirror your production distribution as closely as possible as a matter of principle. When should you add new evaluation datasets?</p> <ul> <li> <p>When you collect datasets to specify specific edge cases.</p> </li> <li> <p>When you run your production model on multiple data modalities.</p> </li> <li> <p>When you augment your training set with data not found in production (synthetic data).</p> </li> </ul> <p></p> <p>The report produced by the evaluation system entails the metrics broken down against each of the data slices. How can you decide whether the evaluation passes or fails?</p> <p>At a high level, you want to compare the new model to the previous model and another fixed older model. Tactically, you can (1) set thresholds on the differences between the new and the old models for most metrics, (2) set thresholds on the differences between data slices, and (3) set thresholds against the fixed older model to prevent slower performance \u201cleaks.\u201d</p>"},{"location":"spring2021/lecture-10/#shadow-tests","title":"Shadow Tests","text":"<p>Shadow tests are integration tests between your prediction system and your serving system. They help you catch production bugs before those bugs meet users. In many settings, models (which are built in frameworks such as sklearn, Pytorch, TensorFlow, etc.) are developed in isolation from the existing software system. For example, a model to flag inappropriate tweets may be developed in TensorFlow on a static set of data, not directly in the streaming environment of the broader software architecture. Because the prediction system and the serving system are developed in different settings with different assumptions and environments, there are many opportunities for bugs to creep in. These bugs can be tricky to catch prior to integration, so shadow tests can help identify them beforehand.</p> <p>Firstly, shadow tests help you detect bugs in the production deployment. In the code path you're using to build the production model, maybe there's some bug there. You want to make sure that you catch that before users see that bug.</p> <p>Secondly, shadow tests also help you detect inconsistencies between the offline model and the online model. There\u2019s a translation step in the training pipeline in many companies - going from the offline trained model to the online production model (the model itself, the preprocessing pipeline, etc.). A common bug source in production ML systems happens because of the inconsistencies cropping up in that translation step. A good health check ensures that your actual production model is producing the exact predictions on a fixed set of data as the model you have running on your laptop.</p> <p>Thirdly, shadow tests help you detect issues that don't appear on the data you have offline but appear on production data.</p> <p>How do we design shadow tests? These can require a significant amount of infrastructure, as they are dependent on actual model integration opportunities being available.</p> <ul> <li> <p>Typical shadow tests involve testing the performance of a candidate model on real data without returning or acting on the output. For example, a company may integrate and run a new model alongside the previous model without returning the output to the user.</p> </li> <li> <p>Analyzing the consistency of the predictions between the two models can help spot important differences before they impact production performance.</p> </li> <li> <p>Another option is to gather production data, save it offline, and test the model\u2019s performance on the fresh data offline.</p> </li> </ul> <p>Overall, evaluating the distribution of model predictions in offline vs. online settings, candidate vs. production, or any similar setting of a model update before deploying a new model can help you avoid bugs.</p>"},{"location":"spring2021/lecture-10/#ab-tests","title":"A/B Tests","text":"<p>Shadow tests evaluate the prediction performance of a model as part of the broader software architecture, but not the impact on users. A/B tests fill this role. A/B tests are a common practice in software engineering, especially in web systems. A/B testing is defined as \u201ca randomized experimentation process wherein two or more versions of a variable (web page, page element, etc.) are shown to different segments of website visitors at the same time to determine which version leaves the maximum impact and drive business metrics.\u201d[1]</p> <p></p> <p>In model evaluation, A/B tests determine the impact of different model predictions on user and business metrics. One common way of A/B testing models is to \u201ccanary\u201d data or return predictions on a small portion of the data (i.e., 1% or 10%) to the relevant users. The remaining data acts as a control and functions under existing system behavior (i.e., an old model or even no model). Evaluating the difference in metrics between the two groups can determine the relative impact of your model. This simple baseline can work well. Adding more statistically principled splits, which is common in A/B testing, can be a good idea.</p>"},{"location":"spring2021/lecture-10/#labeling-tests","title":"Labeling Tests","text":"<p>Machine learning models operate in a GIGO paradigm: garbage in, garbage out. To prevent poor quality labels from cropping up and corrupting the model, you need to unit test the labeling systems and procedures.</p> <ul> <li> <p>You should start by training, certifying, and evaluating individual labelers, who each play a crucial role in the quality of the labels.</p> </li> <li> <p>A simple and common label quality test is to spot check labels as they come in by opening up 100 or 1000 labels from a batch and evaluating them yourself to understand their quality. Using a performant model\u2019s guidance, you can make this process more efficient and only look at labels where the model and the labeler disagree.</p> </li> <li> <p>Another test can be to aggregate labels of multiple labels and measure agreement across labels. The higher the agreement, the better quality the labels are.</p> </li> <li> <p>Using metrics of agreement, you can assign \u201ctrust scores\u201d to labelers based on their performance relative to other labelers and weigh the labels accordingly.</p> </li> </ul>"},{"location":"spring2021/lecture-10/#expectation-tests","title":"Expectation Tests","text":"<p>Expectation tests address the data preprocessing and storage system. Essentially, they are unit tests for your data. They are designed to catch data quality issues and bad data before they make their way into the pipeline.</p> <p>The typical way that expectation tests operate is rule- or threshold-based. At each step of the data processing pipeline, the output should conform to a specific format that matches a rule or specific format. If the rule or threshold does not pass, then that stage of the expectation test and the data pipeline\u2019s related step fails. Such tests are frequently run with batch data pipeline jobs. Great Expectations is an open-source library gaining popularity for running expectation tests. The library allows you to set hard rules for the kinds of values or behaviors (i.e., statistics) you expect from your data.</p> <p>How do you set the rules and thresholds for expectation tests? Most expectation tests are set manually. A more sophisticated option is to profile a high-quality sample of your data and set thresholds accordingly. In practice, to avoid false alarms from overly sensitive tests, a combination of both approaches is needed.</p>"},{"location":"spring2021/lecture-10/#challenges-and-recommendations-operationalizing-ml-tests","title":"Challenges and Recommendations Operationalizing ML Tests","text":"<p>Running tests is an excellent idea in theory but can pose many practical challenges for data science and ML teams.</p> <ul> <li> <p>The first challenge is often organizational. In contrast to software engineering teams for whom testing is table stakes, data science teams often struggle to implement testing and code review norms.</p> </li> <li> <p>The second challenge is infrastructural. Most CI/CD platforms don\u2019t support GPUs, data integrations, or other required elements of testing ML systems effectively or efficiently.</p> </li> <li> <p>The third challenge is tooling, which has not yet been standardized for operations like comparing model performance and slicing datasets.</p> </li> <li> <p>Finally, decision-making for ML test performance is hard. What is \u201cgood enough\u201d test performance is often highly contextual, which is a challenge that varies across ML systems and teams.</p> </li> </ul> <p>Let\u2019s boil all these lessons for testing down into a clear set of recommendations specific to ML systems:</p> <ol> <li> <p>Test each part of the ML system, not just the model. You build the machine that builds the model, not just the model!</p> </li> <li> <p>Test code, data, and model performance, not just code.</p> </li> <li> <p>Testing model performance is an art, not a science. There is a considerable amount of intuition that guides testing ML systems.</p> </li> <li> <p>Thus, the fundamental goal of testing model performance is to build a granular understanding of how well your model performs and where you don\u2019t expect it to perform well. Using this intuition derived from testing, you can make better decisions about productionizing your model effectively.</p> </li> <li> <p>Build up to this gradually! You don\u2019t need to do everything detailed in this lecture, and certainly not all at once. Start with:</p> <ol> <li> <p>Infrastructure tests</p> </li> <li> <p>Evaluation tests</p> </li> <li> <p>Expectation tests</p> </li> </ol> </li> </ol>"},{"location":"spring2021/lecture-10/#4-explainable-and-interpretable-ai","title":"4 - Explainable and Interpretable AI","text":""},{"location":"spring2021/lecture-10/#definitions","title":"Definitions","text":"<p>What do explainable and interpretable AI, buzzwords you\u2019ve undoubtedly heard before, actually mean? Let\u2019s start by outlining some more fundamental terms about the problem space:</p> <ul> <li> <p>Domain predictability: the degree to which it is possible to detect data outside the model\u2019s domain of competence.</p> </li> <li> <p>Interpretability: the degree to which a human can consistently predict the model\u2019s result (Kim et al., 2016).</p> </li> <li> <p>Explainability: the degree to which a human can understand the cause of a decision (Miller, 2017).</p> </li> </ul> <p>We\u2019ll walk through four different methods of making models interpretable and explainable:</p> <ol> <li> <p>Use an interpretable family of models.</p> </li> <li> <p>Distill the complex model to an interpretable one.</p> </li> <li> <p>Understand the contribution of features to the prediction.</p> </li> <li> <p>Understand the contribution of training data points to the prediction.</p> </li> </ol>"},{"location":"spring2021/lecture-10/#use-an-interpretable-family-of-models","title":"Use An Interpretable Family of Models","text":"<p>Examples of interpretable families of models are simple, familiar models like linear regression, logistic regression, generalized linear models, and decision trees. If you understand the math of these models, it\u2019s pretty easy to understand why a model made the decision it did. Because of the reasonably elementary math, these models are interpretable and explainable. However, they are not very powerful.</p> <p>Another class of models that are interpretable is attention models. Examining where a model is \u201clooking\u201d helps us anticipate a model\u2019s prediction, thus making them interpretable. However, attention maps are not particularly explainable. They do not produce complete explanations for a model\u2019s output, just a directional explanation. Furthermore, attention maps are not reliable explanations. Attention maps tell us only where a model is looking, not why it is looking there. Frequently, models focus exclusively on an image\u2019s salient region without an underlying reasoning that relates to the task at hand. In the sample below, the attention model is \u201clooking\u201d at the salient region for classification, which has a very different meaning in each context.</p> <p></p> <p>The conflation of attention with explanation is a critical pitfall to avoid.</p>"},{"location":"spring2021/lecture-10/#distill-a-complex-to-an-interpretable-one","title":"Distill A Complex To An Interpretable One","text":"<p>Instead of restricting models to only interpretable families, we can fit a more complex model and interpret its decision using another model from an interpretable family. The trick is to train this additional model, referred to as a surrogate model, on the raw data and the complex model\u2019s predictions. The surrogate model\u2019s corresponding interpretation can be used as a proxy for understanding the complex model.</p> <p>This technique is quite simple and fairly general to apply. In practice, however, two concerns manifest.</p> <ol> <li> <p>If the surrogate itself performs well on the predictions, why not try to directly apply it rather than the more complex model?</p> </li> <li> <p>If it doesn\u2019t perform well, how do we know that it genuinely represents the complex model\u2019s behavior?</p> </li> </ol> <p></p> <p>Another category of surrogate models is local surrogate models (LIME). Rather than apply the surrogate model in a global context on all the data, LIME models focus on a single point to generate an explanation for. A perturbation is applied to the point, resulting in a local neighborhood of perturbed data points. On top of these perturbed data points, a surrogate model is trained to map the points to the original predictions from the complex model. If the surrogate model classifies similarly to the complex model, the surrogate can be considered a proxy for interpretation purposes. This method is used widely, as it works for all data types (including images and text). However, defining the right perturbations and ensuring the stability of the explanations is challenging.</p>"},{"location":"spring2021/lecture-10/#understand-the-contribution-of-features-to-the-prediction","title":"Understand The Contribution of Features To The Prediction","text":"<p>Better understanding each feature\u2019s role in making a prediction is another option for interpretable and explainable ML. Data visualization is one such option, with plots like partial dependence plots and individual conditional expectation plots.</p> <p></p> <p>A numerical method is permutation feature importance, which selects a feature, randomizes its order in the dataset, and sees how that affects performance. While this method is very easy and widely used, it doesn\u2019t work for high-dimensional data or cases where there is feature interdependence.</p> <p>A more principled approach to explaining the contribution of individual features is SHAP (Shapley Additive Explanations). At a high level, SHAP scores test how much changes in a single feature impact the output of a classifier when controlling for the values of the other features. This is a reliable method to apply, as it works on a variety of data and is mathematically principled. However, it can be tricky to implement and doesn\u2019t provide explanations.</p> <p>Gradient-based saliency maps are a popular method for explanations and interpretations. This intuitive method selects an input, performs a forward pass, computes the gradient with respect to the pixels, and visualizes the gradients. Essentially, how much does a unit change in the value of the input\u2019s pixels affect the prediction of the model? This is a straightforward and common method. Similar to the challenge with attention, the explanations may not be correct, and the overall method is fragile and sensitive to small changes.</p>"},{"location":"spring2021/lecture-10/#understand-the-contribution-of-training-data-points-to-the-prediction","title":"Understand The Contribution of Training Data Points To The Prediction","text":"<p>Instead of focusing on features and their explicit relevance to the prediction, we can also take a hard look at the training data points themselves.</p> <ul> <li> <p>Prototypes and criticisms are one such approach, though it is less applicable to deep learning. In this method, prototypes are clusters of data that explain much of the variance in the model. Criticisms are data points not explained by the prototypes.</p> </li> <li> <p>Another approach is to look specifically at \u201cinfluential instances\u201d or data points that cause major changes in the model\u2019s predictions when removed from the data set.</p> </li> </ul>"},{"location":"spring2021/lecture-10/#do-you-need-explainability","title":"Do You Need \"Explainability\"?","text":"<p>A good question to ask yourself whether or not \u201cexplainable AI\u201d is a real need for your applications. There are a couple of cases where this question can be useful:</p> <ol> <li> <p>Regulators demand it. In this case, there\u2019s not much you can do besides produce some kind of explainable model. However, it can be helpful to ask for clarification on what explainability is judged as.</p> </li> <li> <p>Users demand it. In some cases, users themselves may want trust or explainability in the system. Investigate the necessity for the explainability and trust to come directly from the model itself. Can good product design inspire trust more effectively? For example, allowing doctors to simply override models can reduce the immediate need for explainability. A big associated concern is how often users interact with the model. Infrequent interactions likely require explainable AI, as humans do not get a chance to build their feel for the system. More frequent interactions allow for the simpler objective of interpretability.</p> </li> <li> <p>Deployment demands it. Sometimes, ML stakeholders may demand explainability as a component of ensuring confidence in ML system deployment. In this context, explainability is the wrong objective; domain predictability is the real aim. Rather than full-on explainability, interpretability can be helpful for deployment, especially visualizations for debugging.</p> </li> </ol> <p>At present, true explainability for deep learning models is not possible.</p> <ul> <li> <p>Current explanation methods are not faithful to the original model performance; it can be easy to cherry-pick specific examples that can overstate explainability.</p> </li> <li> <p>Furthermore, these methods tend to be unreliable and highly sensitive to the input.</p> </li> <li> <p>Finally, as described in the attention section, the full explanation is often not available to modern explainability methods.</p> </li> </ul> <p>Because of these reasons, explainability is not practically feasible for deep learning models (as of 2021). Read Cynthia Rudin\u2019s 2019 paper for more detail.</p>"},{"location":"spring2021/lecture-10/#caveats-for-explainable-and-interpretable-ai","title":"Caveats For Explainable and Interpretable AI","text":"<ul> <li> <p>If you genuinely need to explain your model\u2019s predictions, use an interpretable model family (read more here).</p> </li> <li> <p>Don\u2019t try to force-fit deep learning explainability methods; they produce cool results but are not reliable enough for production use cases.</p> </li> <li> <p>Specific interpretability methods like LIME and SHAP are instrumental in helping users reach interpretability thresholds faster.</p> </li> <li> <p>Finally, the visualization for interpretability can be pretty useful for debugging.</p> </li> </ul>"},{"location":"spring2021/lecture-10/#5-resources","title":"5 - Resources","text":"<ul> <li> <p>ML Test Score Paper</p> </li> <li> <p>Behavioral testing paper</p> </li> <li> <p>Jeremy Jordan\u2019s effective testing</p> </li> <li> <p>Robustness Gym</p> </li> <li> <p>Made with ML\u2019s guide to testing</p> </li> <li> <p>Eugene Yan\u2019s practical guide to maintaining machine learning</p> </li> <li> <p>Chip Huyen\u2019s CS329 lecture on evaluating models</p> </li> <li> <p>Interpretable ML Book</p> </li> </ul> <p>[1] https://vwo.com/ab-testing-2/</p>"},{"location":"spring2021/lecture-11/","title":"Lecture 11: Deployment &amp; Monitoring","text":""},{"location":"spring2021/lecture-11/#video","title":"Video","text":"<p>Deployment:</p> <p>Monitoring:</p>"},{"location":"spring2021/lecture-11/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-11/#notes","title":"Notes","text":"<p>Download notes as PDF</p> <p>Lecture by Josh Tobin. Notes transcribed by James Le and Vishnu Rachakonda.</p> <p>ML in production scales to meet users\u2019 demands by delivering thousands to millions of predictions per second. On the other hand, models in notebooks only work if you run the cells in the right order. To be frank, most data scientists and ML engineers do not know how to build production ML systems. Therefore, the goal of this lecture is to give you different flavors of accomplishing that task.</p>"},{"location":"spring2021/lecture-11/#i-model-deployment","title":"I - Model Deployment","text":""},{"location":"spring2021/lecture-11/#1-types-of-deployment","title":"1 - Types of Deployment","text":"<p>One way to conceptualize different approaches to deploy ML models is to think about where to deploy them in your application\u2019s overall architecture.</p> <ul> <li> <p>The client-side runs locally on the user machine (web browser, mobile devices, etc..)</p> </li> <li> <p>It connects to the server-side that runs your code remotely.</p> </li> <li> <p>The server connects with a database to pull data out, render the data, and show the data to the user.</p> </li> </ul>"},{"location":"spring2021/lecture-11/#batch-prediction","title":"Batch Prediction","text":"<p>Batch prediction means that you train the models offline, dump the results into a database, then run the rest of the application normally. You periodically run your model on new data coming in and cache the results in a database. Batch prediction is commonly used in production when the universe of inputs is relatively small (e.g., one prediction per user per day).</p> <p>The pros of batch prediction:</p> <ul> <li> <p>It is simple to implement.</p> </li> <li> <p>It requires relatively low latency to the user.</p> </li> </ul> <p>The cons of batch prediction:</p> <ul> <li> <p>It does not scale to complex input types.</p> </li> <li> <p>Users do not get the most up-to-date predictions.</p> </li> <li> <p>Models frequently become \u201cstale\u201d and hard to detect.</p> </li> </ul>"},{"location":"spring2021/lecture-11/#model-in-service","title":"Model-In-Service","text":"<p>Model-in-service means that you package up your model and include it in the deployed web server. Then, the web server loads the model and calls it to make predictions.</p> <p>The pros of model-in-service prediction:</p> <ul> <li>It reuses your existing infrastructure.</li> </ul> <p>The cons of model-in-service prediction:</p> <ul> <li> <p>The web server may be written in a different language.</p> </li> <li> <p>Models may change more frequently than the server code.</p> </li> <li> <p>Large models can eat into the resources for your webserver.</p> </li> <li> <p>Server hardware is not optimized for your model (e.g., no GPUs).</p> </li> <li> <p>Model and server may scale differently.</p> </li> </ul>"},{"location":"spring2021/lecture-11/#model-as-service","title":"Model-As-Service","text":"<p>Model-as-service means that you deploy the model separately as its own service. The client and server can interact with the model by making requests to the model service and receiving responses.</p> <p>The pros of model-as-service prediction:</p> <ul> <li> <p>It is dependable, as model bugs are less likely to crash the web app.</p> </li> <li> <p>It is scalable, as you can choose the optimal hardware for the model and scale it appropriately.</p> </li> <li> <p>It is flexible, as you can easily reuse the model across multiple applications.</p> </li> </ul> <p>The cons of model-as-service prediction:</p> <ul> <li> <p>It adds latency.</p> </li> <li> <p>It adds infrastructural complexity.</p> </li> <li> <p>Most importantly, you are now on the hook to run a model service...</p> </li> </ul>"},{"location":"spring2021/lecture-11/#2-building-a-model-service","title":"2 - Building A Model Service","text":""},{"location":"spring2021/lecture-11/#rest-apis","title":"REST APIs","text":"<p>REST APIs represent a way of serving predictions in response to canonically formatted HTTP requests. There are alternatives such as gRPC and GraphQL. For instance, in your command line, you can use curl to post some data to an URL and get back JSON that contains the model predictions.</p> <p>Sadly, there is no standard way of formatting the data that goes into an ML model.</p>"},{"location":"spring2021/lecture-11/#dependency-management","title":"Dependency Management","text":"<p>Model predictions depend on the code, the model weights, and the code dependencies. All three need to be present on your webserver. For code and model weights, you can simply copy them locally (or write a script to extract them if they are large). But dependencies are trickier because they cause troubles. As they are hard to make consistent and update, your model behavior might change accordingly.</p> <p>There are two high-level strategies to manage code dependencies:</p> <ol> <li> <p>You constrain the dependencies of your model.</p> </li> <li> <p>You use containers.</p> </li> </ol>"},{"location":"spring2021/lecture-11/#onnx","title":"ONNX","text":"<p>If you go with the first strategy, you need a standard neural network format. The Open Neural Network Exchange (ONNX, for short) is designed to allow framework interoperability. The dream is to mix different frameworks, such that frameworks that are good for development (PyTorch) don\u2019t also have to be good at inference (Caffe2).</p> <ul> <li> <p>The promise is that you can train a model with one tool stack and then deploy it using another for inference/prediction. ONNX is a robust and open standard for preventing framework lock-in and ensuring that your models will be usable in the long run.</p> </li> <li> <p>The reality is that since ML libraries change quickly, there are often bugs in the translation layer. Furthermore, how do you deal with non-library code (like feature transformations)?</p> </li> </ul>"},{"location":"spring2021/lecture-11/#docker","title":"Docker","text":"<p>If you go with the second strategy, you want to learn Docker. Docker is a computer program that performs operating-system-level virtualization, also known as containerization. What is a container, you might ask? It is a standardized unit of fully packaged software used for local development, shipping code, and deploying system.</p> <p>The best way to describe it intuitively is to think of a process surrounded by its filesystem. You run one or a few related processes, and they see a whole filesystem, not shared by anyone.</p> <ul> <li> <p>This makes containers extremely portable, as they are detached from the underlying hardware and the platform that runs them.</p> </li> <li> <p>They are very lightweight, as a minimal amount of data needs to be included.</p> </li> <li> <p>They are secure, as the exposed attack surface of a container is extremely small.</p> </li> </ul> <p>Note here that containers are different from virtual machines.</p> <ul> <li> <p>Virtual machines require the hypervisor to virtualize a full hardware stack. There are also multiple guest operating systems, making them larger and more extended to boot. This is what AWS / GCP / Azure cloud instances are.</p> </li> <li> <p>Containers, on the other hand, require no hypervisor/hardware virtualization. All containers share the same host kernel. There are dedicated isolated user-space environments, making them much smaller in size and faster to boot.</p> </li> </ul> <p></p> <p>In brief, you should familiarize yourself with these basic concepts:</p> <ol> <li> <p>Dockerfile defines how to build an image.</p> </li> <li> <p>Image is a built packaged environment.</p> </li> <li> <p>Container is where images are run inside.</p> </li> <li> <p>Repository hosts different versions of an image.</p> </li> <li> <p>Registry is a set of repositories.</p> </li> </ol> <p>Furthermore, Docker has a robust ecosystem. It has the DockerHub for community-contributed images. It\u2019s incredibly easy to search for images that meet your needs, ready to pull down and use with little-to-no modification.</p> <p>Though Docker presents how to deal with each of the individual microservices, we also need an orchestrator to handle the whole cluster of services. Such an orchestrator distributes containers onto the underlying virtual machines or bare metal so that these containers talk to each other and coordinate to solve the task at hand. The standard container orchestration tool is Kubernetes.</p>"},{"location":"spring2021/lecture-11/#performance-optimization","title":"Performance Optimization","text":"<p>We will talk mostly about how to run your model service faster on a single machine. Here are the key questions that you want to address:</p> <ul> <li> <p>Do you want inference on a GPU or not?</p> </li> <li> <p>How can you run multiple copies of the model at the same time?</p> </li> <li> <p>How to make the model smaller?</p> </li> <li> <p>How to improve model performance via caching, batching, and GPU sharing?</p> </li> </ul>"},{"location":"spring2021/lecture-11/#gpu-or-no-gpu","title":"GPU or no GPU?","text":"<p>Here are the pros of GPU inference:</p> <ul> <li> <p>You use the same hardware that your model is trained on probably.</p> </li> <li> <p>If your model gets bigger and you want to limit model size or tune batch size, you will get high throughput.</p> </li> </ul> <p>Here are the cons of GPU inference:</p> <ul> <li> <p>GPU is complex to set up.</p> </li> <li> <p>GPUs are expensive.</p> </li> </ul>"},{"location":"spring2021/lecture-11/#concurrency","title":"Concurrency","text":"<p>Instead of running a single model copy on your machine, you run multiple model copies on different CPUs or cores. In practice, you need to be careful about thread tuning - making sure that each model copy only uses the minimum number of threads required. Read this blog post from Roblox for the details.</p>"},{"location":"spring2021/lecture-11/#model-distillation","title":"Model distillation","text":"<p>Model distillation is a compression technique in which a small \u201cstudent\u201d model is trained to reproduce the behavior of a large \u201cteacher\u201d model. The method was first proposed by Bucila et al., 2006 and generalized by Hinton et al., 2015. In distillation, knowledge is transferred from the teacher model to the student by minimizing a loss function. The target is the distribution of class probabilities predicted by the teacher model. That is\u200a\u2014\u200athe output of a softmax function on the teacher model\u2019s logits.</p> <p>Distillation can be finicky to do yourself, so it is infrequently used in practice. Read this blog post from Derrick Mwiti for several model distillation techniques for deep learning.</p>"},{"location":"spring2021/lecture-11/#model-quantization","title":"Model quantization","text":"<p>Model quantization is a model compression technique that makes the model physically smaller to save disk space and require less memory during computation to run faster. It decreases the numerical precision of a model\u2019s weights. In other words, each weight is permanently encoded using fewer bits. Note here that there are tradeoffs with accuracy.</p> <ul> <li> <p>A straightforward method is implemented in the TensorFlow Lite toolkit. It turns a matrix of 32-bit floats into 8-bit integers by applying a simple \u201ccenter-and-scale\u201d transform to it: W_8 = W_32 / scale + shift (scale and shift are determined individually for each weight matrix). This way, the 8-bit W is used in matrix multiplication, and only the result is then corrected by applying the \u201ccenter-and-scale\u201d operation in reverse.</p> </li> <li> <p>PyTorch also has quantization built-in that includes three techniques: dynamic quantization, post-training static quantization, and quantization-aware training.</p> </li> </ul>"},{"location":"spring2021/lecture-11/#caching","title":"Caching","text":"<p>For many ML models, the input distribution is non-uniform (some are more common than others). Caching takes advantage of that. Instead of constantly calling the model on every input no matter what, we first cache the model\u2019s frequently-used inputs. Before calling the model, we check the cache and only call it on the frequently-used inputs.</p> <p>Caching techniques can get very fancy, but the most basic way to get started is using Python\u2019s functools.</p> <p></p>"},{"location":"spring2021/lecture-11/#batching","title":"Batching","text":"<p>Typically, ML models achieve higher throughput when making predictions in parallel (especially true for GPU inference). At a high level, here\u2019s how batching works:</p> <ul> <li> <p>You gather predictions that are coming in until you have a batch for your system. Then, you run the model on that batch and return predictions to those users who request them.</p> </li> <li> <p>You need to tune the batch size and address the tradeoff between throughput and latency.</p> </li> <li> <p>You need to have a way to shortcut the process if latency becomes too long.</p> </li> <li> <p>The last caveat is that you probably do not want to implement batching yourself.</p> </li> </ul>"},{"location":"spring2021/lecture-11/#sharing-the-gpu","title":"Sharing The GPU","text":"<p>Your model may not take up all of the GPU memory with your inference batch size. Why not run multiple models on the same GPU? You probably want to use a model serving solution that supports this out of the box.</p>"},{"location":"spring2021/lecture-11/#model-serving-libraries","title":"Model Serving Libraries","text":"<p>There are canonical open-source model serving libraries for both PyTorch (TorchServe) and TensorFlow (TensorFlow Serving). Ray Serve is another promising choice. Even NVIDIA has joined the game with Triton Inference Server.</p> <p></p>"},{"location":"spring2021/lecture-11/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>If you have too much traffic for a single machine, let\u2019s split traffic among multiple machines. At a high level, you duplicate your prediction service, use a load balancer to split traffic, and send the traffic to the appropriate copy of your service. In practice, there are two common methods:</p> <ol> <li> <p>Use a container orchestration toolkit like Kubernetes.</p> </li> <li> <p>Use a serverless option like AWS Lambda.</p> </li> </ol>"},{"location":"spring2021/lecture-11/#container-orchestration","title":"Container Orchestration","text":"<p>In this paradigm, your Docker containers are coordinated by Kubernetes. K8s provides a single service for you to send requests to. Then it divides up traffic that gets sent to that service to virtual copies of your containers (that are running on your infrastructure).</p> <p>You can build a system like this yourself on top of K8s if you want to. But there are emerging frameworks that can handle all such infrastructure out of the box if you have a K8s cluster running. KFServing is a part of the Kubeflow package, a popular K8s-native ML infrastructure solution. Seldon provides a model serving stack on top of K8s.</p>"},{"location":"spring2021/lecture-11/#deploying-code-as-serverless-functions","title":"Deploying Code As Serverless Functions","text":"<p>The idea here is that the app code and dependencies are packaged into .zip files (or Docker containers) with a single entry point function. All the major cloud providers such as AWS Lambda, Google Cloud Functions, or Azure Functions will manage everything else: instant scaling to 10,000+ requests per second, load balancing, etc.</p> <p></p> <p>The good thing is that you only pay for compute-time. Furthermore, this approach lowers your DevOps load, as you do not own any servers.</p> <p>The tradeoff is that you have to work with severe constraints:</p> <ol> <li> <p>Your entire deployment package is quite limited.</p> </li> <li> <p>You can only do CPU execution.</p> </li> <li> <p>It can be challenging to build model pipelines.</p> </li> <li> <p>There are limited state management and deployment tooling.</p> </li> </ol>"},{"location":"spring2021/lecture-11/#model-deployment","title":"Model Deployment","text":"<p>If serving is how you turn a model into something that can respond to requests, deployment is how you roll out, manage, and update these services. You probably want to be able to roll out gradually, roll back instantly, and deploy pipelines of models. Many challenging infrastructure considerations go into this, but hopefully, your deployment library will take care of this for you.</p>"},{"location":"spring2021/lecture-11/#managed-options","title":"Managed Options","text":"<p>If you do not want to deal with any of the things mentioned thus far, there are managed options in the market. All major cloud providers have ones that enable you to package your model in a predefined way and turn it into an API. Startups like Algorithmia and Cortex are some alternatives. The big drawback is that pricing tends to be high, so you pay a premium fee in exchange for convenience.</p>"},{"location":"spring2021/lecture-11/#takeaways","title":"Takeaways","text":"<ul> <li> <p>If you are making CPU inference, you can get away with scaling by launching more servers or going serverless.</p> </li> <li> <p>Serverless makes sense if you can get away with CPUs, and traffic is spiky or low-volume.</p> </li> <li> <p>If you are using GPU inference, serving tools will save you time.</p> </li> <li> <p>It\u2019s worth keeping an eye on startups in this space for GPU inference.</p> </li> </ul>"},{"location":"spring2021/lecture-11/#3-edge-deployment","title":"3 - Edge Deployment","text":"<p>Edge prediction means that you first send the model weights to the client edge device. Then, the client loads the model and interacts with it directly.</p> <p>The pros of edge prediction:</p> <ul> <li> <p>It has low latency.</p> </li> <li> <p>It does not require an Internet connection.</p> </li> <li> <p>It satisfies data security requirements, as data does not need to leave the user\u2019s device.</p> </li> </ul> <p>The cons of edge prediction:</p> <ul> <li> <p>The client often has limited hardware resources available.</p> </li> <li> <p>Embedded and mobile frameworks are less full-featured than TensorFlow and PyTorch.</p> </li> <li> <p>It is challenging to update models.</p> </li> <li> <p>It is difficult to monitor and debug when things go wrong.</p> </li> </ul>"},{"location":"spring2021/lecture-11/#tools-for-edge-deployment","title":"Tools For Edge Deployment","text":"<p>TensorRT is NVIDIA\u2019s framework meant to help you optimize models for inference on NVIDIA devices in data centers and embedded/automotive environments. TensorRT is also integrated with application-specific SDKs to provide developers a unified path to deploy conversational AI, recommender, video conference, and streaming apps in production.</p> <p></p> <p>ApacheTVM is an open-source machine learning compiler framework for CPUs, GPUs, and ML accelerators. It aims to enable ML engineers to optimize and run computations efficiently on any hardware backend. In particular, it compiles ML models into minimum deployable modules and provides the infrastructure to automatically optimize models on more backends with better performance.</p> <p>Tensorflow Lite provides a trained TensorFlow model framework to be compressed and deployed to a mobile or embedded application. TensorFlow\u2019s computationally expensive training process can still be performed in the environment that best suits it (personal server, cloud, overclocked computer). TensorFlow Lite then takes the resulting model (frozen graph, SavedModel, or HDF5 model) as input, packages, deploys, and then interprets it in the client application, handling the resource-conserving optimizations along the way.</p> <p></p> <p>PyTorch Mobile is a framework for helping mobile developers and machine learning engineers embed PyTorch models on-device. Currently, it allows any TorchScript model to run directly inside iOS and Android applications. PyTorch Mobile\u2019s initial release supports many different quantization techniques, which shrink model sizes without significantly affecting performance. PyTorch Mobile also allows developers to directly convert a PyTorch model to a mobile-ready format without needing to work through other tools/frameworks.</p> <p>JavaScript is a portable way of running code on different devices. Tensorflow.js enables you to run TensorFlow code in JavaScript. You can use off-the-shelf JavaScript models or convert Python TensorFlow models to run in the browser or under Node.js, retrain pre-existing ML models using your data, and build/train models directly in JavaScript using flexible and intuitive APIs.</p> <p>Core ML was released by Apple back in 2017. It is optimized for on-device performance, which minimizes a model\u2019s memory footprint and power consumption. Running strictly on the device also ensures that user data is kept secure. The app runs even in the absence of a network connection. Generally speaking, it is straightforward to use with just a few lines of code needed to integrate a complete ML model into your device. The downside is that you can only make the model inference, as no model training is possible.</p> <p></p> <p>ML Kit was announced by Google Firebase in 2018. It enables developers to utilize ML in mobile apps either with (1) inference in the cloud via API or (2) inference on-device (like Core ML). For the former option, ML Kit offers six base APIs with pertained models such as Image Labeling, Text Recognition, and Barcode Scanning. For the latter option, ML Kit offers lower accuracy but more security to user data, compared to the cloud version.</p> <p>If you are interested in either of the above options, check out this comparison by the FritzAI team. Additionally, FritzAI is an ML platform for mobile developers that provide pre-trained models, developer tools, and SDKs for iOS, Android, and Unity.</p>"},{"location":"spring2021/lecture-11/#more-efficient-models","title":"More Efficient Models","text":"<p>Another thing to consider for edge deployment is to make the models more efficient. One way to do this is to use the same quantization and distillation techniques discussed above. Another way is to pick mobile-friendly model architectures. The first successful example is MobileNet, which performs various downsampling techniques to a traditional ConvNet architecture to maximize accuracy while being mindful of the restricted resources for a mobile or an embedded device. This analysis by Yusuke Uchida explains why MobileNet and its variants are fast.</p> <p>A well-known case study of applying knowledge distillation in practice is Hugging Face\u2019s DistilBERT, a smaller language model derived from the supervision of the popular BERT language model. DistilBERT removes the toke-type embeddings and the pooler (used for the next sentence classification task) from BERT while keeping the rest of the architecture identical and reducing the number of layers by a factor of two. Overall, DistilBERT has about half the total number of parameters of the BERT base and retains 95% of BERT\u2019s performances on the language understanding benchmark GLUE.</p>"},{"location":"spring2021/lecture-11/#mindset-for-edge-deployment","title":"Mindset For Edge Deployment","text":"<ul> <li> <p>It is crucial to choose your architecture with your target hardware in mind. Specifically, you can make up a factor of 2-10 through distillation, quantization, and other tricks (but not more than that).</p> </li> <li> <p>Once you have a model that works on your edge device, you can iterate locally as long as you add model size and latency to your metrics and avoid regression.</p> </li> <li> <p>You should treat tuning the model for your device as an additional risk in the deployment cycle and test it accordingly. In other words, you should always test your models on production hardware before deploying them for real.</p> </li> <li> <p>Since models can be finicky, it\u2019s a good idea to build fallback mechanisms into the application if the model fails or is too slow.</p> </li> </ul>"},{"location":"spring2021/lecture-11/#takeaways_1","title":"Takeaways","text":"<ul> <li> <p>Web deployment is easier, so only perform edge deployment if you need to.</p> </li> <li> <p>You should choose your framework to match the available hardware and corresponding mobile frameworks. Else, you can try Apache TVM to be more flexible.</p> </li> <li> <p>You should start considering hardware constraints at the beginning of the project and choose the architectures accordingly.</p> </li> </ul>"},{"location":"spring2021/lecture-11/#ii-model-monitoring","title":"II - Model Monitoring","text":"<p>Once you deploy models, how do you make sure they are staying healthy and working well? Enter model monitoring.</p> <p>Many things can go wrong with a model once it\u2019s been trained. This can happen even if your model has been trained properly, with a reasonable validation and test loss, as well as robust performance across various slices and quality predictions. Even after you\u2019ve troubleshot and tested a model, things can still go wrong!</p>"},{"location":"spring2021/lecture-11/#1-why-model-degrades-post-deployment","title":"1 - Why Model Degrades Post-Deployment?","text":"<p>Model performance tends to degrade after you\u2019ve deployed a model. Why does this occur? In supervised learning, we seek to fit a function f to approximate a posterior using the data available to us. If any component of this process changes (i.e., the data x), the deployed model can see an unexpectedly degraded performance. See the below chart for examples of how such post-deployment degradations can occur theoretically and in practice:</p> <p></p> <p>In summary, there are three core ways that the model\u2019s performance can degrade: data drift, concept drift, and domain shift.</p> <ol> <li> <p>In data drift, the underlying data expectation that your model is built can unexpectedly change, perhaps through a bug in the upstream data pipeline or even due to malicious users feeding the model bad data.</p> </li> <li> <p>In concept drift, the actual outcome you seek to model, or the relationship between the data and the outcome, may fray. For example, users may start to pick movies in a different manner based on the output of your model, thereby changing the fundamental \u201cconcept\u201d the model needs to approximate.</p> </li> <li> <p>Finally, in domain shift, if your dataset does not appropriately sample the production, post-deployment setting, the model\u2019s performance may suffer; this could be considered a \u201clong tail\u201d scenario, where many rare examples that are not present in the development data occur.</p> </li> </ol>"},{"location":"spring2021/lecture-11/#2-data-drift","title":"2 - Data Drift","text":"<p>There are a few different types of data drift:</p> <ul> <li> <p>Instantaneous drift: In this situation, the paradigm of the draft dramatically shifts. Examples are deploying the model in a new domain (e.g., self-driving car model in a new city), a bug in the preprocessing pipeline, or even major external shifts like COVID.</p> </li> <li> <p>Gradual drift: In this situation, the value of data gradually changes with time. For example, users\u2019 preferences may change over time, or new concepts can get introduced to the domain.</p> </li> <li> <p>Periodic drift: Data can have fluctuating value due to underlying patterns like seasonality or time zones.</p> </li> <li> <p>Temporary drift: The most difficult to detect, drift can occur through a short-term change in the data that shifts back to normal. This could be via a short-lived malicious attack, or even simply because a user with different demographics or behaviors uses your product in a way that it\u2019s not designed to be used.</p> </li> </ul> <p>While these categories may seem like purely academic categories, the consequences of data shift are very real. This is a real problem that affects many companies and is only now starting to get the attention it merits.</p>"},{"location":"spring2021/lecture-11/#3-what-should-you-monitor","title":"3 - What Should You Monitor?","text":"<p>There are four core types of signals to monitor for machine learning models.</p> <p></p> <p>These metrics trade off with another in terms of how informative they are and how easy they are to access. Put simply, the harder a metric may be to monitor, the more useful it likely is.</p> <ul> <li> <p>The hardest and best metrics to monitor are model performance metrics, though these can be difficult to acquire in real-time (labels are hard to come by).</p> </li> <li> <p>Business metrics can be helpful signals of model degradation in monitoring but can easily be confounded by other impactful considerations.</p> </li> <li> <p>Model inputs and predictions are a simple way to identify high-level drift and are very easy to gather. Still, they can be difficult to assess in terms of actual performance impact, leaving it more of an art than science.</p> </li> <li> <p>Finally, system performance (e.g., GPU usage) can be a coarse method of catching serious bugs.</p> </li> </ul> <p>In considering which metrics to focus on, prioritize ground-truth metrics (model and business metrics), then approximate performance metrics (business and input/outputs), and finally, system health metrics.</p>"},{"location":"spring2021/lecture-11/#4-how-do-you-measure-distribution-changes","title":"4 - How Do You Measure Distribution Changes?","text":""},{"location":"spring2021/lecture-11/#select-a-reference-window","title":"Select A Reference Window","text":"<p>To measure distribution changes in metrics you\u2019re monitoring, start by picking a reference set of production data to compare new data to. There are a few different ways of picking this reference data (e.g., sliding window or fixed window of production data), but the most practical thing to do is to use your training or evaluation data as the reference. Data coming in looking different from what you developed your model using is an important signal to act on.</p>"},{"location":"spring2021/lecture-11/#select-a-measurement-window","title":"Select A Measurement Window","text":"<p>After picking a reference window, the next step is to choose a measurement window to compare, measure distance, and evaluate for drift. The challenge is that selecting a measurement window is highly problem-dependent. One solution is to pick one or several window sizes and slide them over the data. To avoid recomputing metrics over and over again, when you slide the window, it\u2019s worth looking into the literature on mergeable (quantile) sketching algorithms.</p>"},{"location":"spring2021/lecture-11/#compare-windows-using-a-distance-metric","title":"Compare Windows Using A Distance Metric","text":"<p>What distance metrics should we use to compare the reference window to the measurement window? Some 1-D metric categories are:</p> <ol> <li> <p>Rule-based distance metrics (e.g., data quality): Summary statistics, the volume of data points, number of missing values, or more complex tests like overall comparisons are common data quality checks that can be applied. Great Expectations is a valuable library for this. Definitely invest in simple rule-based metrics. They catch a large number of bugs, as publications from Amazon and Google detail.</p> </li> <li> <p>Statistical distance metrics (e.g., KS statistics, KL divergence, D_1 distance, etc.)</p> <ol> <li> <p>KL Divergence: Defined as the expectation of a ratio of logs of two different distributions, this commonly known metric is very sensitive to what happens in the tails of the distribution. It\u2019s not well-suited to data shift testing since it\u2019s easily disturbed, is not interpretable, and struggles with data in different ranges.</p> </li> <li> <p>KS Statistic: This metric is defined as the max distance between CDFs, which is easy to interpret and is thus used widely in practice. Say yes to the KS statistic!</p> </li> <li> <p>D1 Distance: Defined as the sum of distances between PDFs, this is a metric used at Google. Despite seeming less principled, it\u2019s easily interpretable and has the added benefit of knowing Google uses it (so why not you?).</p> </li> </ol> </li> </ol> <p></p> <p>An open area of research is understanding the impact of differing drift patterns on distance metrics and model performance. Another open area of research is high-dimensional distance metrics. Some options here are:</p> <ol> <li> <p>Maximum mean discrepancy</p> </li> <li> <p>Performing multiple 1D comparisons across the data: While suffering from the multiple hypothesis testing problem, this is a practical approach.</p> </li> <li> <p>Prioritize some features for 1D comparisons: Another option is to avoid testing all the features and only focus on those that merit comparison; for example, those features you know may have shifted in the data.</p> </li> <li> <p>Projections: In this approach, large data points are put through a dimensionality reduction process and then subject to a two-sample statistical test. Reducing the dimensionality with a domain-specific approach (e.g., mean pixel value for images, length of sentence) is recommended.</p> </li> </ol> <p>At a high level, this entire distance metric work aims to identify not just a score for any data shift but also understand its impact on the model. While choosing a metric can be complicated with all the possible options, you should focus on understanding your model\u2019s robustness in a post-deployment scenario.</p>"},{"location":"spring2021/lecture-11/#5-how-do-you-tell-if-a-change-is-bad","title":"5 - How Do You Tell If A Change Is Bad?","text":"<p>There\u2019s no hard and fast rule for finding if a change in the data is bad. An easy option is to set thresholds on the test values. Don\u2019t use a statistical test like the KS test, as they are too sensitive to small shifts. Other options include setting manual ranges, comparing values over time, or even applying an unsupervised model to detect outliers. In practice, fixed rules and specified ranges of test values are used most in practice.</p> <p></p>"},{"location":"spring2021/lecture-11/#6-tools-for-monitoring","title":"6 - Tools For Monitoring","text":"<p>There are three categories of tools useful for monitoring:</p> <ol> <li> <p>System monitoring tools like AWS CloudWatch, Datadog, New Relic, and honeycomb test traditional performance metrics</p> </li> <li> <p>Data quality tools like Great Expectations, Anomalo, and Monte Carlo test if specific windows of data violate rules or assumptions.</p> </li> <li> <p>ML monitoring tools like Arize, Fiddler, and Arthur can also be useful, as they specifically test models.</p> </li> </ol>"},{"location":"spring2021/lecture-11/#7-evaluation-store","title":"7 - Evaluation Store","text":"<p>Monitoring is more central to ML than for traditional software.</p> <ul> <li> <p>In traditional SWE, most bugs cause loud failures, and the data that is monitored is most valuable to detect and diagnose problems. If the system is working well, the data from these metrics and monitoring systems may not be useful.</p> </li> <li> <p>In machine learning, however, monitoring plays a different role. First off, bugs in ML systems often lead to silent degradations in performance. Furthermore, the data that is monitored in ML is literally the code used to train the next iteration of models.</p> </li> </ul> <p>Because monitoring is so essential to ML systems, tightly integrating it into the ML system architecture brings major benefits. In particular, better integrating and monitoring practices, or creating an evaluation store, can close the data flywheel loop, a concept we talked about earlier in the class.</p> <p></p> <p>As we build models, we create a mapping between data and model. As the data changes and we retrain models, monitoring these changes doesn\u2019t become an endpoint--it becomes a part of the entire model development process. Monitoring, via an evaluation store, should touch all parts of your stack. One challenge that this process helps solve is effectively choosing which data points to collect, store, and label. Evaluation stores can help identify which data to collect more points for based on uncertain performance. As more data is collected and labeled, efficient retraining can be performed using evaluation store guidance.</p>"},{"location":"spring2021/lecture-11/#conclusion","title":"Conclusion","text":"<p>In summary, make sure to monitor your models!</p> <ul> <li> <p>Something will always go wrong, and you should have a system to catch errors.</p> </li> <li> <p>Start by looking at data quality metrics and system metrics, as they are easiest.</p> </li> <li> <p>In a perfect world, the testing and monitoring should be linked, and they should help you close the data flywheel.</p> </li> <li> <p>There will be a lot of tooling and research that will hopefully come soon!</p> </li> </ul>"},{"location":"spring2021/lecture-12/","title":"Lecture 12: Research Directions","text":""},{"location":"spring2021/lecture-12/#video","title":"Video","text":""},{"location":"spring2021/lecture-12/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-12/#notes","title":"Notes","text":"<p>Download notes as PDF</p> <p>Lecture by Pieter Abbeel. Notes transcribed by James Le and Vishnu Rachakonda.</p> <p>Of all disciplines, deep learning is probably the one where research and practice are closest together. Often, something gets invented in research and is put into production in less than a year. Therefore, it\u2019s good to be aware of research trends that you might want to incorporate in projects you are working on.</p> <p>Because the number of ML and AI papers increases exponentially, there\u2019s no way that you can read every paper. Thus, you need other methods to keep up with research. This lecture provides a sampling of research directions, the overall research theme running across these samples, and advice on keeping up with the relentless flood of new research.</p>"},{"location":"spring2021/lecture-12/#1-unsupervised-learning","title":"1 - Unsupervised Learning","text":"<p>Deep supervised learning, the default way of doing ML, works! But it requires so much annotated data. Can we get around it by learning with fewer labels? The answer is yes! And there are two major approaches: deep semi-supervised learning and deep unsupervised learning.</p>"},{"location":"spring2021/lecture-12/#deep-semi-supervised-learning","title":"Deep Semi-Supervised Learning","text":"<p>Semi-supervised means half supervised, half unsupervised. Assuming a classification problem where each data point belongs to one of the classes, we attempt to come up with an intuition to complete the labeling for the unlabeled data points. One way to formalize this is: If anything is close to a labeled example, then it will assume that label. Thus, we can propagate the labels out from where they are given to the neighboring data points.</p> <p>How can we generalize the approach above to image classification?</p> <p></p> <p>Xie et al. (2020) proposes Noisy Student Training:</p> <ul> <li> <p>First, they train a teacher model with labeled data.</p> </li> <li> <p>Then, they infer pseudo-labels on the unlabeled data. These are not  real labels, but those that they get from using the trained  teacher model.</p> </li> <li> <p>Even though these labels are not perfect (because they train on a  small amount of labeled data), they can still see where they are  more confident about those pseudo labels and inject those into  their training set as additional labeled data.</p> </li> <li> <p>When they retrain, they use dropout, data augmentation, and  stochastic depth to inject noise into the training process.  This enables the student model to be more robust and  generalizable.</p> </li> </ul>"},{"location":"spring2021/lecture-12/#deep-unsupervised-learning","title":"Deep Unsupervised Learning","text":"<p>Deep semi-supervised learning assumes that the labels in the supervised dataset are still valid for the unsupervised dataset. There\u2019s a limit to the applicability because we assume that the unlabeled data is roughly from the same distribution as the labeled data.</p> <p></p> <p>With deep unsupervised learning, we can transfer the learning with multi-headed networks.</p> <ul> <li> <p>First, we train a neural network. Then, we have two tasks and give  the network two heads - one for task 1 and another for task 2.</p> </li> <li> <p>Most parameters live in the shared trunk of the network\u2019s body.  Thus, when you train for task 1 and task 2, most of the learnings  are shared. Only a little bit gets specialized to task 1 versus  task 2.</p> </li> </ul> <p>The key hypothesis here is that: For task 1 (which is unsupervised), if the neural network is smart enough to do things like predicting the next word in a sentence, generating realistic images, or translating images from one scale to another; then that same neural network is ready to do deep supervised learning from a very small dataset for task 2 (what we care about).</p>"},{"location":"spring2021/lecture-12/#gpt-2","title":"GPT-2","text":"<p>For instance, task 1 could be predicting the next word in a sentence, while task 2 could be predicting the sentiment in a corpus. OpenAI\u2019s GPT-2 is the landmark result for next-word prediction where deep unsupervised learning could work. The results were so realistic, and there was a lot of press coverage. OpenAI deemed it to be too dangerous to be released at the time.</p> <p></p> <p>Furthermore, GPT-2 can tackle complex common sense reasoning and question answering tasks for various benchmarks. The table below displays those benchmarks where GPT-2 was evaluated on. The details of the tasks do not really matter. What\u2019s more interesting is that: This is the first time a model, trained unsupervised on a lot of text to predict the next token and fine-tuned to specific supervised tasks, beats prior methods that might have been more specialized to each of these supervised tasks.</p> <p></p> <p>Another fascinating insight is that as we grow the number of model parameters, the performance goes up consistently. This means with unsupervised learning, we can incorporate much more data for larger models. This research funding inspired OpenAI to fundraise $1B for future projects to essentially have more compute available to train larger models because it seems like doing that will lead to better results. So far, that has been true (GPT-3 performs better than GPT-2).</p>"},{"location":"spring2021/lecture-12/#bert","title":"BERT","text":"<p>BERT is Google\u2019s approach that came out around the same time as GPT-2. While GPT-2 predicts the next word or token, BERT predicts a word or token that was removed. In this task, the neural network looks at the entire corpus as it fills things back in, which often helps in later tasks (as the neural network has already been unsupervised-train on the entire text).</p> <p></p> <p>The table below displays BERT\u2019s performance on the GLUE benchmark. The takeaway message is not so much in the details of these supervised tasks; but the fact that these tasks have a relatively small amount of labeled data compared to the unsupervised training that happens ahead of time. As BERT outperformed all SOTA methods, it revolutionized how natural language processing should be done.</p> <p></p> <p>BERT is one of the biggest updates that Google has made since RankBrain in 2015 and has proven successful in comprehending the intent of the searcher behind a search query.</p>"},{"location":"spring2021/lecture-12/#unsupervised-learning-in-vision","title":"Unsupervised Learning In Vision","text":"<p>Can we do the same thing for vision tasks? Let\u2019s explore a few of them.</p> <ul> <li> <p>Predict A Missing Patch: A patch is high-dimensional, so the  number of possibilities in that patch is very high (much larger  than the number of words in English, for instance). Therefore,  it\u2019s challenging to predict precisely and make that work as well  as in languages.</p> </li> <li> <p>Solve Jigsaw Puzzles: If the network can do this, it understands  something about images of the world. The trunk of the network  should hopefully be reusable.</p> </li> <li> <p>Predict Rotation: Here, you collect random images and predict  what degree has been rotated. Existing methods work immensely well  for such a task.</p> </li> </ul> <p></p> <p>A technique that stood out in recent times is contrastive learning, which includes two variants - SimCLR (Chen et al., 2020) and MoCo (He et al., 2019). Here\u2019s how you train your model with contrastive learning:</p> <ul> <li> <p>Imagine that you download two images of a dog and a cat from the  Internet, and you don\u2019t have labels yet.</p> </li> <li> <p>You duplicate the dog image and make two versions of it (a greyscale  version and a cropped version).</p> </li> <li> <p>For these two dog versions, the neural network should bring them  together while pushing the cat image far away.</p> </li> </ul> <p>You then fine-tune with a simple linear classifier on top of training completely unsupervised. This means that you must get the right features extracted from the images during training. The results of contrastive learning methods confirm that the higher the number of model parameters, the better the accuracy.</p>"},{"location":"spring2021/lecture-12/#2-reinforcement-learning","title":"2 - Reinforcement Learning","text":"<p>Reinforcement learning (RL) has not been practical yet but nevertheless has shown promising results. In RL, the AI is an agent, more so than just a pattern recognizer. The agent acts in an environment where it is goal-oriented. It wants to achieve something during the process, which is represented by a reward function.</p> <p></p>"},{"location":"spring2021/lecture-12/#challenges","title":"Challenges","text":"<p>Compared to unsupervised learning, RL brings about a host of additional challenges:</p> <ul> <li> <p>Credit assignment: When the RL agent sees something, it has to  take action. But it is not told whether the action was good or bad  right away.</p> </li> <li> <p>Stability: Because the RL agent learns by trial and error, it  can destabilize and make big mistakes. Thus, it needs to be clever  in updating itself not to destroy things along the way.</p> </li> <li> <p>Exploration: The RL agent has to try things that have not been  done before.</p> </li> </ul> <p>Despite these challenges, some great RL successes have happened.</p>"},{"location":"spring2021/lecture-12/#successes","title":"Successes","text":"<p>DeepMind has shown that neural networks can learn to play the Atari game back in 2013. Under the hood is the Deep Q-Network architecture, which was trained from its own trial-and-error, looking at the score in the game to internalize what actions might be good or bad.</p> <p>The game of Go was cracked by DeepMind - showing that the computer can play better than the best human player (AlphaGo, AlphaGoZero, and AlphaZero).</p> <p>RL also works for the robot locomotion task. You don\u2019t have to design the controller yourself. You just implement the RL algorithm (TRPO, GAE, DDPG, PPO, and more) and let the agent train itself, which is a general approach to have AI systems acquire new skills. In fact, the robot can acquire such a variety of skills, as demonstrated in this DeepMimic work.</p> <p></p> <p>You can also accomplish the above for non-human-like characters in dynamic animation tasks. This is going to change how you can design video games or animated movies. Instead of designing the keyframes for every step along the way in your video or your game, you can train an agent to go from point A to point B directly.</p> <p>RL has been shown to work on real robots.</p> <ul> <li> <p>BRETT (Berkeley  Robot for the Elimination of Tedious Tasks) could learn to put  blocks into matching openings in under an hour using a neural  network trained from scratch. This technique has been used for  NASA SuperBall  robots for space exploration ideas.</p> </li> <li> <p>A similar idea was applied to robotic manipulation solving  Rubik\u2019s cube,  done at OpenAI in 2019. The in-hand manipulation is a very  difficult robotic control problem that was mastered with RL.</p> </li> </ul>"},{"location":"spring2021/lecture-12/#covariantai","title":"CovariantAI","text":"<p>The fact that RL worked so well actually inspired Pieter and his former students (Tianhao Zhang, Rocky Duan, and Peter Chen) to start a company called Covariant in 2017. Their goal is to bring these advances from the lab into the real world. An example is autonomous order picking.</p>"},{"location":"spring2021/lecture-12/#3-unsupervised-reinforcement-learning","title":"3 - Unsupervised Reinforcement Learning","text":"<p>RL achieved mastery on many simulated domains. But we must ask the question: How fast is the learning itself? Tsividis et al., 2017 shows that a human can learn in about 15 minutes to perform better than Double DQN (a SOTA approach at the time of the study) learned after 115 hours.</p> <p>How can we bridge this learning gap?</p> <p>Based on the 2018 DeepMind Control Suite, pixel-based learning needs 50M more training steps than state-based learning to solve the same tasks. Maybe we can develop an unsupervised learning approach to turn pixel-level representations (which are not that informative) into a new representation that is much more similar to the underlying state.</p> <p></p> <p>CURL brings together contrastive learning and RL.</p> <ul> <li> <p>In RL, there\u2019s typically a replay buffer where we store the past  experiences. We load observations from there and feed them into an  encoder neural network. The network has two heads: an actor to  estimate the best action to take next and a critic to estimate how  good that action would be.</p> </li> <li> <p>CURL adds an extra head at the bottom, which includes augmented  observations, and does contrastive learning on that. Similar  configurations of the robot are brought closer together, while  different ones are separated.</p> </li> </ul> <p>The results confirm that CURL can match existing SOTA approaches that learn from states and from pixels. However, it struggles in hard environments, with insufficient labeled images being the root cause.</p>"},{"location":"spring2021/lecture-12/#4-meta-reinforcement-learning","title":"4 - Meta Reinforcement Learning","text":"<p>The majority of fully general RL algorithms work well for any environments that can be mathematically defined. However, environments encountered in the real world are a tiny subset of all environments that could be defined. Maybe the learning takes such a long time because the algorithms are too general. If they are a bit more specialized in things they will encounter, perhaps the learning is faster.</p> <p>Can we develop a fast RL algorithm to take advantage of this?</p> <p>In traditional RL research, human experts develop the RL algorithm. However, there are still no RL algorithms nearly as good as humans after many years. Can we learn a better RL algorithm? Or even learn a better entire agent?</p>"},{"location":"spring2021/lecture-12/#rl2","title":"RL^2","text":"<p>RL^2 (Duan et al., 2016) is a meta-RL framework proposed to tackle this issue:</p> <ul> <li> <p>Imagine that we have multiple meta-training environments (A, B, and  so on).</p> </li> <li> <p>We also have a meta-RL algorithm that learns the RL algorithm and  outputs a \u201cfast\u201d RL agent (from having interacted with these  environments).</p> </li> <li> <p>In the future, our agent will be in an environment F that is related  to A, B, and so on.</p> </li> </ul> <p>Formally speaking, RL^2 maximizes the expected reward on the training Markov Decision Process (MDP) but can generalize to testing MDP. The RL agent is represented as a Recurrent Neural Network (RNN), a generic computation architecture where:</p> <ul> <li> <p>Different weights in the RNN mean different RL algorithms and  priors.</p> </li> <li> <p>Different activations in the RNN mean different current policies.</p> </li> <li> <p>The meta-trained objective can be optimized with an existing \u201cslow\u201d  RL algorithm.</p> </li> <li> <p>The resulting RNN is ready to be dropped in a new environment.</p> </li> </ul> <p>RL^2 was evaluated on a classic Multi-Armed Bandit setting and performed better than provably (asymptotically) optimal RL algorithms invented by humans like Gittings Index, UCB1, and Thompson Sampling. Another task that RL^2 was evaluated on is visual navigation, where the agent explores a maze and finds a specified target as quickly as possible. Although this setting is maze-specific, we can scale up RL^2 to other large-scale games and robotic environments and use it to learn in a new environment quickly.</p>"},{"location":"spring2021/lecture-12/#learn-more","title":"Learn More","text":"<ul> <li> <p>Schmidhuber. Evolutionary principles in self-referential  learning. (1987)</p> </li> <li> <p>Wiering, Schmidhuber. Solving POMDPs with Levin search and  EIRA. (1996)</p> </li> <li> <p>Schmidhuber, Zhao, Wiering. Shifting inductive bias with  success-story algorithm, adaptive Levin search, and incremental  self-improvement.  (MLJ 1997)</p> </li> <li> <p>Schmidhuber, Zhao, Schraudolph. Reinforcement learning with  self-modifying  policies (1998)</p> </li> <li> <p>Zhao, Schmidhuber. Solving a complex prisoner\u2019s dilemma with  self-modifying  policies. (1998)</p> </li> <li> <p>Schmidhuber. A general method for incremental self-improvement  and multiagent  learning. (1999)</p> </li> <li> <p>Singh, Lewis, Barto. Where do rewards come  from? (2009)</p> </li> <li> <p>Singh, Lewis, Barto. Intrinsically Motivated Reinforcement  Learning: An Evolutionary  Perspective (2010)</p> </li> <li> <p>Niekum, Spector, Barto. Evolution of reward functions for  reinforcement  learning (2011)</p> </li> <li> <p>Wang et al., (2016). Learning to Reinforcement  Learn</p> </li> <li> <p>Finn et al., (2017). Model-Agnostic  Meta-Learning  (MAML)</p> </li> <li> <p>Mishra, Rohinenjad et al., (2017). Simple Neural AttentIve  Meta-Learner</p> </li> <li> <p>Frans et al., (2017). Meta-Learning Shared  Hierarchies</p> </li> </ul>"},{"location":"spring2021/lecture-12/#5-few-shot-imitation-learning","title":"5 - Few-Shot Imitation Learning","text":"<p>People often complement RL with imitation learning, which is basically supervised learning where the output is an action for an agent. This gives you more signal than traditional RL since for every input, you consistently have a corresponding output. As the diagram below shows, the imitation learning algorithm learns a policy in a supervised manner from many demonstrations and outputs the correct action based on the environment.</p> <p></p> <p>The challenge for imitation learning is to collect enough demonstrations to train an algorithm, which is time-consuming. To make the collection of demonstrations more efficient, we can apply multi-task meta-learning. Many demonstrations for different tasks can be learned by an algorithm, whose output is fed to a one-shot imitator that picks the correct action based on a single demonstration. This process is referred to as one-shot imitation learning (Duan et al., 2017), as displayed below.</p> <p></p> <p>Conveniently, one-shot imitators are trained using traditional network architectures. A combination of CNNs, RNNs, and MLPs perform the heavy visual processing to understand the relevant actions in training demos and recommend the right action for the current frame of an inference demo. One example of this in action is block stacking.</p> <p></p>"},{"location":"spring2021/lecture-12/#learn-more_1","title":"Learn More","text":"<ul> <li> <p>Abbeel et al., (2008). Learning For Control From Multiple  Demonstrations</p> </li> <li> <p>Kolter, Ng. The Stanford LittleDog: A Learning And Rapid  Replanning Approach To Quadrupled  Locomotion (2008)</p> </li> <li> <p>Ziebart et al., (2008). Maximum Entropy Inverse Reinforcement  Learning</p> </li> <li> <p>Schulman et al., (2013). Motion Planning with Sequential Convex  Optimization and Convex Collision  Checking</p> </li> <li> <p>Finn, Levine. Deep Visual Foresight for Planning Robot  Motion (2016)</p> </li> </ul>"},{"location":"spring2021/lecture-12/#6-domain-randomization","title":"6 - Domain Randomization","text":"<p>Simulated data collection is a logical substitute for expensive real data collection. It is less expensive, more scalable, and less dangerous (e.g., in the case of robots) to capture at scale. Given this logic, how can we make sure simulated data best matches real-world conditions?</p>"},{"location":"spring2021/lecture-12/#use-realistic-simulated-data","title":"Use Realistic Simulated Data","text":"<p>One approach is to make the simulator you use for training models as realistic as possible. Two variants of doing this are to carefully match the simulation to the world (James and John, 2016; Johns, Leutenegger, and Division, 2016; Mahler et al., 2017; Koenemann et al., 2015) and augment simulated data with real data (Richter et al., 2016; Bousmalis et al., 2017). While this option is logically appealing, it can be hard and slow to do in practice.</p>"},{"location":"spring2021/lecture-12/#domain-confusion","title":"Domain Confusion","text":"<p>Another option is domain confusion (Tzeng et al., 2014; Rusu et al., 2016).</p> <ul> <li> <p>In this approach, suppose you train a model on real and simulated  data at the same time.</p> </li> <li> <p>After completing training, a discriminator network examines the  original network at some layer to understand if the original  network is learning something about the real world.</p> </li> <li> <p>If you can fool the discriminator with the output of the layer, the  original network has completely integrated its understanding of  real and simulated data.</p> </li> <li> <p>In effect, there is no difference between simulated and real data to  the original network, and the layers following the examined layer  can be trained fully on simulated data.</p> </li> </ul>"},{"location":"spring2021/lecture-12/#domain-randomization","title":"Domain Randomization","text":"<p>Finally, a simpler approach called domain randomization (Tobin et al., 2017; Sadeghi and Levine, 2016) has taken off of late. In this approach, rather than making simulated data fully realistic, the priority is to generate as much variation in the simulated data as possible. For example, in the below tabletop scenes, the dramatic variety of the scenes (e.g., background colors of green and purple) can help the model generalize well to the real world, even though the real world looks nothing like these scenes. This approach has shown promise in drone flight and pose estimation. The simple logic of more data leading to better performance in real-world settings is powerfully illustrated by domain randomization and obviates the need for existing variation methods like pre-training on ImageNet.</p>"},{"location":"spring2021/lecture-12/#7-deep-learning-for-science-and-engineering","title":"7 - Deep Learning For Science and Engineering","text":""},{"location":"spring2021/lecture-12/#alphafold","title":"AlphaFold","text":"<p>In other areas of this lecture, we\u2019ve been focusing on research areas of machine learning where humans already perform well (i.e., pose estimation or grasping). In science and engineering applications, we enter the realm of machine learning performing tasks humans cannot. The most famous result is AlphaFold, a Deepmind-created system that solved protein folding, an important biological challenge. In the CASP challenge, AlphaFold 2 far outpaced all other results in performance. AlphaFold is quite complicated, as it maps an input protein sequence to similar protein sequences and subsequently decides the folding structure based on the evolutionary history of complementary amino acids.</p> <p></p> <p>Other examples of DL systems solving science and engineering challenges are in circuit design, high-energy physics, and symbolic mathematics.</p>"},{"location":"spring2021/lecture-12/#learn-more_2","title":"Learn More","text":"<ul> <li> <p>AlphaFold: Improved protein structure prediction using  potentials from deep  learning.  Deepmind (Senior et al.)</p> </li> <li> <p>BagNet: Berkeley Analog Generator with Layout Optimizer Boosted  with Deep Neural  Networks. K.  Hakhamaneshi, N. Werblun, P. Abbeel, V. Stojanovic. IEEE/ACM  International Conference on Computer-Aided Design (ICAD),  Westminster, Colorado, November 2019.</p> </li> <li> <p>Evaluating Protein Transfer Learning with  TAPE. R.  Rao, N. Bhattacharya, N. Thomas, Y, Duan, X. Chen, J. Canny, P.  Abbeel, Y. Song.</p> </li> <li> <p>Opening the black box: the anatomy of a deep learning atomistic  potential.  Justin Smith</p> </li> <li> <p>Exploring Machine Learning Applications to Enable  Next-Generation  Chemistry.  Jennifer Wei (Google).</p> </li> <li> <p>GANs for  HEP.  Ben Nachman</p> </li> <li> <p>Deep Learning for Symbolic  Mathematics. G.  Lample and F. Charton.</p> </li> <li> <p>A Survey of Deep Learning for Scientific  Discovery. Maithra Raghu,  Eric Schmidt.</p> </li> </ul>"},{"location":"spring2021/lecture-12/#8-overarching-research-theme","title":"8 - Overarching Research Theme","text":"<p>As compute scales to support incredible numbers of FLOPs, more science and engineering challenges will be solved with deep learning systems. There has been exponential growth in the amount of compute used to generate the most impressive research results like GPT-3.</p> <p></p> <p>As compute and data become more available, we open a new problem territory that we can refer to as deep learning to learn. More specifically, throughout history, the constraint on solving problems has been human ingenuity. This is a particularly challenging realm to contribute novel results to because we\u2019re competing against the combined intellectual might available throughout history. Is our present ingenuity truly greater than that of others 20-30 years ago, let alone 200-300? Probably not. However, our ability to bring new tools like compute and data most certainly is. Therefore, spending as much time in this new problem territory, where data and compute help solve problems, is likely to generate exciting and novel results more frequently in the long run.</p> <p></p>"},{"location":"spring2021/lecture-12/#9-how-to-keep-up","title":"9 - How To Keep Up","text":"<p>\u201cGive a man a fish and you feed him for a day, teach a man to fish and you feed him for a lifetime\u201d (Lao Tzu)</p> <p>Here are some tips on how to keep up with ML research:</p> <ul> <li> <p>(Mostly) don\u2019t read (most) papers. There are just too many!</p> </li> <li> <p>When you do want to keep up, use the following:</p> <ul> <li> <p>Tutorials at conferences: these capture the essence of important  concepts in a practical, distilled way</p> </li> <li> <p>Graduate courses and seminars</p> </li> <li> <p>Yannic Kilcher YouTube  channel</p> </li> <li> <p>Two Minutes Paper  Channel</p> </li> <li> <p>The Batch by Andrew  Ng</p> </li> <li> <p>Import AI by Jack Clark</p> </li> </ul> </li> <li> <p>If you DO decide to read papers,</p> <ul> <li> <p>Follow a principled process for reading papers</p> </li> <li> <p>Use Arxiv Sanity</p> </li> <li> <p>Twitter</p> </li> <li> <p>AI/DL Facebook Group</p> </li> <li> <p>ML Subreddit</p> </li> <li> <p>Start a reading group: read papers together with friends -  either everyone reads then discusses, or one or two people  read and give tutorials to others.</p> </li> </ul> </li> </ul> <p></p> <p>Finally, should you do a Ph.D. or not?</p> <ul> <li> <p>You don\u2019t have to do a Ph.D. to work in AI!</p> </li> <li> <p>However, if you REALLY want to become one of the world\u2019s experts in  a topic you care about, then a Ph.D. is a technically deep and  demanding path to get there. Crudely speaking, a Ph.D. enables you  to develop new tools and techniques rather than using existing  tools and techniques.</p> </li> </ul>"},{"location":"spring2021/lecture-13/","title":"Lecture 13: ML Teams and Startups","text":""},{"location":"spring2021/lecture-13/#video","title":"Video","text":""},{"location":"spring2021/lecture-13/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-13/#notes","title":"Notes","text":"<p>Download notes as PDF</p> <p>Lecture by Josh Tobin. Notes transcribed by James Le and Vishnu Rachakonda.</p> <p>Over the past few years, machine learning (ML) has grown tremendously. But as young as ML is as a discipline, the craft of managing an ML team is even younger. Many of today\u2019s ML managers were thrust into management roles out of necessity or because they were the best individual contributors, and many come from purely academic backgrounds. At some companies, engineering or product leaders are tasked with building new ML functions without real ML experience.</p> <p>Running any technical team is hard:</p> <ul> <li> <p>You have to hire great people.</p> </li> <li> <p>You need to manage and develop them.</p> </li> <li> <p>You need to manage your team\u2019s output and make sure your vectors are aligned.</p> </li> <li> <p>You would want to make good long-term technical choices and manage technical debt.</p> </li> <li> <p>You also must manage expectations from leadership.</p> </li> </ul> <p>Running an ML team is even harder:</p> <ul> <li> <p>ML talents are expensive and scarce.</p> </li> <li> <p>ML teams have a diverse set of roles.</p> </li> <li> <p>ML projects have unclear timelines and high uncertainty.</p> </li> <li> <p>ML is also the \u201chigh-interest credit card of technical debt.\"</p> </li> <li> <p>Leadership often doesn\u2019t understand ML.</p> </li> </ul> <p>The goals of this lecture are two-fold: (1) to give you insight into how to think about building and managing ML teams (as a leader); and (2) to help you get a job in ML (as a newcomer).</p> <p></p>"},{"location":"spring2021/lecture-13/#1-ml-roles","title":"1 - ML Roles","text":""},{"location":"spring2021/lecture-13/#common-roles","title":"Common Roles","text":"<p>Let\u2019s take a look at the most common ML roles and the skills they require:</p> <ul> <li> <p>The ML Product Manager works with the ML team, other business functions, the end-users, and the data owners. This person designs documentation, creates wireframes, and comes up with the plan to prioritize and execute ML projects.</p> </li> <li> <p>The DevOps Engineer deploys and monitors production systems. This person handles the infrastructure that runs the deployed ML product using platforms like AWS or GCP.</p> </li> <li> <p>The Data Engineer builds data pipelines, aggregates and collects data from storage, and monitors data behavior. This person works with distributed systems using tools such as Hadoop, Kafka, Airflow.</p> </li> <li> <p>The ML Engineer trains and deploys prediction models. This person uses tools like TensorFlow and Docker to work with prediction systems running on real data in production.</p> </li> <li> <p>The ML Researcher trains prediction models, often those that are forward-looking or not production-critical. This person uses libraries like TensorFlow and PyTorch on notebook environments to build models and reports describing their experiments.</p> </li> <li> <p>The Data Scientist is a blanket term used to describe all of the roles above. In some organizations, this role entails answering business questions via analytics. He/she can work with wide-ranging tools from SQL and Excel to Pandas and Scikit-Learn.</p> </li> </ul>"},{"location":"spring2021/lecture-13/#skills-required","title":"Skills Required","text":"<p>So what skills are needed for these roles? The chart above displays a nice visual, where the horizontal axis is the level of ML expertise and the size of the bubble is the level of communication and technical writing (the bigger, the better).</p> <ul> <li> <p>The ML DevOps is primarily a software engineering role, which often comes from a standard software engineering pipeline.</p> </li> <li> <p>The Data Engineer belongs to the software engineering team that works actively with ML teams.</p> </li> <li> <p>The ML Engineer requires a rare mix of ML and Software Engineering skills. This person is either an engineer with significant self-teaching OR a science/engineering Ph.D. who works as a traditional software engineer after graduate school.</p> </li> <li> <p>The ML Researcher is an ML expert who usually has an MS or Ph.D. degree in Computer Science or Statistics or finishes an industrial fellowship program.</p> </li> <li> <p>The ML Product Manager is just like a traditional Product Manager, but with a deep knowledge of the ML development process and mindset.</p> </li> <li> <p>The Data Scientist role constitutes a wide range of backgrounds from undergraduate to Ph.D. students.</p> </li> </ul>"},{"location":"spring2021/lecture-13/#2-ml-organizations","title":"2 - ML Organizations","text":""},{"location":"spring2021/lecture-13/#organization-archetypes","title":"Organization Archetypes","text":"<p>There exists not yet a consensus on the right way to structure an ML team. Still, a few best practices are contingent upon different organization archetypes and their ML maturity level. First, let\u2019s see what the different ML organization archetypes are.</p>"},{"location":"spring2021/lecture-13/#archetype-1-nascent-and-ad-hoc-ml","title":"Archetype 1 - Nascent and Ad-hoc ML","text":"<ul> <li> <p>These are organizations where no one is doing ML, or ML is done on an ad-hoc basis. Obviously, there is little ML expertise in-house.</p> </li> <li> <p>They are either small-to-medium businesses or less technology-forward large companies in industries like education or logistics.</p> </li> <li> <p>There is often low-hanging fruit for ML.</p> </li> <li> <p>But there is little support for ML projects, and it\u2019s challenging to hire and retain good talent.</p> </li> </ul>"},{"location":"spring2021/lecture-13/#archetype-2-research-and-development-ml","title":"Archetype 2 - Research and Development ML","text":"<ul> <li> <p>These are organizations in which ML efforts are centered in the R&amp;D arm of the organization. They often hire ML researchers and doctorate students with experience publishing papers.</p> </li> <li> <p>They are larger companies in sectors such as oil and gas, manufacturing, or telecommunications.</p> </li> <li> <p>They can hire experienced researchers and work on long-term business priorities to get big wins.</p> </li> <li> <p>However, it is very difficult to get quality data. Most often, this type of research work rarely translates into actual business value, so usually, the amount of investment remains small.</p> </li> </ul>"},{"location":"spring2021/lecture-13/#archetype-3-product-embedded-ml","title":"Archetype 3 - Product-Embedded ML","text":"<ul> <li> <p>These are organizations where certain product teams or business units have ML expertise alongside their software or analytics talent. These ML individuals report up to the team\u2019s engineering/tech lead.</p> </li> <li> <p>They are either software companies or financial services companies.</p> </li> <li> <p>ML improvements are likely to lead to business value. Furthermore, there is a tight feedback cycle between idea iteration and product improvement.</p> </li> <li> <p>Unfortunately, it is still very hard to hire and develop top talent, and access to data and compute resources can lag. There are also potential conflicts between ML project cycles and engineering management, so long-term ML projects can be hard to justify.</p> </li> </ul>"},{"location":"spring2021/lecture-13/#archetype-4-independent-ml-division","title":"Archetype 4 - Independent ML Division","text":"<ul> <li> <p>These are organizations in which the ML division reports directly to senior leadership. The ML Product Managers work with Researchers and Engineers to build ML into client-facing products. They can sometimes publish long-term research.</p> </li> <li> <p>They are often large financial services companies.</p> </li> <li> <p>Talent density allows them to hire and train top practitioners. Senior leaders can marshal data and compute resources. This gives the organizations to invest in tooling, practices, and culture around ML development.</p> </li> <li> <p>A disadvantage is that model handoffs to different business lines can be challenging since users need the buy-in to ML benefits and get educated on the model use. Also, feedback cycles can be slow.</p> </li> </ul>"},{"location":"spring2021/lecture-13/#archetype-5-ml-first","title":"Archetype 5 - ML-First","text":"<ul> <li> <p>These are organizations in which the CEO invests in ML, and there are experts across the business focusing on quick wins. The ML division works on challenging and long-term projects.</p> </li> <li> <p>They are large tech companies and ML-focused startups.</p> </li> <li> <p>They have the best data access (data thinking permeates the organization), the most attractive recruiting funnel (challenging ML problems tends to attract top talent), and the easiest deployment procedure (product teams understand ML well enough).</p> </li> <li> <p>This type of organization archetype is hard to implement in practice since it is culturally difficult to embed ML thinking everywhere.</p> </li> </ul>"},{"location":"spring2021/lecture-13/#team-structure-design-choices","title":"Team Structure Design Choices","text":"<p>Depending on the above archetype that your organization resembles, you can make the appropriate design choices, which broadly speaking follow these three categories:</p> <ol> <li> <p>Software Engineer vs. Research: To what extent is the ML team responsible for building or integrating with software? How important are Software Engineering skills on the team?</p> </li> <li> <p>Data Ownership: How much control does the ML team have over data collection, warehousing, labeling, and pipelining?</p> </li> <li> <p>Model Ownership: Is the ML team responsible for deploying models into production? Who maintains the deployed models?</p> </li> </ol> <p>Below are our design suggestions:</p> <p>If your organization focuses on ML R&amp;D:</p> <ul> <li> <p>Research is most definitely prioritized over Software Engineering skills. Because of this, there would potentially be a lack of collaboration between these two groups.</p> </li> <li> <p>ML team has no control over the data and typically will not have data engineers to support them.</p> </li> <li> <p>ML models are rarely deployed into production.</p> </li> </ul> <p>If your organization has ML embedded into the product:</p> <ul> <li> <p>Software Engineering skills will be prioritized over Research skills. Often, the researchers would need strong engineering skills since everyone would be expected to product-ionize his/her models.</p> </li> <li> <p>ML teams generally do not own data production and data management. They will need to work with data engineers to build data pipelines.</p> </li> <li> <p>ML engineers totally own the models that they deploy into production.</p> </li> </ul> <p>If your organization has an independent ML division:</p> <ul> <li> <p>Each team has a potent mix of engineering and research skills; therefore, they work closely together within teams.</p> </li> <li> <p>ML team has a voice in data governance discussions, as well as a robust data engineering function.</p> </li> <li> <p>ML team hands-off models to users but is still responsible for maintaining them.</p> </li> </ul> <p>If your organization is ML-First:</p> <ul> <li> <p>Different teams are more or less research-oriented, but in general, research teams collaborate closely with engineering teams.</p> </li> <li> <p>ML team often owns the company-wide data infrastructure.</p> </li> <li> <p>ML team hands the models to users, who are responsible for operating and maintaining them.</p> </li> </ul> <p>The picture below neatly sums up these suggestions:</p> <p></p>"},{"location":"spring2021/lecture-13/#3-managing-ml-teams","title":"3 - Managing ML Teams","text":""},{"location":"spring2021/lecture-13/#managing-ml-teams-is-challenging","title":"Managing ML Teams Is Challenging","text":"<p>The process of actually managing an ML team is quite challenging for four reasons:</p> <ol> <li> <p>Engineering Estimation: It\u2019s hard to know how easy or hard an ML project is in advance. As you explore the data and experiment with different models, there is enormous scope for new learnings about the problem that materially impact the timeline. Furthermore, knowing what methods will work is often impossible. This makes it hard to say upfront how long or how much work may go into an ML project.</p> </li> <li> <p>Nonlinear Progress: As the chart below from a blog post by Lukas Biewald (CEO of Weights and Biases) shows, progress on ML projects is unpredictable over time, even when the effort expended grows considerably. It\u2019s very common for projects to stall for extended periods of time.</p> </li> </ol> <p></p> <ol> <li> <p>Cultural gaps: The relative culture of engineering and research professionals is very different. Research tends to favor novel, creative ideas, while engineering prefers tried and true methods that work. As a result, ML teams often experience a clash of cultures, which can turn toxic if not appropriately managed. A core challenge of running ML teams is addressing the cultural barriers between ML and software engineering so that teams can harmoniously experiment and deliver ML products.</p> </li> <li> <p>Leadership Deficits: It\u2019s common to see a lack of detailed understanding of ML at senior levels of management in many companies. As a result, expressing feasibility and setting the right expectations for ML projects, especially high-priority ones, can be hard.</p> </li> </ol>"},{"location":"spring2021/lecture-13/#how-to-manage-ml-teams-better","title":"How To Manage ML Teams Better","text":"<p>Managing ML teams is hardly a solved problem, but you can take steps to improve the process.</p>"},{"location":"spring2021/lecture-13/#plan-probabilistically","title":"Plan Probabilistically","text":"<p>Many engineering projects are managed in a waterfall fashion, with the sequential tasks defined up front clearly. Instead of forcing this method of engineering management on difficult ML projects, try assigning a likelihood of success to different tasks to better capture the experimental process inherent to ML engineering. As these tasks progress or stall, rapidly re-evaluate your task ordering to better match what is working. Having this sense of both (1) how likely a task is to succeed and (2) how important it is makes project planning considerably more realistic.</p> <p></p>"},{"location":"spring2021/lecture-13/#have-a-portfolio-of-approaches","title":"Have A Portfolio Of Approaches","text":"<p>Embrace multiple ideas and approaches to solve crucial research challenges that gate production ML. Don\u2019t make your plan dependent on one approach working!</p>"},{"location":"spring2021/lecture-13/#measure-inputs-not-results","title":"Measure Inputs, Not Results","text":"<p>As you work through several approaches in your portfolio, do not overly emphasize whose ideas ultimately work as a reflection of contribution quality. This can negatively impact team members\u2019 creativity, as they focus more on trying to find only what they currently think could work, rather than experimenting in a high-quality fashion (which is ultimately what leads to ML success).</p>"},{"location":"spring2021/lecture-13/#have-researchers-and-engineers-work-together","title":"Have Researchers and Engineers Work Together","text":"<p>The collaboration between engineering and research is essential for quality ML products to get into production. Emphasize collaboration across the groups and professionals!</p>"},{"location":"spring2021/lecture-13/#get-end-to-end-pipelines-together-quickly-to-demonstrate-quick-wins","title":"Get End-to-end Pipelines Together Quickly to Demonstrate Quick Wins","text":"<p>Taking this approach makes it more likely that your ML project will succeed in the long term. It allows you to demonstrate progress to your leadership more effectively and clearly.</p>"},{"location":"spring2021/lecture-13/#educate-leadership-on-ml-timeline-uncertainty","title":"Educate Leadership on ML Timeline Uncertainty","text":"<p>This can be hard, as leadership is ultimately accountable for addressing blind spots and understanding timeline risk. There are things you can do, however, to help improve leadership\u2019s knowledge about ML timelines. Avoid building hype around narrow progress metrics material only to the ML team (e.g., \u201cWe improved F1 score by 0.2 and have achieved awesome performance!\u201d). Instead, be realistic, communicate risk, and emphasize real product impact (e.g., \u201cOur model improvements should increase the number of conversions by 10%, though we must continue to validate its performance on additional demographic factors.) Sharing resources like this a16z primer and this class from Prof. Pieter Abbeel can increase awareness of your company\u2019s leadership.</p>"},{"location":"spring2021/lecture-13/#4-hiringgetting-hired","title":"4 - Hiring/Getting Hired","text":""},{"location":"spring2021/lecture-13/#the-ai-talent-gap","title":"The AI Talent Gap","text":"<p>With the novelty of ML systems, it\u2019s fair to say that not many people have built real ML systems. Estimates vary from as few as 10,000 (Element AI) to as many as 200-300,000 people (Tencent). Whatever way you slice the numbers (contained in this blog post), the reality is that there is not much-experienced talent in the AI/ML field, especially compared to the number of trained software developers in the US (3.6M) or in the world (18.2M).</p>"},{"location":"spring2021/lecture-13/#sourcing","title":"Sourcing","text":"<p>Because of this shallow talent pool and the skyrocketing demand, hiring for ML positions is pretty hard. Typical ML roles come in the following structure:</p> <ul> <li> <p>ML Adjacent roles: ML product manager, DevOps, Data Engineer</p> </li> <li> <p>Core ML Roles: ML Engineer, ML Research/ML Scientist</p> </li> <li> <p>Business analytics roles: Data Scientist</p> </li> </ul> <p>For ML adjacent roles, traditional ML knowledge is less important, as demonstrated interest, conversational understanding, and experience can help these professionals play an impactful role on ML teams. Let\u2019s focus on how to hire for the core ML roles.</p> <p>While there\u2019s no perfect way to hire ML engineers, there\u2019s definitely a wrong way to hire them, with extensive job descriptions that demand only the best qualifications. Certainly, there are many good examples of this bad practice floating around.</p> <ul> <li> <p>Rather than this unrealistic process, consider hiring for software engineering skills, an interest in ML, and a desire to learn. You can always train people in the art and science of ML, especially when they come with strong software engineering fundamentals.</p> </li> <li> <p>Another option is to consider adding junior talent, as many recent grads come out with good ML knowledge nowadays.</p> </li> <li> <p>Finally, and most importantly, be more specific about what you need the position and professional to do. It\u2019s impossible to find one person that can do everything from full-fledged DevOps to algorithm development.</p> </li> </ul> <p>To hire ML researchers, here are our tips:</p> <ul> <li> <p>Evaluate the quality of publications, over the quantity, with an eye towards the originality of the ideas, the execution, etc.</p> </li> <li> <p>Prioritize researchers that focus on important problems instead of trendy problems.</p> </li> <li> <p>Experience outside academia is also a positive, as these researchers may be able to transition to industry more effectively.</p> </li> <li> <p>Finally, keep an open mind about research talent and consider talented people without PhDs or from adjacent fields like physics, statistics, etc.</p> </li> </ul> <p>To find quality candidates for these roles, some ideas for sourcing are:</p> <ul> <li> <p>To experiment with standard job recruiting avenues like LinkedIn, Hired, recruiters, on-campus-recruiting, etc.</p> </li> <li> <p>To monitor arXiv and top conferences and flag first authors of papers you like.</p> </li> <li> <p>To look for good implementations of papers you like.</p> </li> <li> <p>To attend ML research conferences (NeurIPS, ICML, etc.)</p> </li> </ul> <p>As you seek to recruit, stay on top of what professionals want and make an effort to position your company accordingly. ML practitioners want to be empowered to do great work with interesting data. Building a culture of learning and impact can help recruit the best talent to your team. Additionally, sell sell sell! Talent needs to know how good your team is and how meaningful the mission can be.</p> <p></p>"},{"location":"spring2021/lecture-13/#interviewing","title":"Interviewing","text":"<p>As you interview candidates for ML roles, try to validate your hypotheses of their strengths while testing a minimum bar on weaker aspects. For example, make sure ML researchers can think creatively about new ML problems while ensuring they meet a baseline for code quality. It\u2019s essential to test both ML knowledge and software engineering skill for all industry professionals, though the relative strengths can vary.</p> <p>The actual ML interview process is much less well-defined than software engineering interviews, though it is modeled off of it. Some helpful inclusions are projects or exercises that test the ability to work with ML-specific code, like take-home ML projects.</p> <p></p>"},{"location":"spring2021/lecture-13/#finding-a-job","title":"Finding A Job","text":"<p>To find an ML job, you can take a look at the following sources:</p> <ul> <li> <p>Standard sources such as LinkedIn, recruiters, on-campus recruiting, etc.</p> </li> <li> <p>ML research conferences (NeurIPS, ICLR, ICML).</p> </li> <li> <p>Apply directly (remember, there\u2019s a talent gap!).</p> </li> </ul> <p>Standing out for competitive roles can be tricky! Here are some tips in increasing order of impressiveness that you can apply to differentiate yourself:</p> <ol> <li> <p>Build software engineering skills (e.g., at a well-known software company).</p> </li> <li> <p>Exhibit ML interest (e.g., conference attendance, online courses certificates, etc.).</p> </li> <li> <p>Show you have a broad knowledge of ML (e.g., write blog posts synthesizing a research area).</p> </li> <li> <p>Demonstrate ability to get ML projects done (e.g., create side projects, re-implement papers).</p> </li> <li> <p>Prove you can think creatively in ML (e.g., win Kaggle competitions, publish papers).</p> </li> </ol> <p>As you prepare for interviews, prepare for both the traditional ML theoretical topics and the general software engineering interview (e.g., read Cracking the Coding Interview).</p>"},{"location":"spring2021/lecture-13/#5-conclusion","title":"5 - Conclusion","text":"<p>Being a new and evolving discipline for most traditional organizations, forming ML teams is full of known and unknown challenges. Here are the final few take-homes:</p> <ul> <li> <p>There are many different skills involved in production ML, so there are opportunities for many people to contribute.</p> </li> <li> <p>ML teams are becoming more standalone and more interdisciplinary.</p> </li> <li> <p>Managing ML teams is complex. There is no silver bullet, but shifting toward probabilistic planning can help.</p> </li> <li> <p>ML talent is scarce. As a manager, be specific about what skills are must-have in the ML job descriptions. As a job seeker, it can be brutally challenging to break in as an outsider, so use projects as a signal to build awareness.</p> </li> </ul>"},{"location":"spring2021/lecture-2a/","title":"Lecture 2A: CNNs","text":""},{"location":"spring2021/lecture-2a/#video","title":"Video","text":""},{"location":"spring2021/lecture-2a/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-2a/#notes","title":"Notes","text":"<p>Lecture by Sergey Karayev.</p> <p>In this video, we first review convolution operation, the most basic property of Convolutional Neural Networks. Then, we look at other important operations for ConvNets. Finally, we transition to looking at a classic ConvNet architecture called LeNet.</p> <ul> <li>00:00 - Introduction</li> <li>01:08 - Convolutional Filters</li> <li>07:10 - Filter Stacks and ConvNets</li> <li>11:25 - Strides and Padding</li> <li>14:35 - Filter Math</li> <li>21:44 - Convolution Implementation Notes</li> <li>24:04 - Increasing the Receptive Field with Dilated Convolutions</li> <li>27:30 - Decreasing the Tensor Size with Pooling and 1x1-Convolutions</li> <li>30:54 - LeNet Architecture</li> </ul>"},{"location":"spring2021/lecture-2b/","title":"Lecture 2B: Computer Vision","text":""},{"location":"spring2021/lecture-2b/#video","title":"Video","text":""},{"location":"spring2021/lecture-2b/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-2b/#notes","title":"Notes","text":"<p>Lecture by Sergey Karayev.</p> <p>In this video, we will review notable applications of deep learning in computer vision. First, we will tour some ConvNet architectures. Then, we will talk about localization, detection, and segmentation problems. We will conclude with more advanced methods.</p> <p>Learn more at this website: https://paperswithcode.com/area/computer-vision</p> <ul> <li>00:00 - Introduction</li> <li>02:51 - AlexNet</li> <li>05:09 - ZFNet</li> <li>06:54 - VGGNet</li> <li>09:06 - GoogLeNet</li> <li>11:57 - ResNet</li> <li>15:15 - SqueezeNet</li> <li>17:05 - Architecture Comparisons</li> <li>20:00 - Localization, Detection, and Segmentation Tasks</li> <li>24:00 - Overfeat, YOLO, and SSD Methods</li> <li>28:01 - Region Proposal Methods (R-CNN, Faster R-CNN, Mask R-CNN, U-Net)</li> <li>34:33 - Advanced Tasks (3D Shape Inference, Face Landmark Recognition, and Pose Estimation)</li> <li>37:00 - Adversarial Attacks</li> <li>40:56 - Style Transfer</li> </ul>"},{"location":"spring2021/lecture-3/","title":"Lecture 3: RNNs","text":""},{"location":"spring2021/lecture-3/#video","title":"Video","text":""},{"location":"spring2021/lecture-3/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-3/#notes","title":"Notes","text":"<p>Lecture by Josh Tobin.</p> <ul> <li>00:00 - Introduction</li> <li>01:34 - Sequence Problems</li> <li>06:28 - Review of RNNs</li> <li>22:00 - Vanishing Gradient Issue</li> <li>27:52 - LSTMs and Its Variants</li> <li>34:10 - Bidirectionality and Attention from Google's Neural Machine Translation</li> <li>46:38 - CTC Loss</li> <li>52:12 - Pros and Cons of Encoder-Decoder LSTM Architectures</li> <li>54:55 - WaveNet</li> </ul>"},{"location":"spring2021/lecture-4/","title":"Lecture 4: Transformers","text":""},{"location":"spring2021/lecture-4/#video","title":"Video","text":""},{"location":"spring2021/lecture-4/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-4/#notes","title":"Notes","text":"<p>Lecture by Sergey Karayev.</p> <p>In this video, you will learn about the origin of transfer learning in computer vision, its application in NLP in the form of embedding, NLP's ImageNet moment, and the Transformers model families.</p> <ul> <li>00:00 - Introduction</li> <li>00:42 - Transfer Learning in Computer Vision</li> <li>04:00 - Embeddings and Language Models</li> <li>10:09 - NLP's ImageNet moment: ELMO and ULMFit on datasets like SQuAD, SNLI, and GLUE</li> <li>16:49 - Rise of Transformers</li> <li>18:20 - Attention in Detail: (Masked) Self-Attention, Positional Encoding, and Layer Normalization</li> <li>27:33 - Transformers Variants: BERT, GPT/GPT-2/GPT-3, DistillBERT, T5, etc.</li> <li>36:20 - GPT3 Demos</li> <li>42:53 - Future Directions</li> </ul>"},{"location":"spring2021/lecture-5/","title":"Lecture 5: ML Projects","text":"<p>Learn how to set up Machine Learning projects like a pro. This includes an understanding of the ML lifecycle, an acute mind of the feasibility and impact, an awareness of the project archetypes, and an obsession with metrics and baselines.</p>"},{"location":"spring2021/lecture-5/#video","title":"Video","text":""},{"location":"spring2021/lecture-5/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-5/#notes","title":"Notes","text":"<p>Lecture by Josh Tobin. Notes transcribed by James Le and Vishnu Rachakonda.</p>"},{"location":"spring2021/lecture-5/#1-why-do-ml-projects-fail","title":"1 - Why Do ML Projects Fail?","text":"<p>Based on a report from TechRepublic a few years back, despite increased interest in adopting machine learning (ML) in the enterprise, 85% of machine learning projects ultimately fail to deliver on their intended promises to business. Failure can happen for many reasons; however, a few glaring dangers will cause any AI project to crash and burn.</p> <ul> <li> <p>ML is still very much a research endeavor. Therefore it is very  challenging to aim for a 100% success rate.</p> </li> <li> <p>Many ML projects are technically infeasible or poorly scoped.</p> </li> <li> <p>Many ML projects never leap production, thus getting stuck at the  prototype phase.</p> </li> <li> <p>Many ML projects have unclear success criteria because of a lack of  understanding of the value proposition.</p> </li> <li> <p>Many ML projects are poorly managed because of a lack of interest  from leadership.</p> </li> </ul>"},{"location":"spring2021/lecture-5/#2-lifecycle","title":"2 - Lifecycle","text":"<p>It\u2019s essential to understand what constitutes all of the activities in a machine learning project. Typically speaking, there are four major phases:</p> <ol> <li> <p>Planning and Project Setup: At this phase, we want to decide the problem to work on, determine the requirements and goals, figure out how to allocate resources properly, consider the ethical implications, etc.</p> </li> <li> <p>Data Collection and Labeling: At this phase, we want to collect training data and potentially annotate them with ground truth, depending on the specific sources where they come from. We may find that it\u2019s too hard to get the data, or it might be easier to label for a different task. If that\u2019s the case, go back to phase 1.</p> </li> <li> <p>Model Training and Model Debugging: At this phase, we want to implement baseline models quickly, find and reproduce state-of-the-art methods for the problem domain, debug our implementation, and improve the model performance for specific tasks. We may realize that we need to collect more data or that labeling is unreliable (thus, go back to phase 2). Or we may recognize that the task is too challenging and there is a tradeoff between project requirements (thus, go back to phase 1).</p> </li> <li> <p>Model Deploying and Model Testing: At this phase, we want to pilot the model in a constrained environment (i.e., in the lab), write tests to prevent regressions, and roll the model into production. We may see that the model doesn\u2019t work well in the lab, so we want to keep improving the model\u2019s accuracy (thus, go back to phase 3). Or we may want to fix the mismatch between training data and production data by collecting more data and mining hard cases (thus go back to phase 2). Or we may find out that the metric picked doesn\u2019t actually drive downstream user behavior, and/or performance in the real world isn\u2019t great. In such situations, we want to revisit the projects\u2019 metrics and requirements (thus, go back to phase 1).</p> </li> </ol> <p></p> <p>Besides the per-project activities mentioned above, there are two other things that any ML team will need to solve across any projects they get involved with: (1) building the team and hiring people; and (2) setting up infrastructure and tooling to build ML systems repeatedly and at scale.</p> <p>Additionally, it might be useful to understand state-of-the-art results in your application domain so that you know what\u2019s possible and what to try next.</p>"},{"location":"spring2021/lecture-5/#3-prioritizing-projects","title":"3 - Prioritizing Projects","text":"<p>To prioritize projects to work on, you want to find high-impact problems and assess the potential costs associated with them. The picture below shows a general framework that encourages us to target projects with high impact and high feasibility.</p> <p></p>"},{"location":"spring2021/lecture-5/#high-impact","title":"High Impact","text":"<p>There are no silver bullets to find high-impact ML problems to work on, but here are a few useful mental models:</p> <ul> <li> <p>Where can you take advantage of cheap prediction?</p> </li> <li> <p>Where is there friction in your product?</p> </li> <li> <p>Where can you automate complicated manual processes?</p> </li> <li> <p>What are other people doing?</p> </li> </ul>"},{"location":"spring2021/lecture-5/#cheap-prediction","title":"Cheap Prediction","text":"<p>In the book \u201cPrediction Machines,\u201d the authors (Ajay Agrawal, Joshua Gans, and Avi Goldfarb) come up with an excellent mental model on the economics of Artificial Intelligence: As AI reduces the cost of prediction and prediction is central for decision making, cheap predictions would be universal for problems across business domains. Therefore, you should look for projects where cheap predictions will have a huge business impact.</p>"},{"location":"spring2021/lecture-5/#product-needs","title":"Product Needs","text":"<p>Another lens is to think about what your product needs. In the article \u201cThree Principles for Designing ML-Powered Products,\u201d the Spotify Design team emphasizes the importance of building ML from a product perspective and looking for parts of the product experience with high friction. Automating those parts is exactly where there is a lot of impact for ML to make your business better.</p>"},{"location":"spring2021/lecture-5/#ml-strength","title":"ML Strength","text":"<p>In his popular blog post \u201cSoftware 2.0,\u201d Andrej Karpathy contrasts software 1.0 (which are traditional programs with explicit instructions) and software 2.0 (where humans specify goals, while the algorithm searches for a program that works). Software 2.0 programmers work with datasets, which get compiled via optimization\u200a\u2014\u200awhich works better, more general, and less computationally expensive. Therefore, you should look for complicated rule-based software where we can learn the rules instead of programming them.</p>"},{"location":"spring2021/lecture-5/#inspiration-from-others","title":"Inspiration From Others","text":"<p>Instead of reinventing the wheel, you can look at what other companies are doing. In particular, check out papers from large frontier organizations (Google, Facebook, Nvidia, Netflix, etc.) and blog posts from top earlier-stage companies (Uber, Lyft, Spotify, Stripe, etc.).</p> <p>Here is a list of excellent ML use cases to check out (credit to Chip Huyen\u2019s ML Systems Design Lecture 2 Note):</p> <ul> <li> <p>Human-Centric Machine Learning Infrastructure at  Netflix (Ville  Tuulos, InfoQ 2019)</p> </li> <li> <p>2020 state of enterprise machine  learning  (Algorithmia, 2020)</p> </li> <li> <p>Using Machine Learning to Predict Value of Homes On  Airbnb  (Robert Chang, Airbnb Engineering &amp; Data Science, 2017)</p> </li> <li> <p>Using Machine Learning to Improve Streaming Quality at  Netflix  (Chaitanya Ekanadham, Netflix Technology Blog, 2018)</p> </li> <li> <p>150 Successful Machine Learning Models: 6 Lessons Learned at  Booking.com  (Bernardi et al., KDD, 2019)</p> </li> <li> <p>How we grew from 0 to 4 million women on our fashion app, with a  vertical machine learning  approach  (Gabriel Aldamiz, HackerNoon, 2018)</p> </li> <li> <p>Machine Learning-Powered Search Ranking of Airbnb  Experiences  (Mihajlo Grbovic, Airbnb Engineering &amp; Data Science, 2019)</p> </li> <li> <p>From shallow to deep learning in  fraud  (Hao Yi Ong, Lyft Engineering, 2018)</p> </li> <li> <p>Space, Time and  Groceries  (Jeremy Stanley, Tech at Instacart, 2017)</p> </li> <li> <p>Creating a Modern OCR Pipeline Using Computer Vision and Deep  Learning  (Brad Neuberg, Dropbox Engineering, 2017)</p> </li> <li> <p>Scaling Machine Learning at Uber with  Michelangelo  (Jeremy Hermann and Mike Del Balso, Uber Engineering, 2019)</p> </li> <li> <p>Spotify\u2019s Discover Weekly: How machine learning finds your new  music  (Sophia Ciocca, 2017)</p> </li> </ul>"},{"location":"spring2021/lecture-5/#high-feasibility","title":"High Feasibility","text":"<p>The three primary cost drivers of ML projects in order of importance are data availability, accuracy requirement, and problem difficulty.</p> <p></p>"},{"location":"spring2021/lecture-5/#data-availability","title":"Data Availability","text":"<p>Here are the questions you need to ask concerning the data availability:</p> <ul> <li> <p>How hard is it to acquire data?</p> </li> <li> <p>How expensive is data labeling?</p> </li> <li> <p>How much data will be needed?</p> </li> <li> <p>How stable is the data?</p> </li> <li> <p>What are the data security requirements?</p> </li> </ul>"},{"location":"spring2021/lecture-5/#accuracy-requirement","title":"Accuracy Requirement","text":"<p>Here are the questions you need to ask concerning the accuracy requirement:</p> <ul> <li> <p>How costly are wrong predictions?</p> </li> <li> <p>How frequently does the system need to be right to be useful?</p> </li> <li> <p>What are the ethical implications?</p> </li> </ul> <p>It is worth noting that ML project costs tend to scale super-linearly in the accuracy requirement. The fundamental reason is that you typically need a lot more data and more high-quality labels to achieve high accuracy numbers.</p>"},{"location":"spring2021/lecture-5/#problem-difficulty","title":"Problem Difficulty","text":"<p>Here are the questions you need to ask concerning the problem difficulty:</p> <ul> <li> <p>Is the problem well-defined?</p> </li> <li> <p>Is there good published work on similar problems?</p> </li> <li> <p>What are the computing requirements?</p> </li> <li> <p>Can a human do it?</p> </li> </ul> <p>So what\u2019s still hard in machine learning? As a caveat, it\u2019s historically very challenging to predict what types of problems will be difficult for ML to solve in the future. But generally speaking, both unsupervised learning and reinforcement learning are still hard, even though they show promise in limited domains where tons of data and compute are available.</p> <p>Zooming into supervised learning, here are three types of hard problems:</p> <ul> <li> <p>Output is complex: These are problems where the output is  high-dimensional or ambiguous. Examples include 3D reconstruction,  video prediction, dialog systems, open-ended recommendation  systems, etc.</p> </li> <li> <p>Reliability is required: These are problems where high precision  and robustness are required. Examples include systems that can  fail safely in out-of-distribution scenarios, is robust to  adversarial attacks, or needs to tackle highly precise tasks.</p> </li> <li> <p>Generalization is required: These are problems with  out-of-distribution data or in the domains of reasoning, planning,  and causality. Examples include any systems for self-driving  vehicles or any systems that deal with small data.</p> </li> </ul> <p>Finally, this is a nice checklist for you to run an ML feasibility assessment:</p> <ul> <li> <p>Are you sure that you need ML at all?</p> </li> <li> <p>Put in the work upfront to define success criteria with all of the  stakeholders.</p> </li> <li> <p>Consider the ethics of using ML.</p> </li> <li> <p>Do a literature review.</p> </li> <li> <p>Try to build a labeled benchmark dataset rapidly.</p> </li> <li> <p>Build a minimal viable product with manual rules</p> </li> <li> <p>Are you \u201creally sure\u201d that you need ML at all?</p> </li> </ul>"},{"location":"spring2021/lecture-5/#4-archetypes","title":"4 - Archetypes","text":"<p>So far, we\u2019ve talked about the lifecycle and the impact of all machine learning projects. Ultimately, we generally want these projects, or applications of machine learning, to be useful for products. As we consider how ML can be applied in products, it\u2019s helpful to note that there are common machine learning product archetypes or recurrent patterns through which machine learning is applied to products. You can think of these as \u201cmental models\u201d you can use to assess your project and easily prioritize the needed resources.</p> <p>There are three common archetypes in machine learning projects: Software 2.0, Human-in-the-loop, and autonomous systems. They are shown in the table below, along with common examples and questions. We\u2019ll dive deeper into each.</p> Archetype Examples Questions Software 2.0 <p>- Improve code completion in IDE</p> <p>- Build customized recommendation system</p> <p>- Build a better video game AI</p> <p>- Do your models truly improve performance?</p> <p>- Does performance improvement generate business value?</p> <p>- Do performance improvements lead to a data flywheel?</p> Human-in-the-loop <p>- Turn sketches into slides</p> <p>- Email auto-completion</p> <p>- Help radiologists do job faster</p> <p>- How good does the system need to be to be useful?</p> <p>- How can you collect enough data to make it good?</p> Autonomous Systems <p>- Full self-driving</p> <p>- Automated customer support</p> <p>- Automated website design</p> <p>- What is an acceptable failure rate for the system?</p> <p>- How can you guarantee that it won\u2019t exceed the failure rate?</p> <p>- How inexpensively can you label data from the system?</p>"},{"location":"spring2021/lecture-5/#software-20","title":"Software 2.0","text":"<p>Software 2.0, which we previously alluded to from the Karpathy article, is defined as \u201caugmenting existing rules-based or deterministic software with machine learning, a probabilistic approach.\u201d Examples of this are taking a code completer in an IDE and improving the experience for the user by adding an ML component. Rather than suggesting a command based solely on the leading characters the programmer has written, you might add a model that suggests commands based on previous commands the programmer has written.</p> <p>As you build a software 2.0 project, strongly consider the concept of the data flywheel. For certain ML projects, as you improve your model, your product will get better and more users will engage with the product, thereby generating more data for the model to get even better. It\u2019s a classic virtuous cycle and truly the gold standard for ML projects.</p> <p></p> <p>In embarking on creating a data flywheel, critically consider where the model could fail in relation to your product. For example, do more users lead to collecting more data that is useful for improving your model? An actual system needs to be set up to capture this data and ensure that it's meaningful for the ML lifecycle. Furthermore, consider whether more data will lead to a better model (your job as an ML practitioner) or whether a better model and better predictions will actually lead to making the product better. Ideally, you should have a quantitative assessment of what makes your product \u201cbetter\u201d and map model improvement to it.</p>"},{"location":"spring2021/lecture-5/#human-in-the-loop-hil","title":"Human-in-the-Loop (HIL)","text":"<p>HIL systems are defined as machine learning systems where the output of your model will be reviewed by a human before being executed in the real world. For example, consider translating sketches into slides. An ML algorithm can take a sketch\u2019s input and suggest to a user a particular slide design. Every output of the ML model is considered and executed upon by a human, who ultimately has to decide on the slide\u2019s design.</p>"},{"location":"spring2021/lecture-5/#autonomous-systems","title":"Autonomous Systems","text":"<p>Autonomous systems are defined as machine learning systems where the system itself makes decisions or engages in outputs that are almost never reviewed by a human. Canonically, consider the self-driving car!</p>"},{"location":"spring2021/lecture-5/#feasibility","title":"Feasibility","text":"<p>Let\u2019s discuss how the product archetypes relate back to project priority. In terms of feasibility and impact, the two axes on which we consider priority, software 2.0 tends to have high feasibility but potentially low impact. The existing system is often being optimized rather than wholly replaced. However, this status with respect to priority is not static by any means. Building a data flywheel into your software 2.0 project can improve your product\u2019s impact by improving the model\u2019s performance on the task and future ones.</p> <p>In the case of human-in-the-loop systems, their feasibility and impact sit squarely in between autonomous systems and software 2.0. HIL systems, in particular, can benefit disproportionately in their feasibility and impact from effective product design, which naturally takes into account how humans interact with technology and can mitigate risks for machine learning model behavior. Consider how the Facebook photo tagging algorithm is implemented. Rather than tagging the user itself, the algorithm frequently asks the user to tag themselves. This effective product design allows the model to perform more effectively in the user\u2019s eye and reduces the impact of false classifications. Grammarly similarly solicits user input as part of its product design through offering explanations. Finally, recommender systems also implement this idea. In general, good product design can smooth the rough edges of ML (check out the concept of designing collaborative AI).</p> <p>There are industry-leading resources that can help you merge product design and ML. Apple\u2019s ML product design guidelines suggest three key questions to anyone seeking to put ML into a product:</p> <ol> <li> <p>What role does ML play in your product?</p> </li> <li> <p>How can you learn from your users?</p> </li> <li> <p>How should your app handle mistakes?</p> </li> </ol> <p>Associated with each question is a set of design paradigms that help address the answers to each question. There are similarly great resources from Microsoft and Spotify.</p> <p>Finally, autonomous systems can see their priority improved by improving their feasibility. Specifically, you can add humans in the loop or reduce the system\u2019s natural autonomy to improve its feasibility. In the case of self-driving cars, many companies add safety drivers as guardrails to improve autonomous systems. In Voyage\u2019s case, they take a more dramatic approach of constraining the problem for the autonomous system: they only run self-driving cars in senior living communities, a narrow subset of the broader self-driving problem.</p>"},{"location":"spring2021/lecture-5/#5-metrics","title":"5 - Metrics","text":"<p>So far, we\u2019ve talked about the overall ideas around picking projects and structuring them based on their archetypes and the specific considerations that go into them. Now, we\u2019ll shift gears and be a little more tactical to focus on metrics and baselines, which will help you execute projects more effectively.</p>"},{"location":"spring2021/lecture-5/#choosing-a-metric","title":"Choosing a Metric","text":"<p>Metrics help us evaluate models. There\u2019s a delicate balance between the real world (which is always messy and multifaceted) and the machine learning paradigm (which optimizes a single metric) in choosing a metric. In practical production settings, we often care about multiple dimensions of performance (i.e., accuracy, speed, cost, etc.). The challenge is to reconcile all the possible evaluation methods with the reality that ML systems work best at optimizing a single number. How can we balance these competing needs in building an ML product?</p> <p>As you start evaluating models, choose a single metric to focus on first, such as precision, accuracy, recall, etc. This can serve as an effective first filter of performance. Subsequently, you can put together a formula that combines all the metrics you care about. Note that it\u2019s important to be flexible and regularly update this formula as your models or the requirements for the product change.</p>"},{"location":"spring2021/lecture-5/#combining-metrics","title":"Combining Metrics","text":"<p>Two simple ways of combining metrics into a formula are averaging and thresholding.</p> <p>Averaging is less common but easy and intuitive; you can just take a simple average or a weighted average of the model\u2019s metrics and pick the highest average.</p> <p>More practically, you can apply a threshold evaluation to the model\u2019s metrics. In this method, out of n evaluation metrics, you threshold n-1 and optimize the nth metric. For example, if we look at a model\u2019s precision, memory requirement, and cost to train, we might threshold the memory requirement (no more than X MB) and the cost (no more than $X) and optimize precision (as high as possible). As you choose which metrics to threshold and what to set their threshold values to, make sure to consider domain-specific needs and the actual values of the metrics (how good/bad they might be).</p> <p></p>"},{"location":"spring2021/lecture-5/#6-baselines","title":"6 - Baselines","text":"<p>In any product development process, setting expectations properly is vital. For machine learning products, baselines help us set expectations for how well our model will perform. In particular, baselines set a useful lower bound for our model\u2019s performance. What\u2019s the minimum expectation we should have for a model\u2019s performance? The better defined and clear the baseline is, the more useful it is for setting the right expectations. Examples of baselines are human performance on a similar task, state-of-the-art models, or even simple heuristics.</p> <p>Baselines are especially important for helping decide the next steps. Consider the example below of two models with the same loss curve but differing performance with respect to the baseline. Clearly, they require different action items! As seen below, on the left, where we are starting to approach or exceed the baseline, we need to be mindful of overfitting and perhaps incorporate regularization of some sort. On the right, where the baseline hugely exceeds our model\u2019s performance, we clearly have a lot of work to do to improve the model and address its underfitting.</p> <p></p> <p>There are a number of sources to help us define useful baselines. Broadly speaking, there are external baselines (baselines defined by others) or internal baselines you can define yourself. With internal baselines, in particular, you don\u2019t need anything too complicated, or even something with ML! Simple tests like averaging across your dataset can help you understand if your model is achieving meaningful performance. If your model can\u2019t exceed a simple baseline like this, you might need to really re-evaluate the model.</p> <p>Human baselines are a particularly powerful form of baseline since we often seek to replace or augment human actions. In creating these baselines, note that there\u2019s usually an inverse relationship between the quality of the baseline and the ease of data collection. In a nutshell, the harder it is to get a human baseline, the better and more useful it probably is.</p> <p></p> <p>For example, a Mechanical Turk-created baseline is easy to generate nowadays, but the quality might be hit or miss because of the variance in annotators. However, trained, specialized annotators can be hard to acquire, but the specificity of their knowledge translates into a great baseline. Choosing where to situate your baseline on this range, from low quality/easy to high quality/hard, depends on the domain. Concentrating data collection strategically, ideally in classes where the model is least performant, is a simple way of improving the quality of the baseline.</p>"},{"location":"spring2021/lecture-5/#tldr","title":"TLDR","text":"<ol> <li> <p>Machine learning projects are iterative. Deploy something fast to  begin the cycle.</p> </li> <li> <p>Choose projects with high impact and low cost of wrong predictions.</p> </li> <li> <p>The secret sauce to make projects work well is to build automated  data flywheels.</p> </li> <li> <p>In the real world, you care about many things, but you should always  have just one to work on.</p> </li> <li> <p>Good baselines help you invest your effort the right way.</p> </li> </ol>"},{"location":"spring2021/lecture-5/#further-resources","title":"Further Resources","text":"<ul> <li> <p>Andrew Ng\u2019s \u201cMachine Learning  Yearning\u201d</p> </li> <li> <p>Andrej Kaparthy\u2019s \u201cSoftware  2.0\u201d</p> </li> <li> <p>Agrawal, Gans, and Goldfarb\u2019s \u201cThe Economics of  AI\u201d</p> </li> <li> <p>Chip Huyen\u2019s \u201cIntroduction to Machine Learning Systems  Design\u201d</p> </li> <li> <p>Apple\u2019s \u201cHuman-Interface Guidelines for Machine  Learning\u201d</p> </li> <li> <p>Google\u2019s \u201cRules of Machine  Learning\u201d</p> </li> </ul>"},{"location":"spring2021/lecture-6/","title":"Lecture 6: MLOps Infrastructure &amp; Tooling","text":""},{"location":"spring2021/lecture-6/#video","title":"Video","text":""},{"location":"spring2021/lecture-6/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-6/#notes","title":"Notes","text":"<p>Lecture by Sergey Karayev. Notes transcribed by James Le and Vishnu Rachakonda.</p>"},{"location":"spring2021/lecture-6/#1-dream-vs-reality-for-ml-practitioners","title":"1 - Dream vs. Reality for ML Practitioners","text":"<p>The dream of ML practitioners is that we are provided the data, and somehow we build an optimal machine learning prediction system available as a scalable API or an edge deployment. That deployment then generates more data for us, which can be used to improve our system.</p> <p>The reality is that you will have to:</p> <ul> <li>Aggregate, process, clean, label, and version the data</li> <li>Write and debug model code</li> <li>Provision compute</li> <li>Run many experiments and review the results</li> <li>Discover that you did something wrong or maybe try a different architecture -&gt; Write more code and provision more compute</li> <li>Deploy the model when you are happy</li> <li>Monitor the predictions that the model makes on production data so that you can gather some good examples and feed them back to the initial data flywheel loop</li> </ul> <p></p> <p>For example, the slide above is from Andrej Karpathy\u2019s talk at PyTorch Devcon 2019 discussing Tesla\u2019s self-driving system. Their dream is to build a system that goes from the data gathered through their training, evaluation, and inference processes and gets deployed on the cars. As people drive, more data will be collected and added back to the training set. As this process repeats, Tesla\u2019s ML engineers can all go on vacation :)</p> <p></p> <p>The picture above (from the famous Google paper \u201cMachine Learning: The High-Interest Credit Card of Technical Debt\u201d) shows that the ML code portion in a real-world ML system is a lot smaller than the infrastructure needed for its support. As ML projects move from small-scale research experiments to large-scale industry deployments, your organization most likely will require a massive amount of infrastructure to support large inferences, distributed training, data processing pipelines, reproducible experiments, model monitoring, etc.</p>"},{"location":"spring2021/lecture-6/#2-three-buckets-of-tooling-landscape","title":"2 - Three Buckets of Tooling Landscape","text":"<p>We can break down the landscape of all this necessary infrastructure into three buckets: data, training/evaluation, and deployment.</p> <ul> <li>The data bucket includes the data sources, data lakes/warehouses, data processing, data exploration, data versioning, and data labeling.</li> <li>The training/evaluation bucket includes compute sources, resource management, software engineering, frameworks and distributed training libraries, experiment management, and hyper-parameter tuning.</li> <li>The deployment bucket includes continuous integration and testing, edge deployment, web deployment, monitoring, and feature store.</li> </ul> <p>There are also several vendors offering \u201call-in-one\u201d MLOps solutions that cover all three buckets. This lecture focuses on the training/evaluation bucket.</p>"},{"location":"spring2021/lecture-6/#3-software-engineering","title":"3 - Software Engineering","text":"<p>When it comes to writing deep learning code, Python is the clear programming language of choice. As a general-purpose language, Python is easy to learn and easily accessible, enabling you to find skilled developers on a faster basis. It has various scientific libraries for data wrangling and machine learning (Pandas, NumPy, Scikit-Learn, etc.). Regardless of whether your engineering colleagues write code in a lower-level language like C, C++, or Java, it is generally neat to join different components with a Python wrapper.</p> <p>When choosing your IDEs, there are many options out there (Vim, Emacs, Sublime Text, Jupyter, VS Code, PyCharm, Atom, etc.). Each of these has its uses in any application, and you\u2019re better to switch between them to remain agile without relying heavily on shortcuts and packages. It also helps teams work better if they can jump into different IDEs and comment/collaborate with other colleagues. In particular, Visual Studio Code makes for a very nice Python experience, where you have access to built-in git staging and diffing, peek at documentation, linter code as you write, and open projects remotely.</p> <p>Jupyter Notebooks have rapidly grown in popularity among data scientists to become the standard for quick prototyping and exploratory analysis. For example, Netflix based all of their machine learning workflows on them, effectively building a whole notebook infrastructure to leverage them as a unifying layer for scheduling workflows. Jeremy Howard develops his fast.ai codebase entirely with notebooks and introduces a project called nbdev that shows people how to develop well-tested code in a notebook environment.</p> <p>However, there are many problems with using notebooks as a last resort when working in teams that aim to build machine/deep learning products. Alexander Mueller's blog post outlines the five reasons why they suck:</p> <ul> <li>It is challenging to enable good code versioning because notebooks are big JSON files that cannot be merged automatically.</li> <li>Notebook \u201cIDE\u201d is primitive, as they have no integration, no lifting, and no code-style correction. Data scientists are not software engineers, and thus, tools that govern their code quality and help improve it are very important.</li> <li>It is very hard to structure code reasonably, put code into functions, and develop tests while working in notebooks. You better develop Python scripts based on test-driven development principles as soon as you want to reproduce some experiments and run notebooks frequently.</li> <li>Notebooks have out-of-order execution artifacts, meaning that you can easily destroy your current working state when jumping between cells of notebooks.</li> <li>It is also difficult to run long or distributed tasks. If you want to handle big datasets, better pull your code out of notebooks, start a Python folder, create fixtures, write tests, and then deploy your application to a cluster.</li> </ul> <p></p> <p>Recently, a new application framework called Streamlit was introduced. The creators of the framework wanted machine learning engineers to be able to create beautiful apps without needing a tools team; in other words, these internal tools should arise as a natural byproduct of the machine learning workflow. According to the launch blog post, here are the core principles of Streamlit:</p> <ul> <li>Embrace Python scripting: Streamlit apps are just scripts that run from top to bottom. There\u2019s no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps.</li> <li>Treat widgets as variables: There are no callbacks in Streamlit. Every interaction simply reruns the script from top to bottom. This approach leads to a clean codebase.</li> <li>Reuse data and computation: Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default data store that lets Streamlit apps safely and effortlessly reuse information.</li> </ul> <p>Right now, Streamlit is building features that enable sharing machine learning projects to be as easy as pushing a web app to Heroku.</p> <p>We recommend using conda to set up your Python and CUDA environments and pip-tools to separate mutually compatible versions of all requirements for our lab.</p>"},{"location":"spring2021/lecture-6/#4-compute-hardware","title":"4 - Compute Hardware","text":"<p>We can break down the compute needs into an early-stage development step and a late-stage training/evaluation step.</p> <ul> <li>During the development stage, we write code, debug models, and look at the results. It\u2019d be nice to be able to compile and train models via an intuitive GUI quickly.</li> <li>During the training/evaluation stage, we design model architecture, search for hyper-parameters, and train large models. It\u2019d be nice to launch experiments and review results easily.</li> </ul> <p>Compute matters with each passing year due to the fact that the results came out of deep learning are using more and more compute (check out this 2018 report from OpenAI). Looking at recent Transformer models, while OpenAI\u2019s GPT-3 has not been fully commercialized yet, Google already released the Switch Transformer with orders of magnitude larger in the number of parameters.</p> <p>So should you get your own hardware, go straight to the cloud, or use on-premise options?</p>"},{"location":"spring2021/lecture-6/#gpu-basics","title":"GPU Basics","text":"<p>This is basically an NVIDIA game, as they are the only provider of good deep learning GPUs. However, Google\u2019s TPUs are the fastest, which is available only on GCP.</p> <p>There is a new NVIDIA architecture every year: Kepler -&gt; Pascal -&gt; Volta -&gt; Turing -&gt; Ampere. NVIDIA often released the server version of the cards first, then the \u201centhusiast\u201d version, and finally the consumer version. If you use these cards for business purposes, then you suppose to use the server version.</p> <p>GPUs have a different amount of RAM. You can only compute on the data that is on the GPU memory. The more data you can fit on the GPU, the larger your batches are, the faster your training goes.</p> <p>For deep learning, you use 32-bit precision. In fact, starting with the Volta architecture, NVIDIA developed tensor cores that are specifically designed for deep learning operations (mixed-precision between 32 and 16 bit). Tensor Cores reduce the used cycles needed for calculating multiply and addition operations and the reliance on repetitive shared memory access, thus saving additional cycles for memory access. This is very useful for the convolutional/Transformer models that are prevalent nowadays.</p> <p></p> <p>Let\u2019s go through different GPU architectures:</p> <ul> <li>Kepler/Maxwell: They are 2-4x slower than the Pascal/Volta ones below. You should not buy these old guards (K80).</li> <li>Pascal: They are in the 1080 Ti cards from 2017, which are still useful if bought used (especially for recurrent neural networks). P100 is the equivalent cloud offering.</li> <li>Volta/Turing: These are the preferred choices over the Kepler and Pascal because of their support for 16-bit mixed-precision via tensor cores. Hardware options are 2080 Ti and Titan RTX, while the cloud option is V100.</li> <li>Ampere: This architecture is available in the latest hardware (3090) and cloud (A100) offerings. They have the most tensor cores, leading to at least 30% speedup over Turing.</li> </ul> <p>You can check out this recent GPU benchmark from Lambda Labs and consult Tim Dettmers\u2019 advice on which GPUs to get.</p>"},{"location":"spring2021/lecture-6/#cloud-options","title":"Cloud Options","text":"<p>Amazon Web Services, Google Cloud Platform, and Microsoft Azure are the cloud heavyweights with largely similar functions and prices. There are also startups like Lambda Labs and Corewave that provide cloud GPUs.</p>"},{"location":"spring2021/lecture-6/#on-prem-options","title":"On-Prem Options","text":"<p>You can either build your own or buy pre-built devices from vendors like Lambda Labs, NVIDIA, Supermicro, Cirrascale, etc.</p>"},{"location":"spring2021/lecture-6/#recommendations","title":"Recommendations","text":"<p>Even though the cloud is expensive, it\u2019s hard to make on-prem devices scale past a certain point. Furthermore, dev-ops things are easier to be done in the cloud than to be set up by yourself. And if your machine dies or requires maintenance, that will be a constant headache if you are responsible for managing it.</p> <p>Here are our recommendations for three profiles:</p> <ul> <li>Hobbyists: Build your own machine (maybe a 4x Turing or a 2x Ampere PC) during development. Either use the same PC or use cloud instances during training/evaluation.</li> <li>Startups: Buy a sizeable Lambda Labs machine for every ML scientist during development. Buy more shared server machines or use cloud instances during training/evaluation.</li> <li>Larger companies: Buy an even more powerful machine for every ML scientist during development. Use cloud with fast instances with proper provisioning and handling of failures during training/evaluation.</li> </ul>"},{"location":"spring2021/lecture-6/#5-resource-management","title":"5 - Resource Management","text":"<p>With all the resources we have discussed (compute, dependencies, etc.), our challenge turns to manage them across the specific use cases we may have. Across all the resources, our goal is always to be able to easily experiment with the necessary resources to achieve the desired application of ML for our product.</p> <p>For this challenge of allocating resources to experimenting users, there are some common solutions:</p> <ol> <li>Script a solution ourselves: In theory, this is the simplest solution. We can check if a resource is free and then lock it if a particular user is using it or wants to.</li> <li>SLURM: If we don't want to write the script entirely ourselves, standard cluster job schedulers like SLURM can help us. The workflow is as follows: First, a script defines a job\u2019s requirements. Then, the SLURM queue runner analyzes this and then executes the jobs on the correct resource.</li> <li>Docker/Kubernetes: The above approach might still be too manual for your needs, in which case you can turn to Docker/Kubernetes. Docker packages the dependency stack into a lighter-than-VM package called a container (that excludes the OS). Kubernetes lets us run these Docker containers on a cluster. In particular, Kubeflow is an OSS project started by Google that allows you to spawn/manage Jupyter notebooks and manage multi-step workflows. It also has lots of plug-ins for extra processes like hyperparameter tuning and model deployment. However, Kubeflow can be a challenge to setup.</li> <li>Custom ML software: There\u2019s a lot of novel work and all-in-one solutions being developed to provision compute resources for ML development efficiently. Platforms like AWS Sagemaker, Paperspace Gradient, and Determined AI are advancing. Newer startups like Anyscale and Grid.AI (creators of PyTorch Lightning) are also tackling this. Their vision is around allowing you to seamlessly go from training models on your computer to running lots of training jobs in the cloud with a simple set of SDK commands.</li> </ol>"},{"location":"spring2021/lecture-6/#6-frameworks-and-distributed-training","title":"6 - Frameworks and Distributed Training","text":""},{"location":"spring2021/lecture-6/#deep-learning-frameworks","title":"Deep Learning Frameworks","text":"<p>If you\u2019ve built a deep learning model in the last few years, you\u2019ve probably used a deep learning framework. Frameworks like TensorFlow have crucially shaped the development of the deep learning revolution. The reality is that deep learning frameworks have existed for a while. Projects like Theano and Torch have been around for 10+ years. In contemporary use, there are three main frameworks we\u2019ll focus on - TensorFlow, Keras, and PyTorch. We evaluate frameworks based on their utility for production and development.</p> <p></p> <p>When TensorFlow came out in 2015, it was billed heavily as a production-optimized DL framework with an underlying static optimized graph that could be deployed across compute environments. However, TF 1.0 had a pretty unpleasant development experience; in addition to developing your models, you had to consider the underlying execution graph you were describing. This kind of \u201cmeta-development\u201d posed a challenge for newcomers. The Keras project solved many of these issues by offering a simpler way to define models, and eventually became a part of TensorFlow. PyTorch, when it was introduced in 2017, offered a polar opposite to TensorFlow. It made development super easy by consisting almost exclusively of simple Python commands, but was not designed to be fast at scale.</p> <p>Using TF/Keras or PyTorch is the current recommended way to build deep learning models unless you have a powerful reason not to. Essentially, both have converged to pretty similar points that balance development and production. TensorFlow adopted eager execution by default and became a lot easier to develop quickly in. PyTorch subsumed Caffe2 and became much faster as a result, specifically by adding the ability to compile speedier model artifacts. Nowadays, PyTorch has a lot of momentum, likely due to its ease of development. Newer projects like fast.ai and PyTorch Lighting add best practices and additional functionality to PyTorch, making it even more popular. According to this 2018 article on The Gradient, more than 80% of submissions are in PyTorch in academic projects.</p> <p>All these frameworks may seem like excessive quibbling, especially since PyTorch and TensorFlow have converged in important ways. Why do we even require such extensive frameworks?</p> <p>It\u2019s theoretically possible to define entire models and their required matrix math (e.g., a CNN) in NumPy, the classic Python numerical computing library. However, we quickly run into two challenges: back-propagating errors through our model and running the code on GPUs, which are powerful computation accelerators. For these issues to be addressed, we need frameworks to help us with auto-differentiation, an efficient way of computing the gradients, and software compatibility with GPUs, specifically interfacing with CUDA. Frameworks allow us to abstract the work required to achieve both features, while also layering in valuable abstractions for all the latest layer designs, optimizers, losses, and much more. As you can imagine, the abstractions offered by frameworks save us valuable time on getting our model to run and allow us to focus on optimizing our model.</p> <p>New projects like JAX and HuggingFace offer different or simpler abstractions. JAX focuses primarily on fast numerical computation with autodiff and GPUs across machine learning use cases (not just deep learning). HuggingFace abstracts entire model architectures in the NLP realm. Instead of loading individual layers, HuggingFace lets you load the entirety of a contemporary mode (along with weights)l like BERT, tremendously speeding up development time. HuggingFace works on both PyTorch and TensorFlow.</p>"},{"location":"spring2021/lecture-6/#distributed-training","title":"Distributed Training","text":"<p>Distributed training is a hot topic as the datasets and the models we train become too large to work on a single GPU. It\u2019s increasingly a must-do. The important thing to note is that distributed training is a process to conduct a single model training process; don\u2019t confuse it with training multiple models on different GPUs. There are two approaches to distributed training: data parallelism and model parallelism.</p>"},{"location":"spring2021/lecture-6/#data-parallelism","title":"Data Parallelism","text":"<p>Data parallelism is quite simple but powerful. If we have a batch size of X samples, which is too large for one GPU, we can split the X samples evenly across N GPUs. Each GPU calculates the gradients and passes them to a central node (either a GPU or a CPU), where the gradients are averaged and backpropagated through the distributed GPUs. This paradigm generally results in a linear speed-up time (e.g., two distributed GPUs results in a ~2X speed-up in training time). In modern frameworks like PyTorch, PyTorch Lightning, and even in schedulers like SLURM, data-parallel training can be achieved simply by specifying the number of GPUs or calling a data parallelism-enabling object (e.g., torch.nn.DataParallel). Other tools like Horovod (from Uber) use non-framework-specific ways of enabling data parallelism (e.g., MPI, a standard multiprocessing framework). Ray, the original open-source project from the Anyscale team, was designed to enable general distributed computing applications in Python and can be similarly applied to data-parallel distributed training.</p>"},{"location":"spring2021/lecture-6/#model-parallelism","title":"Model Parallelism","text":"<p>Model parallelism is a lot more complicated. If you can\u2019t fit your entire model\u2019s weights on a single GPU, you can split the weights across GPUs and pass data through each to train the weights. This usually adds a lot of complexity and should be avoided unless absolutely necessary. A better solution is to pony up for the best GPU available, either locally or in the cloud. You can also use gradient checkpointing, a clever trick wherein you write some gradients to disk as you compute them and load them only as you need them for updates.\u00a0 New work is coming out to make this easier (e.g., research and framework maturity).</p>"},{"location":"spring2021/lecture-6/#7-experiment-management","title":"7 - Experiment Management","text":"<p>As you run numerous experiments to refine your model, it\u2019s easy to lose track of code, hyperparameters, and artifacts. Model iteration can lead to lots of complexity and messiness. For example, you could be monitoring the learning rate\u2019s impact on your model\u2019s performance metric. With multiple model runs, how will you monitor the impact of the hyperparameter?</p> <p>A low-tech way would be to manually track the results of all model runs in a spreadsheet. Without great attention to detail, this can quickly spiral into a messy or incomplete artifact. Dedicated experiment management platforms are a remedy to this issue. Let\u2019s cover a few of the most common ones:</p> <ul> <li>TensorBoard: This is the default experiment tracking platform that comes with TensorFlow. As a pro, it\u2019s easy to get started with. On the flip side, it\u2019s not very good for tracking and comparing multiple experiments. It\u2019s also not the best solution to store past work.</li> <li>MLFlow: An OSS project from Databricks, MLFlow is a complete platform for the ML lifecycle. They have great experiment and model run management at the core of their platform. Another open-source project, Keepsake, recently came out focused solely on experiment tracking.</li> <li>Paid platforms (Comet.ml, Weights and Biases, Neptune): Finally, outside vendors offer deep, thorough experiment management platforms, with tools like code diffs, report writing, data visualization, and model registering features. In our labs, we will use Weights and Biases.</li> </ul>"},{"location":"spring2021/lecture-6/#8-hyperparameter-tuning","title":"8 - Hyperparameter Tuning","text":"<p>To finalize models, we need to ensure that we have the optimal hyperparameters. Since hyperparameter optimization (as this process is called) can be a particularly compute-intensive process, it\u2019s useful to have software that can help. Using specific software can help us kill underperforming model runs with bad hyperparameters early (to save on cost) or help us intelligently sweep ranges of hyperparameter values. Luckily, there\u2019s an increasing number of software providers that do precisely this:</p> <ul> <li>SigOpt offers an API focused exclusively on efficient, iterative hyperparameter optimization. Specify a range of values, get SigOpt\u2019s recommended hyperparameter settings, run the model and return the results to SigOpt, and repeat the process until you\u2019ve found the best parameters for your model.</li> <li>Rather than an API, Ray Tune offers a local software (part of the broader Ray ecosystem) that integrates hyperparameter optimization with compute resource allocation. Jobs are scheduled with specific hyperparameters according to state-of-the-art methods, and underperforming jobs are automatically killed.</li> <li>Weights and Biases also has this feature! With a YAML file specification, we can specify a hyperparameter optimization job and perform a \u201csweep,\u201d during which W&amp;B sends parameter settings to individual \u201cagents\u201d (our machines) and compares performance.</li> </ul>"},{"location":"spring2021/lecture-6/#9-all-in-one-solutions","title":"9 - \u201cAll-In-One\u201d Solutions","text":"<p>Some platforms integrate all the aspects of the applied ML stack we\u2019ve discussed (experiment tracking, optimization, training, etc.) and wrap them into a single experience. To support the \u201clifecycle,\u201d these platforms typically include:</p> <ul> <li>Labeling and data querying services</li> <li>Model training, especially though job scaling and scheduling</li> <li>Experiment tracking and model versioning</li> <li>Development environments, typically through notebook-style interfaces</li> <li>Model deployment (e.g., via REST APIs) and monitoring</li> </ul> <p>One of the earliest examples of such a system is Facebook\u2019s FBLearner (2016), which encompassed data and feature storage, training, inference, and continuous learning based on user interactions with the model\u2019s outputs. You can imagine how powerful having one hub for all this activity can be for ML application and development speed. As a result, cloud vendors (Google, AWS, Azure) have developed similar all-in-one platforms, like Google Cloud AI Platform and AWS SageMaker. Startups like Paperspace Gradient, Neptune, and FloydHub also offer all-in-one platforms focused on deep learning. Determined AI, which focuses exclusively on the model development and training part of the lifecycle, is the rare open-source platform in this space. Domino Data Lab is a traditional ML-focused startup with an extensive feature set worth looking at. It\u2019s natural to expect more MLOps (as this kind of tooling and infra is referred to) companies and vendors to build out their feature set and become platform-oriented; Weights and Biases is a good example of this.</p> <p>In conclusion, take a look at the below table to compare a select number of MLOps platform vendors. Pricing is quite variable.</p> <p></p> <p>Staying up to date across all the tooling can be a real challenge, but check out FSDL\u2019s Tooling Tuesdays on Twitter as a starting point!</p>"},{"location":"spring2021/lecture-7/","title":"Lecture 7: Troubleshooting Deep Neural Networks","text":""},{"location":"spring2021/lecture-7/#video","title":"Video","text":""},{"location":"spring2021/lecture-7/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-7/#notes","title":"Notes","text":"<p>Lecture by Josh Tobin. Notes transcribed by James Le and Vishnu Rachakonda.</p> <p>In traditional software engineering, a bug usually leads to the program crashing. While this is annoying for the user, it is critical for the developer to inspect the errors to understand why. With deep learning, we sometimes encounter errors, but all too often, the program crashes without a clear reason why. While these issues can be debugged manually, deep learning models most often fail because of poor output predictions. What\u2019s worse is that when the model performance is low, there is usually no signal about why or when the models failed.</p> <p>A common sentiment among practitioners is that they spend 80\u201390% of time debugging and tuning the models and only 10\u201320% of time deriving math equations and implementing things. This is confirmed by Andrej Kaparthy, as seen in this tweet.</p>"},{"location":"spring2021/lecture-7/#1-why-is-deep-learning-troubleshooting-hard","title":"1 - Why Is Deep Learning Troubleshooting Hard?","text":"<p>Suppose you are trying to reproduce a research paper result for your work, but your results are worse. You might wonder why your model\u2019s performance is significantly worse than the paper that you\u2019re trying to reproduce?</p> <p></p> <p>Many different things can cause this:</p> <ul> <li> <p>It can be implementation bugs. Most bugs in deep learning are     actually invisible.</p> </li> <li> <p>Hyper-parameter choices can also cause your performance to     degrade. Deep learning models are very sensitive to     hyper-parameters. Even very subtle choices of learning rate and     weight initialization can make a big difference.</p> </li> <li> <p>Performance can also be worse just because of data/model fit.     For example, you pre-train your model on ImageNet data and fit it     on self-driving car images, which are harder to learn.</p> </li> <li> <p>Finally, poor model performance could be caused not by your model     but your dataset construction. Typical issues here include not     having enough examples, dealing with noisy labels and imbalanced     classes, splitting train and test set with different     distributions.</p> </li> </ul>"},{"location":"spring2021/lecture-7/#2-strategy-to-debug-neural-networks","title":"2 - Strategy to Debug Neural Networks","text":"<p>The key idea of deep learning troubleshooting is: Since it is hard to disambiguate errors, it\u2019s best to start simple and gradually ramp up complexity.</p> <p>This lecture provides a decision tree for debugging deep learning models and improving performance. This guide assumes that you already have an initial test dataset, a single metric to improve, and target performance based on human-level performance, published results, previous baselines, etc.</p> <p></p>"},{"location":"spring2021/lecture-7/#3-start-simple","title":"3 - Start Simple","text":"<p>The first step is the troubleshooting workflow is starting simple.</p>"},{"location":"spring2021/lecture-7/#choose-a-simple-architecture","title":"Choose A Simple Architecture","text":"<p>There are a few things to consider when you want to start simple. The first is how to choose a simple architecture. These are architectures that are easy to implement and are likely to get you part of the way towards solving your problem without introducing as many bugs.</p> <p>Architecture selection is one of the many intimidating parts of getting into deep learning because there are tons of papers coming out all-the-time and claiming to be state-of-the-art on some problems. They get very complicated fast. In the limit, if you\u2019re trying to get to maximal performance, then architecture selection is challenging. But when starting on a new problem, you can just solve a simple set of rules that will allow you to pick an architecture that enables you to do a decent job on the problem you\u2019re working on.</p> <ul> <li> <p>If your data looks like images, start with a LeNet-like     architecture and consider using something like ResNet as your     codebase gets more mature.</p> </li> <li> <p>If your data looks like sequences, start with an LSTM with one     hidden layer and/or temporal/classical convolutions. Then, when     your problem gets more mature, you can move to an Attention-based     model or a WaveNet-like model.</p> </li> <li> <p>For all other tasks, start with a fully-connected neural network     with one hidden layer and use more advanced networks later,     depending on the problem.</p> </li> </ul> <p></p> <p>In reality, many times, the input data contains multiple of those things above. So how to deal with multiple input modalities into a neural network? Here is the 3-step strategy that we recommend:</p> <ul> <li> <p>First, map each of these modalities into a lower-dimensional feature     space. In the example above, the images are passed through a     ConvNet, and the words are passed through an LSTM.</p> </li> <li> <p>Then we flatten the outputs of those networks to get a single vector     for each of the inputs that will go into the model. Then we     concatenate those inputs.</p> </li> <li> <p>Finally, we pass them through some fully-connected layers to an     output.</p> </li> </ul>"},{"location":"spring2021/lecture-7/#use-sensible-defaults","title":"Use Sensible Defaults","text":"<p>After choosing a simple architecture, the next thing to do is to select sensible hyper-parameter defaults to start with. Here are the defaults that we recommend:</p> <ul> <li> <p>Adam optimizer with a \u201cmagic\u201d learning rate value of     3e-4.</p> </li> <li> <p>ReLU     activation for fully-connected and convolutional models and     Tanh     activation for LSTM models.</p> </li> <li> <p>He initialization for ReLU activation function and Glorot     initialization for Tanh activation     function.</p> </li> <li> <p>No regularization and data normalization.</p> </li> </ul>"},{"location":"spring2021/lecture-7/#normalize-inputs","title":"Normalize Inputs","text":"<p>The next step is to normalize the input data, subtracting the mean and dividing by the variance. Note that for images, it\u2019s fine to scale values to [0, 1] or [-0.5, 0.5] (for example, by dividing by 255).</p>"},{"location":"spring2021/lecture-7/#simplify-the-problem","title":"Simplify The Problem","text":"<p>The final thing you should do is consider simplifying the problem itself. If you have a complicated problem with massive data and tons of classes to deal with, then you should consider:</p> <ul> <li> <p>Working with a small training set around 10,000 examples.</p> </li> <li> <p>Using a fixed number of objects, classes, input size, etc.</p> </li> <li> <p>Creating a simpler synthetic training set like in research labs.</p> </li> </ul> <p>This is important because (1) you will have reasonable confidence that your model should be able to solve, and (2) your iteration speed will increase.</p> <p>The diagram below neatly summarizes how to start simple:</p> <p></p>"},{"location":"spring2021/lecture-7/#4-implement-and-debug","title":"4 - Implement and Debug","text":"<p>To give you a preview, below are the five most common bugs in deep learning models that we recognize:</p> <ul> <li> <p>Incorrect shapes for the network tensors: This bug is a common     one and can fail silently. This happens many times because the     automatic differentiation systems in the deep learning framework     do silent broadcasting. Tensors become different shapes in the     network and can cause a lot of problems.</p> </li> <li> <p>Pre-processing inputs incorrectly: For example, you forget to     normalize your inputs or apply too much input pre-processing     (over-normalization and excessive data augmentation).</p> </li> <li> <p>Incorrect input to the model\u2019s loss function: For example, you     use softmax outputs to a loss that expects logits.</p> </li> <li> <p>Forgot to set up train mode for the network correctly: For     example, toggling train/evaluation mode or controlling batch norm     dependencies.</p> </li> <li> <p>Numerical instability: For example, you get `inf` or `NaN`     as outputs. This bug often stems from using an exponent, a log, or     a division operation somewhere in the code.</p> </li> </ul> <p>Here are three pieces of general advice for implementing your model:</p> <ul> <li> <p>Start with a lightweight implementation. You want minimum     possible new lines of code for the 1st version of your model. The     rule of thumb is less than 200 lines. This doesn\u2019t count tested     infrastructure components or TensorFlow/PyTorch code.</p> </li> <li> <p>Use off-the-shelf components such as Keras if possible, since     most of the stuff in Keras works well out-of-the-box. If you have     to use TensorFlow, use the built-in functions, don\u2019t do the math     yourself. This would help you avoid a lot of numerical instability     issues.</p> </li> <li> <p>Build complicated data pipelines later. These are important for     large-scale ML systems, but you should not start with them because     data pipelines themselves can be a big source of bugs. Just start     with a dataset that you can load into memory.</p> </li> </ul> <p></p>"},{"location":"spring2021/lecture-7/#get-your-model-to-run","title":"Get Your Model To Run","text":"<p>The first step of implementing bug-free deep learning models is getting your model to run at all. There are a few things that can prevent this from happening:</p> <ul> <li> <p>Shape mismatch/casting issue: To address this type of problem,     you should step through your model creation and inference     step-by-step in a debugger, checking for correct shapes and data     types of your tensors.</p> </li> <li> <p>Out-of-memory issues: This can be very difficult to debug. You     can scale back your memory-intensive operations one-by-one. For     example, if you create large matrices anywhere in your code, you     can reduce the size of their dimensions or cut your batch size in     half.</p> </li> <li> <p>Other issues: You can simply Google it. Stack Overflow would be     great most of the time.</p> </li> </ul> <p>Let\u2019s zoom in on the process of stepping through model creation in a debugger and talk about debuggers for deep learning code:</p> <ul> <li> <p>In PyTorch, you can use     ipdb\u200a\u2014\u200awhich exports     functions to access the interactive     IPython debugger.</p> </li> <li> <p>In TensorFlow, it\u2019s trickier. TensorFlow separates the process of     creating the graph and executing operations in the graph. There     are three options you can try: (1) step through the graph creation     itself and inspect each tensor layer, (2) step into the training     loop and evaluate the tensor layers, or (3) use TensorFlow     Debugger     (tfdb), which does option 1 and 2 automatically.</p> </li> </ul> <p></p>"},{"location":"spring2021/lecture-7/#overfit-a-single-batch","title":"Overfit A Single Batch","text":"<p>After getting your model to run, the next thing you need to do is to overfit a single batch of data. This is a heuristic that can catch an absurd number of bugs. This really means that you want to drive your training error arbitrarily close to 0.</p> <p>There are a few things that can happen when you try to overfit a single batch and it fails:</p> <ul> <li> <p>Error goes up: Commonly, this is due to a flip sign somewhere in     the loss function/gradient.</p> </li> <li> <p>Error explodes: This is usually a numerical issue but can also     be caused by a high learning rate.</p> </li> <li> <p>Error oscillates: You can lower the learning rate and inspect     the data for shuffled labels or incorrect data augmentation.</p> </li> <li> <p>Error plateaus: You can increase the learning rate and get rid     of regulation. Then you can inspect the loss function and the data     pipeline for correctness.</p> </li> </ul> <p></p>"},{"location":"spring2021/lecture-7/#compare-to-a-known-result","title":"Compare To A Known Result","text":"<p>Once your model overfits in a single batch, there can still be some other issues that cause bugs. The last step here is to compare your results to a known result. So what sort of known results are useful?</p> <ul> <li> <p>The most useful results come from an official model implementation     evaluated on a similar dataset to yours. You can step through     the code in both models line-by-line and ensure your model has the     same output. You want to ensure that your model performance is up     to par with expectations.</p> </li> <li> <p>If you can\u2019t find an official implementation on a similar dataset,     you can compare your approach to results from an official model     implementation evaluated on a benchmark dataset. You most     definitely want to walk through the code line-by-line and ensure     you have the same output.</p> </li> <li> <p>If there is no official implementation of your approach, you can     compare it to results from an unofficial model implementation.     You can review the code the same as before but with lower     confidence (because almost all the unofficial implementations on     GitHub have bugs).</p> </li> <li> <p>Then, you can compare to results from a paper with no code (to     ensure that your performance is up to par with expectations),     results from your model on a benchmark dataset (to make sure     your model performs well in a simpler setting), and results from     a similar model on a similar dataset (to help you get a     general sense of what kind of performance can be expected).</p> </li> <li> <p>An under-rated source of results comes from simple baselines     (for example, the average of outputs or linear regression), which     can help make sure that your model is learning anything at all.</p> </li> </ul> <p>The diagram below neatly summarizes how to implement and debug deep neural networks:</p> <p></p>"},{"location":"spring2021/lecture-7/#5-evaluate","title":"5 - Evaluate","text":""},{"location":"spring2021/lecture-7/#bias-variance-decomposition","title":"Bias-Variance Decomposition","text":"<p>To evaluate models and prioritize the next steps in model development, we will apply the bias-variance decomposition. The bias-variance decomposition is the fundamental model fitting tradeoff. In our application, let\u2019s talk more specifically about the formula for bias-variance tradeoff with respect to the test error; this will help us apply the concept more directly to our model\u2019s performance. There are four terms in the formula for test error:</p> <p>Test error = irreducible error + bias + variance + validation overfitting</p> <ol> <li> <p>Irreducible error is the baseline error you don\u2019t expect your     model to do better. It can be estimated through strong baselines,     like human performance.</p> </li> <li> <p>Avoidable bias, a measure of underfitting, is the difference     between our train error and irreducible error.</p> </li> <li> <p>Variance, a measure of overfitting, is the difference between     validation error and training error.</p> </li> <li> <p>Validation set overfitting is the difference between test error     and validation error.</p> </li> </ol> <p>Consider the chart of learning curves and errors below. Using the test error formula for bias and variance, we can calculate each component of test error and make decisions based on the value. For example, our avoidable bias is rather low (only 2 points), while the variance is much higher (5 points). With this knowledge, we should prioritize methods of preventing overfitting, like regularization.</p> <p></p>"},{"location":"spring2021/lecture-7/#distribution-shift","title":"Distribution Shift","text":"<p>Clearly, the application of the bias-variance decomposition to the test error has already helped prioritize our next steps for model development. However, until now, we\u2019ve assumed that the samples (training, validation, testing) all come from the same distribution. What if this isn\u2019t the case? In practical ML situations, this distribution shift often cars. In building self-driving cars, a frequent occurrence might be training with samples from one distribution (e.g., daytime driving video) but testing or inferring on samples from a totally different distribution (e.g., night time driving).</p> <p>A simple way of handling this wrinkle in our assumption is to create two validation sets: one from the training distribution and one from the test distribution. This can be helpful even with a very small testing set. If we apply this, we can actually estimate our distribution shift, which is the difference between testing validation error and testing error. This is really useful for practical applications of ML! With this new term, let\u2019s update our test error formula of bias and variance:</p> <p>Test error = irreducible error + bias + variance + distribution shift + validation overfitting</p>"},{"location":"spring2021/lecture-7/#6-improve-model-and-data","title":"6 - Improve Model and Data","text":"<p>Using the updated formula from the last section, we\u2019ll be able to decide on and prioritize the right next steps for each iteration of a model. In particular, we\u2019ll follow a specific process (shown below).</p> <p></p>"},{"location":"spring2021/lecture-7/#step-1-address-underfitting","title":"Step 1: Address Underfitting","text":"<p>We\u2019ll start by addressing underfitting (i.e., reducing bias). The first thing to try in this case is to make your model bigger (e.g., add layers, more units per layer). Next, consider regularization, which can prevent a tight fit to your data. Other options are error analysis, choosing a different model architecture (e.g., something more state of the art), tuning hyperparameters, or adding features. Some notes:</p> <ul> <li> <p>Choosing different architectures, especially a SOTA one, can be very     helpful but is also risky. Bugs are easily introduced in the     implementation process.</p> </li> <li> <p>Adding features is uncommon in the deep learning paradigm (vs.     traditional machine learning). We usually want the network to     learn features of its own accord. If all else fails, it can be     beneficial in a practical setting.</p> </li> </ul> <p></p>"},{"location":"spring2021/lecture-7/#step-2-address-overfitting","title":"Step 2: Address Overfitting","text":"<p>After addressing underfitting, move on to solving overfitting. Similarly, there\u2019s a recommended series of methods to try in order. Starting with collecting training data (if possible) is the soundest way to address overfitting, though it can be challenging in certain applications. Next, tactical improvements like normalization, data augmentation, and regularization can help. Following these steps, traditional defaults like tuning hyperparameters, choosing a different architecture, or error analysis are useful. Finally, if overfitting is rather intractable, there\u2019s a series of less recommended steps, such as early stopping, removing features, and reducing model size. Early stopping is a personal choice; the fast.ai community is a strong proponent.</p> <p></p>"},{"location":"spring2021/lecture-7/#step-3-address-distribution-shift","title":"Step 3: Address Distribution Shift","text":"<p>After addressing underfitting and overfitting, If there\u2019s a difference between the error on our training validation set vs. our test validation set, we need to address the error caused by the distribution shift. This is a harder problem to solve, so there\u2019s less in our toolkit to apply.</p> <p>Start by looking manually at the errors in the test-validation set. Compare the potential logic behind these errors to the performance in the train-validation set, and use the errors to guide further data collection. Essentially, reason about why your model may be suffering from distribution shift error. This is the most principled way to deal with distribution shift, though it\u2019s the most challenging way practically. If collecting more data to address these errors isn\u2019t possible, try synthesizing data. Additionally, you can try domain adaptation.</p> <p></p>"},{"location":"spring2021/lecture-7/#error-analysis","title":"Error Analysis","text":"<p>Manually evaluating errors to understand model performance is generally a high-yield way of figuring out how to improve the model. Systematically performing this error analysis process and decomposing the error from different error types can help prioritize model improvements. For example, in a self-driving car use case with error types like hard-to-see pedestrians, reflections, and nighttime scenes, decomposing the error contribution of each and where it occurs (train-val vs. test-val) can give rise to a clear set of prioritized action items. See the table for an example of how this error analysis can be effectively structured.</p> <p></p>"},{"location":"spring2021/lecture-7/#domain-adaptation","title":"Domain Adaptation","text":"<p>Domain adaptation is a class of techniques that train on a \u201csource\u201d distribution and generalize to another \u201ctarget\u201d using only unlabeled data or limited labeled data. You should use domain adaptation when access to labeled data from the test distribution is limited, but access to relatively similar data is plentiful.</p> <p>There are a few different types of domain adaptation:</p> <ol> <li> <p>Supervised domain adaptation: In this case, we have limited data     from the target domain to adapt to. Some example applications of     the concept include fine-tuning a pre-trained model or adding     target data to a training set.</p> </li> <li> <p>Unsupervised domain adaptation: In this case, we have lots of     unlabeled data from the target domain. Some techniques you might     see are CORAL, domain confusion, and CycleGAN.</p> </li> </ol> <p>Practically speaking, supervised domain adaptation can work really well! Unsupervised domain adaptation has a little bit further to go.</p>"},{"location":"spring2021/lecture-7/#step-4-rebalance-datasets","title":"Step 4: Rebalance datasets","text":"<p>If the test-validation set performance starts to look considerably better than the test performance, you may have overfit the validation set. This commonly occurs with small validation sets or lots of hyperparameter training. If this occurs, resample the validation set from the test distribution and get a fresh estimate of the performance.</p>"},{"location":"spring2021/lecture-7/#7-tune-hyperparameters","title":"7 - Tune Hyperparameters","text":"<p>One of the core challenges in hyperparameter optimization is very basic: which hyperparameters should you tune? As we consider this fundamental question, let\u2019s keep the following in mind:</p> <ul> <li> <p>Models are more sensitive to some hyperparameters than others. This     means we should focus our efforts on the more impactful     hyperparameters.</p> </li> <li> <p>However, which hyperparameters are most important depends heavily on     our choice of model.</p> </li> <li> <p>Certain rules of thumbs can help guide our initial thinking.</p> </li> <li> <p>Sensitivity is always relative to default values; if you use good     defaults, you might start in a good place!</p> </li> </ul> <p>See the following table for a ranked list of hyperparameters and their impact on the model:</p> <p></p>"},{"location":"spring2021/lecture-7/#techniques-for-tuning-hyperparameter-optimization","title":"Techniques for Tuning Hyperparameter Optimization","text":"<p>Now that we know which hyperparameters make the most sense to tune (using rules of thumb), let\u2019s consider the various methods of actually tuning them:</p> <ol> <li> <p>Manual Hyperparameter Optimization. Colloquially referred to as     Graduate Student Descent, this method works by taking a manual,     detailed look at your algorithm, building intuition, and     considering which hyperparameters would make the most difference.     After figuring out these parameters, you train, evaluate, and     guess a better hyperparameter value using your intuition for the     algorithm and intelligence. While it may seem archaic, this method     combines well with other methods (e.g., setting a range of values     for hyperparameters) and has the main benefit of reducing     computation time and cost if used skillfully. It can be     time-consuming and challenging, but it can be a good starting     point.</p> </li> <li> <p>Grid Search. Imagine each of your parameters plotted against     each other on a grid, from which you uniformly sample values to     test. For each point, you run a training run and evaluate     performance. The advantages are that it\u2019s very simple and can     often produce good results. However, it\u2019s quite inefficient, as     you must run every combination of hyperparameters. It also often     requires prior knowledge about the hyperparameters since we must     manually set the range of values.</p> </li> <li> <p>Random Search: This method is recommended over grid search.     Rather than sampling from the grid of values for the     hyperparameter evenly, we\u2019ll choose n points sampled randomly     across the grid. Empirically, this method produces better results     than grid search. However, the results can be somewhat     uninterpretable, with unexpected values in certain hyperparameters     returned.</p> </li> <li> <p>Coarse-to-fine Search: Rather than running entirely random runs,     we can gradually narrow in on the best hyperparameters through     this method. Initially, start by defining a very large range to     run a randomized search on. Within the pool of results, you can     find N best results and hone in on the hyperparameter values used     to generate those samples. As you iteratively perform this method,     you can get excellent performance. This doesn\u2019t remove the manual     component, as you have to select which range to continuously     narrow your search to, but it\u2019s perhaps the most popular method     available.</p> </li> <li> <p>Bayesian Hyperparameter Optimization: This is a reasonably     sophisticated method, which you can read more about     here     and     here.     At a high level, start with a prior estimate of parameter     distributions. Subsequently, maintain a probabilistic model of the     relationship between hyperparameter values and model performance.     As you maintain this model, you toggle between training with     hyperparameter values that maximize the expected improvement (per     the model) and use training results to update the initial     probabilistic model and its expectations. This is a great,     hands-off, efficient method to choose hyperparameters. However,     these techniques can be quite challenging to implement from     scratch. As libraries and infrastructure mature, the integration     of these methods into training will become easier.</p> </li> </ol> <p>In summary, you should probably start with coarse-to-fine random searches and move to Bayesian methods as your codebase matures and you\u2019re more certain of your model.</p>"},{"location":"spring2021/lecture-7/#8-conclusion","title":"8 - Conclusion","text":"<p>To wrap up this lecture, deep learning troubleshooting and debugging is really hard. It\u2019s difficult to tell if you have a bug because there are many possible sources for the same degradation in performance. Furthermore, the results can be sensitive to small changes in hyper-parameters and dataset makeup.</p> <p>To train bug-free deep learning models, we need to treat building them as an iterative process. If you skipped to the end, the following steps can make this process easier and catch errors as early as possible:</p> <ul> <li> <p>Start Simple: Choose the simplest model and data possible.</p> </li> <li> <p>Implement and Debug: Once the model runs, overfit a single batch     and reproduce a known result.</p> </li> <li> <p>Evaluate: Apply the bias-variance decomposition to decide what     to do next.</p> </li> <li> <p>Tune Hyper-parameters: Use coarse-to-fine random searches to     tune the model\u2019s hyper-parameters.</p> </li> <li> <p>Improve Model and Data: Make your model bigger if your model     under-fits and add more data and/or regularization if your model     over-fits.</p> </li> </ul> <p>Here are additional resources that you can go to learn more:</p> <ul> <li> <p>Andrew Ng\u2019s \u201cMachine Learning     Yearning\u201d     book.</p> </li> <li> <p>This Twitter     thread     from Andrej Karpathy.</p> </li> <li> <p>BYU\u2019s \u201cPractical Advice for Building Deep Neural     Networks\u201d     blog post.</p> </li> </ul>"},{"location":"spring2021/lecture-8/","title":"Lecture 8: Data Management","text":""},{"location":"spring2021/lecture-8/#video","title":"Video","text":""},{"location":"spring2021/lecture-8/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-8/#notes","title":"Notes","text":"<p>Lecture by Sergey Karayev. Notes transcribed by James Le and Vishnu Rachakonda.</p> <p>One of the best data science articles written in 2019 is \u201cData science is different now\u201d by Vicki Boykis. Part of the article is a collection of tweets from other data science and machine learning practitioners.</p> <p></p>"},{"location":"spring2021/lecture-8/#1-data-management-overview","title":"1 - Data Management Overview","text":"<p>When we think about what data management for deep learning entails, there might be many different data sources: images on S3, text files on a file system, logs spread across different machines, and maybe even records in a database. At some point, you need to get all of that data over to a local filesystem next to GPUs. The way you will get data over to that trainable format is different for every project and every company. For instance:</p> <ul> <li> <p>Maybe you train your images on ImageNet, and all the images are just S3 URLs. Then, all you have to do is download them over to the local filesystem.</p> </li> <li> <p>Maybe you have a bunch of text files that you crawled yourself somewhere. You want to use Spark to process them on a cluster and Pandas data frame to analyze/select subsets that will be used in the local filesystem.</p> </li> <li> <p>Maybe you collect logs and records from your database into a data lake/warehouse (like Snowflake). Then, you process that output and convert them into a trainable format.</p> </li> </ul> <p>There are countless possibilities that we are not going to cover completely in this lecture, but here are the key points to remember:</p> <ul> <li> <p>Let the data flow through you: You should spend 10x as much time as you want to on exploring the dataset.</p> </li> <li> <p>Data is the best way to improve your overall ML project performance: Instead of trying new architectures or kicking off the hyper-parameter search, adding more data and augmenting the existing dataset will often be the best bang to your buck.</p> </li> <li> <p>Keep It Simple Stupid: We will discuss complex pipelines and new terms, but it\u2019s important to not over-complicate things and make data management a rocket science.</p> </li> </ul>"},{"location":"spring2021/lecture-8/#2-data-sources","title":"2 - Data Sources","text":"<p>So, where do the training data come from? Most deep learning applications require lots of labeled data (with exceptions in applications of reinforcement learning, GANs, and GPT-3). There are publicly available datasets that can serve as a starting point, but there is no competitive advantage of using them. In fact, most companies usually spend a lot of money and time labeling their own data.</p>"},{"location":"spring2021/lecture-8/#data-flywheel","title":"Data Flywheel","text":"<p>Data flywheel is an exciting concept: if you can get your models in front of the users, you can build your products in a mechanism that your users contribute good data back to you and improve the model predictions. This can enable rapid improvement after you get that v1 model out into the real world.</p>"},{"location":"spring2021/lecture-8/#semi-supervised-learning","title":"Semi-Supervised Learning","text":"<p>Semi-supervised learning is a relatively recent learning technique where the training data is autonomously (or automatically) labeled. It is still supervised learning, but the datasets do not need to be manually labeled by a human; but they can be labeled by finding and exploiting the relations (or correlations) between different input signals (that is, input coming from different sensor modalities).</p> <p>A natural advantage and consequence of semi-supervised learning are that this technique can be performed in an online fashion (given that data can be gathered and labeled without human intervention) more easily (with respect to, e.g., supervised learning), where models can be updated or trained entirely from scratch. Therefore, semi-supervised learning should also be well suited for changing environments, changing data, and, in general, changing requirements.</p> <p></p> <p>For a text example, you can predict the future words from the past words, predict the beginning of a sentence from the end of a sentence, or predict the middle word of a sentence from the words surrounding it. You can even examine whether two sentences occur in the same paragraph in the same corpus of your training data. These are different ways to formulate the problem, where you don\u2019t need to label anything and simply use the data to supervise itself.</p> <p>This technique also applies to vision. Facebook AI recently released a model called SEER trained on 1 billion random images from the Internet. Yet, SEER achieved state-of-the-art accuracy on the ImageNet top-1 prediction task.</p> <p>If you\u2019re interested in learning more about semi-supervised learning, check out:</p> <ul> <li> <p>Lilian Weng's \"Self-Supervised Learning\" post</p> </li> <li> <p>Facebook AI\u2019s \u201cSelf-Supervised Learning: The Dark Matter Of Intelligence\u201d post</p> </li> <li> <p>Facebook AI\u2019s VISSL library for the SEER algorithm</p> </li> </ul>"},{"location":"spring2021/lecture-8/#data-augmentation","title":"Data Augmentation","text":"<p>Recent advances in deep learning models have been largely attributed to the quantity and diversity of data gathered in recent years. Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks. In fact, they are mostly required for training computer vision models. Both Keras and fast.ai provide functions that do this.</p> <p></p> <p>Data augmentation also applies to other types of data.</p> <ul> <li> <p>For tabular data, you can delete some cells to simulate missing data.</p> </li> <li> <p>For text, there are no well-established techniques, but you can replace words with synonyms and change the order of things.</p> </li> <li> <p>For speech and video, you can change speed, insert a pause, mix different sequences, and more.</p> </li> </ul> <p>If you\u2019re interested in learning more about data augmentation, check out:</p> <ul> <li> <p>Berkeley AI\u2019s \u201c1000x Faster Data Augmentation\u201d post</p> </li> <li> <p>Edward Ma\u2019s \u201cnlpaug\u201d repository</p> </li> </ul>"},{"location":"spring2021/lecture-8/#synthetic-data","title":"Synthetic Data","text":"<p>Related to the concept of data augmentation is synthetic data, an underrated idea that is almost always worth starting with. Synthetic data is data that\u2019s generated programmatically. For example, photorealistic images of objects in arbitrary scenes can be rendered using video game engines or audio generated by a speech synthesis model from the known text. It\u2019s not unlike traditional data augmentation, where crops, flips, rotations, and distortions are used to increase the variety of data that models have to learn from. Synthetically generated data takes those same concepts even further.</p> <p>Most of today\u2019s synthetic data is visual. Tools and techniques developed to create photorealistic graphics in movies and computer games are repurposed to create the training data needed for machine learning. Not only can these rendering engines produce arbitrary numbers of images, but they can also produce annotations too. Bounding boxes, segmentation masks, depth maps, and any other metadata is output right alongside pictures, making it simple to build pipelines that produce their own data.</p> <p>Because samples are generated programmatically along with annotations, synthetic datasets are far cheaper to produce than traditional ones. That means we can create more data and iterate more often to produce better results. Need to add another class to your model? No problem. Need to add another key point to the annotation? Done. This is especially useful for applications in driving and robotics.</p> <p></p> <p>If you\u2019re interested in learning more about synthetic data, check out:</p> <ul> <li> <p>Dropbox\u2019s \u201cCreating A Modern OCR Pipeline Using Computer Vision and Deep Learning\u201d post</p> </li> <li> <p>Andrew Moffat\u2019s \u201cmetabrite-receipt-tests\u201d repository</p> </li> <li> <p>Microsoft\u2019s AirSim simulator</p> </li> <li> <p>OpenAI\u2019s \u201cIngredients For Robotics Research\u201d post</p> </li> </ul>"},{"location":"spring2021/lecture-8/#3-data-storage","title":"3 - Data Storage","text":"<p>Data storage requirements for AI vary widely according to the application and the source material. Datasets in intelligence, defense, medical, science, and geology frequently combine petabyte-scale storage volumes with individual file sizes in the gigabyte range. By contrast, data used in areas such as supply chain analytics and fraud detection are much smaller.</p> <p>There are four building blocks in a data storage system:</p> <ol> <li> <p>The filesystem</p> </li> <li> <p>The object storage</p> </li> <li> <p>The database</p> </li> <li> <p>The data lake or data warehouse</p> </li> </ol>"},{"location":"spring2021/lecture-8/#filesystem","title":"Filesystem","text":"<p>The filesystem is the foundational layer of storage.</p> <ul> <li> <p>Its fundamental unit is a \u201cfile\u201d\u200a\u2014\u200awhich can be text or binary, is not versioned, and is easily overwritten.</p> </li> <li> <p>A file system can be as simple as a locally mounted disk containing all the files you need.</p> </li> <li> <p>More advanced options include networked filesystems (NFS), which are accessible over the network by multiple machines, and distributed file systems (HDFS) which are stored and accessed over multiple machines.</p> </li> </ul> <p></p> <p>The plots above display hard-drive speeds for SATA hard drive, SATA SSD, and NVMe SSD.</p> <ul> <li> <p>The left plot shows the sustained throughput in MBps (how much information to copy a file): The latest iteration of hard drive technology (NVMe) is 6-10x more powerful than older iterations.</p> </li> <li> <p>The right plot shows the seek time in milliseconds (how long it takes to go to a file on disk): The NVMe is 25-30x faster than the old-school ones.</p> </li> </ul> <p>What format should we store data in?</p> <ul> <li> <p>For binary data (images, audios, videos), just files are enough. In Tensorflow, you have the TFRecord format to batch binary files, which does not seem to be necessary with the NVMe hard drives.</p> </li> <li> <p>For large tabular and text data, you have two choices:</p> <ul> <li> <p>HDF5 is powerful but bloated and declining.</p> </li> <li> <p>Parquet is widespread and recommended.</p> </li> <li> <p>Feather is an up-and-coming open-source option powered by Apache Arrow.</p> </li> </ul> </li> <li> <p>Both Tensorflow and PyTorch provide their native dataset class interfaces (tf.data and PyTorch DataLoader).</p> </li> </ul>"},{"location":"spring2021/lecture-8/#object-storage","title":"Object Storage","text":"<p>Object storage is an API over the filesystem that allows users to use a command on files (GET, PUT, DELETE) to a service without worrying where they are actually stored.</p> <ul> <li> <p>Its fundamental unit is an \u201cobject,\u201d\u200a \u200awhich is usually binary (images, sound files\u2026).</p> </li> <li> <p>Object storage can be built with data versioning and data redundancy into the API.</p> </li> <li> <p>It is not as fast as local files but fast enough within the cloud.</p> </li> </ul>"},{"location":"spring2021/lecture-8/#database","title":"Database","text":"<p>A database is a persistent, fast, scalable storage and retrieval of structured data.</p> <ul> <li> <p>Its fundamental unit is a \u201crow\u201d (unique IDs, references to other rows, values in columns).</p> </li> <li> <p>Databases are also known for online transaction processing (OLTP). The mental model here is that everything is actually in memory, but the software ensures that everything is logged to disk and never lost.</p> </li> <li> <p>Databases are not built for binary data, so you must store the references (i.e., S3 URLs) instead.</p> </li> </ul> <p>Here are our recommendations:</p> <ul> <li> <p>PostgreSQL is the right choice most of the time, thanks to the support of unstructured JSON.</p> </li> <li> <p>SQLite is perfectly good for small projects.</p> </li> <li> <p>\u201cNoSQL\u201d was a big craze in the 2010s (like MongoDB). However, they are not as fast as the relational database and also have consistency issues frequently.</p> </li> <li> <p>Redis is handy when you need a simple key-value store.</p> </li> </ul>"},{"location":"spring2021/lecture-8/#data-warehouse","title":"Data Warehouse","text":"<p>A data warehouse is a structured aggregation of data for analysis, known as online analytical processing (OLAP).</p> <p></p> <p>Another acronym that you might have heard of is ETL (Extract, Transform, Load). The idea here is to extract data from data sources, transform the data into a common schema, and load the schema into the data warehouse. You can load the subset from the warehouse that you need and generate reports or run analytical queries. Well-known enterprise options in the market are Google BigQuery, Amazon Redshift, and Snowflake.</p>"},{"location":"spring2021/lecture-8/#sql-and-dataframes","title":"SQL and DataFrames","text":"<p>Most data solutions use SQL as the interface to the data, except for some (like Databricks) that use DataFrames. SQL is the standard interface for structured data. But in the Python ecosystem, Pandas is the main DataFrame. Our advice is to become fluent in both.</p>"},{"location":"spring2021/lecture-8/#data-lake","title":"Data Lake","text":"<p>A data lake is the unstructured aggregation of data from multiple sources (databases, logs, expensive data transformations). It operates under the concept of ELT (Extract, Load, Transform) by dumping everything in the lake and transforming the data for specific needs later.</p> <p></p>"},{"location":"spring2021/lecture-8/#data-lakehouse","title":"Data \u201cLakehouse\u201d","text":"<p>The current trend in the field is to combine data warehouses and data lakes in the same suite. The Databricks Lakehouse Platform is both a warehouse and a lake, operated as an open-source project called Delta Lake. You can store both structured and unstructured data in the platform and use them for analytics workloads and machine learning engines.</p>"},{"location":"spring2021/lecture-8/#what-goes-where","title":"What Goes Where?","text":"<ul> <li> <p>Binary data (images, sound files, compressed texts) are stored as objects.</p> </li> <li> <p>Metadata (labels, user activity) is stored in a database.</p> </li> <li> <p>If we need features that are not obtainable from the database (logs), we would want to set up a data lake and a process to aggregate the data required.</p> </li> <li> <p>At training time, we need to copy the necessary data to the filesystem on a fast drive.</p> </li> </ul> <p></p> <p>A lot is going on within the data management tooling and infrastructure. We recommend looking at a16z\u2019s \u201cEmerging Architectures For Modern Data Infrastructure\u201d article to get a broad look into this ecosystem.</p> <p>A highly recommended resource is Martin Kleppmann\u2019s book \u201cDesigning Data-Intensive Applications,\u201d\u200a \u200awhich provides excellent coverage of tools and approaches to build reliable, scalable, and maintainable data storage systems.</p>"},{"location":"spring2021/lecture-8/#4-data-processing","title":"4 - Data Processing","text":""},{"location":"spring2021/lecture-8/#data-dependencies","title":"Data Dependencies","text":"<p>Let\u2019s look at a motivational example of training a photo popularity predictor every night. For each photo, the training data must include these components:</p> <ul> <li> <p>Metadata (such as posting time, title, location) that is in the database.</p> </li> <li> <p>Some features of the user (such as how many times they logged in today) that need to be computed from logs.</p> </li> <li> <p>Outputs of photo classifiers (such as content, style) that can be obtained after running the classifiers.</p> </li> </ul> <p>The idea is that we have different sources of data, and they have different dependencies. The big hurdle here is that some tasks can\u2019t be started until other tasks are finished. Finishing a task should \u201ckick-off\u201d its dependencies.</p> <p>The simplest thing we can do is a \u201cMakefile\u201d to specify what action(s) depend on. But here are some limitations to this approach:</p> <ul> <li> <p>What if re-computation needs to depend on content, not on a date?</p> </li> <li> <p>What if the dependencies are not files but disparate programs and databases?</p> </li> <li> <p>What if the work needs to be spread over multiple machines?</p> </li> <li> <p>What if many dependency graphs are executing all at once, with shared dependencies?</p> </li> </ul>"},{"location":"spring2021/lecture-8/#mapreduce","title":"MapReduce","text":"<p>The old-school big data solutions to this are Hadoop and Apache Spark. These are MapReduce implementations, where you launch different tasks that each take a bit of the data (Map) and reduce their outputs into a single output (Reduce). Both Hadoop and Spark can run data processing operations and simple ML models on commodity hardware, with tricks to speed things up.</p> <p>In the modern environment, you can\u2019t run an ML model (in PyTorch or TensorFlow) as part of running a Spark job (unless that model itself is programmed in Spark). That\u2019s when you need a workflow management system like Apache Airflow.</p>"},{"location":"spring2021/lecture-8/#dag","title":"DAG","text":"<p>In Airflow, a workflow is defined as a collection of tasks with directional dependencies, basically a directed acyclic graph (DAG). Each node in the graph is a task, and the edges define dependencies among the tasks. Tasks belong to two categories: (1) operators that execute some operation and (2) sensors that check for the state of a process or a data structure.</p> <p></p> <p>The main components of Airflow include: (1) a metadata database that stores the state of tasks and workflows, (2) a scheduler that uses the DAGs definitions together with the state of tasks in the metadata database to decide what needs to be executed, and (3) an executor that determines which worker will execute each task.</p> <p>Besides Airflow, here are other notable solutions:</p> <ul> <li> <p>Apache Beam: The TensorFlow team uses Apache Beam to generate big datasets and run those processing steps on Google Cloud Dataflow (a cloud orchestrator).</p> </li> <li> <p>Prefect: A similar idea to Airflow, Prefect is a Python framework that makes it easy to combine tasks into workflows, then deploy, schedule, and monitor their execution through the Prefect UI or API.</p> </li> <li> <p>dbt: dbt provides this data processing ability in SQL (called \u201canalytics engineering.\u201d)</p> </li> <li> <p>Dagster: Dagster is another data orchestrator for ML, analytics, and ETL. You can test locally and run anywhere with a unified view of data pipelines and assets.</p> </li> </ul>"},{"location":"spring2021/lecture-8/#5-feature-store","title":"5 - Feature Store","text":"<p>Feature stores were first popularized by the ML team at Uber as part of their Michelangelo platform. Traditionally, ML systems are divided into two portions, offline processing and online processing.</p> <ul> <li> <p>For the initial work of modeling, data that is generally static, perhaps stored in a data lake. Using some preprocessing methods (usually in SQL or Spark), data, which could be logfiles, requests, etc., are converted into features used to develop and train the model. The end result of this process is a model trained on a static sample of the data. This is an offline process.</p> </li> <li> <p>In contrast, the process of performing inference (e.g., Uber\u2019s need to return ride prices in real-time) often works with real-time data in an online process fashion. From a technology standpoint, whereas the offline use case might involve a data lake and Spark/SQL, the online processing use case involves technologies like Kafka and Cassandra that support speedier processing of creating or accessing the features required to perform inference.</p> </li> </ul> <p></p> <p>This difference in how features need to be created and accessed is a natural place for bugs to crop up. Harmonization of the online and offline processes would reduce bugs, so the Uber team, amongst others, introduced the concept of features stores to do just that. Members of the Uber team developed Tecton, a feature store company, which is one option to implement this system. An open-source alternative is Feast. To summarize, Tecton offers a handy definition of what a feature store is: \u201can ML-specific data system that runs data pipelines that transform raw data into feature values, stores and manages the feature data itself, and serves feature data consistently for training and inference purposes.\u201d</p> <p>A word of caution: don\u2019t over-engineer your system according to what others are doing. It\u2019s easy to wrap yourself up in adopting many tools and systems that aren\u2019t as optimal as their publicity may make them seem. Work with the tools you have first! For an interesting example of this, look at how \u201ccommand-line tools can be 235x faster than your Hadoop cluster\u201d.</p>"},{"location":"spring2021/lecture-8/#6-data-exploration","title":"6 - Data Exploration","text":"<p>The objective of data exploration is to understand and visualize the nature of the data you\u2019re modeling.</p> <ul> <li> <p>Pandas is the Python workhorse of data visualization. It\u2019s highly recommended to be familiar with it.</p> </li> <li> <p>Dask is an alternative that can speed up data processing for large datasets that Pandas cannot handle through parallelization.</p> </li> <li> <p>Similarly, RAPIDS speeds up large dataset processing, though it does through the use of GPUs.</p> </li> </ul>"},{"location":"spring2021/lecture-8/#7-data-labeling","title":"7 - Data Labeling","text":"<p>Effective data labeling is a core ingredient of production machine learning systems. Most data labeling platforms have a standard set of features: the ability to generate bounding boxes, segmentations, key points, class assignments, etc. The crucial objective is agreeing on what makes a good annotation and training annotators accordingly. To avoid annotator error cropping up, write clear guidelines that clarify rules for edge cases and high-quality annotations. One way to acquire the material needed to write such a guide is to start by annotating yourself. As you generate labels, ensure the quality of the annotations holds up across the annotator base. Some participants will be more reliable than others.</p> <p></p> <p>To develop an annotator base, there are a few options.</p>"},{"location":"spring2021/lecture-8/#sources-of-labor","title":"Sources of Labor","text":"<ul> <li> <p>One option is to hire your own annotators, which can help with the speed and quality of annotations. This, however, can be expensive and difficult to scale.</p> </li> <li> <p>Another option is to crowdsource labels via a platform like Amazon Mechanical Turk, which is fast and cheap to set up, but for which the quality can be poorer.</p> </li> <li> <p>\u2026or full-service data labeling companies.</p> </li> </ul>"},{"location":"spring2021/lecture-8/#service-companies","title":"Service Companies","text":"<p>There are entire service companies that focus on data labeling that you can hire. Hiring such a company makes a great deal of sense, considering the time, labor, and software investment needed to label well at scale. To figure out the best data labeling company, start by annotating some gold standard data yourself. Then, contact and evaluate several companies on their value and a sample labeling task. Some companies in this space are FigureEight, Scale.ai, Labelbox, and Supervisely.</p>"},{"location":"spring2021/lecture-8/#software","title":"Software","text":"<p>If the costs of a full-service data labeling company are prohibitive, pure-play labeling software can be an option.</p> <ul> <li> <p>Label Studio is a friendly open-source platform for this. New concepts to make labeling more strategic and efficient are coming to the fore.</p> </li> <li> <p>Aquarium helps you explore your data extensively and map the appropriate labeling strategy for classes that may be less prevalent or performant.</p> </li> <li> <p>Snorkel.ai offers a platform that incorporates weak supervision, which automatically labels data points based on heuristics and human feedback.</p> </li> </ul> <p>In summary, if you can afford not to label, don\u2019t; get a full-service company to take care of it. Failing that, try to use existing software and a part-time annotator base work (in lieu of a crowdsourced workforce).</p>"},{"location":"spring2021/lecture-8/#8-data-versioning","title":"8 - Data Versioning","text":"<p>Data versioning is important because machine learning models are part code and part data. If the data isn\u2019t versioned, the system isn\u2019t fully versioned! There are four levels to data versioning, which is similar to code versioning:</p> <p>Level 0: No versioning.</p> <ul> <li> <p>All data lives on a filesystem, in S3, and/or in a database.</p> </li> <li> <p>The problem arises most acutely in this paradigm, as deployed ML systems (whose code may be versioned) can quickly become divorced from their corresponding data.</p> </li> <li> <p>Furthermore, reverting to older versions will be challenging.</p> </li> </ul> <p>Level 1: Storing a snapshot of everything at training time.</p> <ul> <li> <p>This works and can help you revert, but it\u2019s very hacky.</p> </li> <li> <p>Rather than doing this entire process manually, let\u2019s try to version automatically.</p> </li> </ul> <p>Level 2: Versioned as a mix of assets and code.</p> <ul> <li> <p>You store the large files with unique IDs in S3, with corresponding reference JSON versioned with code.</p> </li> <li> <p>You should avoid storing the data directly in the repository, as the metadata itself can get pretty large. Using git-lfs lets you store them just as easily as code.</p> </li> <li> <p>The git signature + of the raw data file fully defines a model\u2019s data and code.</p> </li> </ul> <p>Level 3: Specialized solutions for version data.</p> <ul> <li> <p>You should avoid them until you can identify their unique value add to your project.</p> </li> <li> <p>Some options here are DVC are Pachyderm. DVC has a Git-like workflow worth taking a closer look at. Dolt versions databases, if that\u2019s your need.</p> </li> </ul> <p></p>"},{"location":"spring2021/lecture-8/#9-data-privacy","title":"9 - Data Privacy","text":"<p>Increasingly, unfettered access to data for machine learning is less desirable and prevalent. This is especially true in regulated industries like healthcare and finance. To address such challenges, researchers are developing new data privacy techniques.</p> <ul> <li> <p>Federated learning trains a global model on several local devices without ever acquiring global access to the data. Federated learning is still research-use only due to these issues: (1) sending model updates can be expensive, (2) the depth of anonymization is not clear, and (3) system heterogeneity when it comes to training is unacceptably high.</p> </li> <li> <p>Another research area is differential privacy, which tries to aggregate data in ways that prevent identification. Finally, learning on encrypted data has potential. Most data privacy efforts are research-focused, as the tooling is not yet mature.</p> </li> </ul>"},{"location":"spring2021/lecture-9/","title":"Lecture 9: AI Ethics","text":""},{"location":"spring2021/lecture-9/#video","title":"Video","text":""},{"location":"spring2021/lecture-9/#slides","title":"Slides","text":"<p>Download slides as PDF</p>"},{"location":"spring2021/lecture-9/#notes","title":"Notes","text":"<p>Lecture by Sergey Karayev. Notes transcribed by James Le and Vishnu Rachakonda.</p> <p>A preamble: Ethics is a vast subject spanning many disciplines and addressing many real different problems. As ML practitioners, we need to have a student mindset and do not assume we have the answers because these are not easy problems.</p>"},{"location":"spring2021/lecture-9/#1-what-is-ethics","title":"1 - What is Ethics?","text":"<p>Let\u2019s start with the definition of ethics:</p> <ul> <li> <p>Ethics are not feelings because your feelings might mislead you.</p> </li> <li> <p>Ethics are not laws because ethics can supersede laws.</p> </li> <li> <p>Ethics are not societal beliefs because even an immoral society has its set of ethics.</p> </li> </ul>"},{"location":"spring2021/lecture-9/#ethical-theories","title":"Ethical Theories","text":"<p>Kevin Binz put together a tour of ethical theories, including:</p> <ul> <li> <p>The divine command theory states that a behavior is moral if the divine commands it. This theory might be accurate, but philosophy doesn\u2019t engage with it.</p> </li> <li> <p>The virtue ethics theory states that a behavior is moral if it upholds a person\u2019s virtues (bravery, generosity, love, etc.). This theory is apparently robust to philosophical inquiry, but there is increasing evidence that virtues are not persistent across a person\u2019s life and somewhat illusory.</p> </li> <li> <p>The deontology (duty-based) theory states that a behavior is moral if it satisfies the categorical imperative (i.e., don\u2019t lie, don\u2019t kill). This theory might lead to counter-intuitive moral decisions in many situations and has unacceptable inflexibility to many people.</p> </li> <li> <p>The utilitarian theory states that a behavior is moral if it brings the most good to the most people. But of course, how do we measure utility?</p> </li> </ul> <p>There does not seem to be a clear winner among professional philosophers. From this survey, there appears to be an even split between virtue, deontology, and utilitarianism.</p>"},{"location":"spring2021/lecture-9/#the-trolley-problem","title":"The Trolley Problem","text":"<p>The \u201ctrolley problem\u201d is often used to gain intuition about a person\u2019s ethics by presenting to him/her a moral dilemma. The classic dilemma is that: You see a trolley that is about to run over five people. But you can divert it to run over only one person. Would you do it? It actually leads to a lot of good memes. \ud83e\udd23</p> <p></p> <p>Another prominent ethical theory is John Rawl\u2019s theory of justice. Rawls argued that equal distribution of resources should be the desirable state of nature instead of following utilitarian philosophies. A Theory of Justice holds that every individual has an equal right to basic liberties. They should have the right to opportunities and an equal chance as other individuals of similar ability.</p> <p>When ethics are applied to technology, it\u2019s essential to understand that they are not static and change with technological progress. Some examples include the industrial revolution, the right to Internet access, birth control, surrogate pregnancy, embryo selection, artificial womb, lab-grown meat, and much more. An excellent book to explore is Juan Enriquez\u2019s \u201cRight/Wrong: How Technology Transforms Our Ethics.\u201d</p>"},{"location":"spring2021/lecture-9/#2-long-term-ai-ethical-problems","title":"2 - Long-Term AI Ethical Problems","text":""},{"location":"spring2021/lecture-9/#autonomous-weapons","title":"Autonomous Weapons","text":"<p>The first example that came to a lot of people\u2019s minds is autonomous weapons. It might be tempting to dismiss it as far-fetched and unrealistic at this time. But as the saying goes, \u201cthe future is already here, just not evenly distributed\u201d:</p> <ul> <li> <p>Israel apparently has autonomous \u2018robo-snipers\u2019 on its borders today.</p> </li> <li> <p>NYPD has been deploying Boston Dynamics robots in crime situations.</p> </li> </ul>"},{"location":"spring2021/lecture-9/#lost-human-labor","title":"Lost Human Labor","text":"<p>Replacing human labor is another concern that has been creeping upon us. With the pandemic, you probably saw many articles saying that millions of people have lost jobs and probably will never get them back (replaced by AI). This could be both good and bad. \ud83e\udd14</p> <p>It\u2019s bad if there are no social safety net and no other jobs for the unemployed. It\u2019s good because there is a megatrend of the demographic inversion. As the world\u2019s population tops out and baby booms vary across regions, the economy can\u2019t function as currently designed. Therefore, we need labor from somewhere. Rodney Brooks, a roboticist from MIT and the founder of iRobot, advocates for having robots in order to have a functioning economy in the next few decades.</p> <p>An interesting spin on this worry is that AI is not necessarily replacing human labor but controlling human labor. This article from The Verge provides more details about working in conditions in warehouses, call centers, and other sectors.</p> <p>If you want to go down the rabbit hole, check out this series \u201cManna - Two Views of Humanity\u2019s Future\u201d from Marshall Brain.</p>"},{"location":"spring2021/lecture-9/#human-extinction","title":"Human Extinction","text":"<p>The final worry is that if AI is superintelligent, then it is capable of replacing humans entirely.</p>"},{"location":"spring2021/lecture-9/#the-alignment-problem","title":"The Alignment Problem","text":"<p>What\u2019s common in all these long-term problems is the alignment problem. This notion is often expressed by the parable of the \u201cpaperclip maximizer\u201d - given the goal of producing paperclips, an AGI will eventually turn every atom space into paperclips. This is an old lesson about how to establish and communicate our goals and values to technologies precisely.</p> <p>The guiding principle to build safe AI is that the AI systems we build need to be aligned with our goals and values. This is a deep topic and active research area in many places (including CHAI at Berkeley). As a matter of fact, this alignment lens is useful for near-term problems as well, as discussed in the rest of the lecture.</p> <p></p>"},{"location":"spring2021/lecture-9/#3-hiring","title":"3 - Hiring","text":"<p>Let\u2019s say we are building an ML model to predict hiring decisions given a resume (inspired by this Reuters article about Amazon\u2019s hiring algorithm).</p> <p>What should the data contain? Should it be the hiring decision that was made? Or should it be the eventual job performance given the person that was hired?</p> <p>The data comes from the world, which is known to be biased in many ways: the hiring pipeline (not enough women educated for a software engineering job), the hiring decisions (employers intentionally or unintentionally select people that match some prejudice), the performance ratings (people get promoted not because they are good of their job, but because they match other expectations of the promoter).</p> <p>Because the world is biased, the data will be biased no matter how we structure the data. Therefore, the model trained on that data will be biased.</p> <p>The model will be used to aid or make an action: sourcing candidates, double-checking human decisions, or making the actual hiring decisions? In the last case, that action will amplify existing biases.</p> <p>Amplifying existing biases is not aligned with our goals and values!\ud83d\ude20</p> <p></p>"},{"location":"spring2021/lecture-9/#4-fairness","title":"4 - Fairness","text":""},{"location":"spring2021/lecture-9/#compas","title":"COMPAS","text":"<p>Let\u2019s look at a case study about COMPAS - Correctional Offender Management Profiling for Alternative Sanctions system to discuss fairness.</p> <ul> <li> <p>The goal of this system is to predict recidivism (committing another crime), such that judges can consult a 1-10 score in pre-trial sentencing decisions.</p> </li> <li> <p>The motivation of this system is to be less biased than humans because the criminal justice system is notoriously biased against certain races.</p> </li> <li> <p>The solution of this system is to (1) gather relevant data, (2) exclude protected class attributes (race, gender, age, etc.), (3) train the model by ensuring that the model\u2019s score corresponds to the same probability of recidivism across all demographic groups.</p> </li> </ul> <p>And yet, this famous ProPublica report exposes the bias of this system against blacks.</p>"},{"location":"spring2021/lecture-9/#fairness-definitions-from-aravind-narayanans-lecture","title":"Fairness Definitions (From Aravind Narayanan\u2019s Lecture)","text":"<p>There are a bunch of fairness definitions. The first one concerns bias. We often mean statistical bias in machine learning - the difference between the model\u2019s expected value and the true value.</p> <ul> <li> <p>In this sense, the COMPAS scores are not biased with respect to re-arrest. This is an important caveat; because we only have data for arrests, not crimes committed. There may well be bias in arrests (the data-generating process).</p> </li> <li> <p>Even if COMPAS is free of statistical bias, is it an adequate fairness criterion? Is this criterion aligned with human values?</p> </li> </ul> <p></p> <p>Taking a step back and look at the classic binary classification problem setup, we have the confusion matrix as seen above. The interesting question to ask is what do different stakeholders want from the classifier?</p> <ul> <li> <p>The decision-maker (the judge or the prosecutor) asks: \u201cOf those that I have labeled high risk, how many recidivated?\u201d This corresponds to the model\u2019s predictive value = TP / (TP + FP).</p> </li> <li> <p>The defendant asks: \u201cWhat is the probability I\u2019ll be incorrectly classified as high risk?\u201d This corresponds to the model\u2019s false positive rate = FP / (FP + FN).</p> </li> <li> <p>The society at large might care about: \u201cIs the selected set demographically balanced?\u201d This could be demographic parity, which leads to the definition of group fairness (\u201cDo outcomes differ between groups, which we have no reason to believe are actually different?\u201d).</p> </li> </ul> <p>A lot of these group fairness metrics have natural motivations, so there\u2019s not a single correct fairness definition. They depend on the politics of the situation.</p> <p>Let\u2019s forget about demographic parity and only pick the two most important metrics (false-positive rate and false-negative rate) while allowing the model to use protected class attributes. We fail the individual fairness definition, which uses a single threshold for the sentencing decision or the pre-sentencing release decision.</p> <p>Even if we pick one metric to optimize for, we still sacrifice some utility (providing public safety or releasing too few defendants).</p> <p>To build more intuition, you should play around with this interactive demo on attacking discrimination with smarter ML from Google Research.</p> <p>Finally, ML can be very good at finding patterns that maybe humans can\u2019t find. For instance, your ZIP code and age might be highly correlated with your race. That means the model can always pick up from a protected class attribute from other attributes. Read this paper on Equality of Opportunity in Supervised Learning for more detail.</p>"},{"location":"spring2021/lecture-9/#tradeoffs","title":"Tradeoffs","text":"<p>There are tradeoffs between different measures of group fairness, between the definitions of group fairness and individual fairness, and between the notions of fairness and utility. In fact, these tradeoffs are not specific to machine learning. They apply to human decision making too. There is also a tension between disparate treatment and disparate impact, which is another deep subject.</p>"},{"location":"spring2021/lecture-9/#seeing-the-water","title":"Seeing The Water","text":"<p>In order to see the water, it would be noteworthy to think about the differences between environmental equity and environmental justice:</p> <ul> <li> <p>Equality: The assumption is that everyone benefits from the same supports. This is equal treatment.</p> </li> <li> <p>Equity: Everyone gets the support they need (\u201caffirmative action\u201d), thus producing equity.</p> </li> <li> <p>Justice: All parties are supported equally because the cause of the inequity was addressed. The systematic barrier has been removed.</p> </li> </ul> <p>The justice mindset is valuable to have. As computer scientists, we have very literal minds and argue for the rationality of our choices. But taking a step back and seeing the whole situation would be even more crucial.</p>"},{"location":"spring2021/lecture-9/#5-representation","title":"5 - Representation","text":""},{"location":"spring2021/lecture-9/#the-problem","title":"The Problem","text":"<p>Watch this simple video: a hand sanitizer dispenser that doesn\u2019t recognize racially diverse hands. It\u2019s a small example but illustrates a big problem: a lack of attention to diverse representation in the development of technology products. This occurs across fields, such as drug development, photography, etc. As pointed out by Timnit Gebru in this New York Times article, the exclusion of people from certain backgrounds poses a serious long-term threat to the viability of ML systems.</p> <p>One way to address this challenge head-on is to focus on the inclusion of people from all backgrounds. Groups like Black in AI, Women in Machine Learning, and Latinx in AI play a big role in building communities of underrepresented people and inviting them into the AI/ML industry. Another is to deliberately ensure products reflect inclusive values. For example, Google Images now yields a diverse set of images for the search term \u201cCEO\u201d whereas it used to return entirely white, middle-aged men.</p>"},{"location":"spring2021/lecture-9/#word-embeddings","title":"Word Embeddings","text":"<p>A particularly relevant example of bias in machine learning is the underlying bias in the Word2Vec model. Word2Vec introduced vector math for word embeddings and is frequently used for NLP applications. The original model was trained on a large corpus, and the weights were open-sourced. As these weights were examined, underlying bias in the word logic was discovered. Terms like \u201cdoctor\u201d and \u201cprogrammer\u201d were associated with men, while \u201chomemaker\u201d and \u201cnurse\u201d were associated with women. Translating our existing biases like these into the ML domain is undesirable, to say the least! \ud83d\ude29</p> <p></p> <p>One potential solution to address this problem is to de-bias at training time with carefully screened data. With newer models like GPT-3 that are trained on massive swathes of data, this can be hard to do in practice. Bender and Gebru advise in a 2021 paper to reduce the dependence on large, unfiltered datasets and more carefully document the data-generating process. Alternatively, you can alert the user proactively of potential bias. Addressing this problem of bias in language models is an open problem.</p>"},{"location":"spring2021/lecture-9/#seeing-the-water_1","title":"Seeing The Water","text":"<p>Part of the challenge lies in agreeing on whether the model should learn about the world as it is in the data or learn about the world in a more idealistic manner. This is application-specific. A model recognizing hate speech on Facebook should probably learn about the world as it is, or a model interacting with humans\u2019 conversations should adhere to proper ideals. Of course, this begs the question of who decides what ideals are desirable and suitable for a model to follow. Consider these questions as you build models for various applications.</p> <p>Ultimately, these challenges in machine learning systems development are rooted in ethics. Face recognition is a boundary-breaking area that has been grappling with ethical concerns. Importantly, face recognition illustrates how technology can impact ethics and change standards. Is the loss of privacy associated with face recognition desirable? Relatedly, are face recognition systems performing well across groups? The question of performance should generally follow ethics to avoid distracting from the fundamental ethical issues (e.g., civil rights, privacy, etc.).</p>"},{"location":"spring2021/lecture-9/#6-best-practices","title":"6 - Best Practices","text":"<p>A recent survey of ML practitioners found these to be the top challenges in ensuring fairness that they face:</p> <ol> <li> <p>Receiving support in fairness-aware data collection and curation</p> </li> <li> <p>Overcoming team\u2019s blind spots</p> </li> <li> <p>Implementing more proactive fairness auditing processes</p> </li> <li> <p>Auditing complex ML systems</p> </li> <li> <p>Deciding how to address particular instances of unfairness</p> </li> <li> <p>Addressing biases in the humans embedded throughout the ML development pipeline</p> </li> </ol>"},{"location":"spring2021/lecture-9/#suggestions","title":"Suggestions","text":"<p>Rachel Thomas, the co-creator of Fast.ai, has some great ideas on how to confront fairness issues proactively:</p> <ol> <li> <p>Perform ethical risk sweeping. Akin to cybersecurity penetration testing, where engineers intentionally try to find faults, you can try to engage in regular fairness checks on behalf of different stakeholders.</p> </li> <li> <p>Expand the ethical circle. Try to consider different perspectives than yours regularly, and invite such people into your decision-making \u201ccircle\u201d to ensure that systems do not lead to unfair outcomes.</p> </li> <li> <p>Think about worst-case scenarios. What incentives may crop up for people to engage in unethical behavior? For example, the upvote-downvote system and recommendations on Reddit can cause toxic behavior. Think about such incentives and requisite safeguards in advance.</p> </li> <li> <p>Close the loop! You have to put in place a process to keep improving, as fairness is not a static test (just like raw performance).</p> </li> </ol> <p>One powerful tool, proposed by Gebru and Mitchell in 2018, is adopting \u201cmodel cards.\u201d For every ML model, make a simple page that discusses the expectations (i.e., input/output), tradeoffs, performance, and known limitations. Engaging in this documentation exercise allows for teams to confront fairness issues head-on more effectively. The objective here is to get everyone on the same page about what the model can and cannot do from a fairness perspective. We believe everyone should do this, considering how easy it is. Other methods like bias audits are also useful, as the Aequitas team at UChicago shows.</p> <p></p>"},{"location":"spring2021/lecture-9/#a-code-of-ethics","title":"A Code of Ethics?","text":"<p>AI is a reflection of society. It\u2019s impossible to expect AI to be completely unbiased when humans still struggle with the problem. However, we can try our best to ensure that these biases are not amplified by AI and mitigate any such damage. Making fairness and ethics a routine part of AI development by professionals and teams is crucial to addressing the challenge. Perhaps an AI code of ethics (akin to the Hippocratic Oath) would make sense!</p>"},{"location":"spring2021/lecture-9/#7-where-to-learn-more","title":"7 - Where To Learn More","text":"<p>Here are some links to learn more:</p> <ol> <li>https://ethics.fast.ai/: a course by the fast.ai team on practical data ethics consisting of 6 lectures.</li> <li>CS 294: Fairness in Machine Learning: A graduate course (similar to FSDL) taught at Berkeley in 2017 about AI ethics.</li> <li>Fair ML Book: A book being written by the instructor of the aforementioned course on fair ML.</li> <li>KDD Tutorial on Fair ML: Taught by folks from CMU, this is a workshop addressing some of the topics in this lecture.</li> <li>The Alignment Problem: a book that confronts present-day issues in AI alignment.</li> <li>Weapons of Math Destruction: a popular book about current issues like Facebook\u2019s News Feed.</li> </ol>"},{"location":"spring2021/notebook-1/","title":"Notebook: Coding a neural net","text":""},{"location":"spring2021/notebook-1/#video","title":"Video","text":"<p>Notebook by Sergey Karayev.</p> <p>In this video, we code a neural network from scratch. You'll get familiar with the Google Colab environment, create a simple linear regression model using only Numpy, and build a multi-layer perception regression model using NumPy, PyTorch, and Keras.</p> <ul> <li>0:30\u200b - Colab Notebook 101</li> <li>5:30\u200b - Numerical computing via NumPy</li> <li>10:15\u200b - Plotting via Matplotlib</li> <li>11:33\u200b - Basic regression with a linear model</li> <li>24:30\u200b - Basic regression with a multi-layer perceptron</li> </ul>"},{"location":"spring2021/notebook-1/#follow-along","title":"Follow Along","text":"<p>Google Colab</p>"},{"location":"spring2021/panel/","title":"Panel Discussion: Do I need a PhD to work in ML?","text":"<p>We gathered a few people to offer different perspectives on whether grad school is required to work on interesting ML problems.</p> <p>Moderated by Sergey Karayev.</p> <p>The panelists:</p> <ul> <li>Pieter Abbeel - Professor at UC Berkeley</li> <li>Georgia Gkioxari - Research Scientist at Facebook, formerly PhD Berkeley</li> <li>Peter Gao - Co-founder and CEO of Aquarium Learning, formerly Cruise Automation</li> <li>Anil Jason - Co-founder and CTO of Quillbot</li> </ul>"},{"location":"spring2021/panel/#video","title":"Video","text":""},{"location":"spring2021/projects/","title":"Course Projects Showcase","text":"<p>The final project is the most important as well as the most fun part of the course. Students worked individually or in pairs over the duration of the course to complete a project involving any part of the full stack of deep learning.</p> <p>The Top 10 projects, as selected by our course TAs, we viewed together with everyone, and posted the video on YouTube.</p> <p>The details of these and other projects are below</p>"},{"location":"spring2021/projects/#artificial-manga-panel-dataset","title":"Artificial Manga Panel Dataset","text":"<ul> <li>By Aasim Sani</li> <li>GitHub Page</li> <li>Project Report</li> <li>Video Presentations: Part 1 and Part 2</li> </ul>"},{"location":"spring2021/projects/#breast-cancer-detection-assistant","title":"Breast Cancer Detection Assistant","text":"<ul> <li>By Harish Narayanan and Daniel Hen</li> <li>Project Report</li> <li>Source Code</li> <li>Video Presentation</li> <li>ML Video Presentation</li> </ul>"},{"location":"spring2021/projects/#human-protein-atlas-single-cell-classification","title":"Human Protein Atlas Single Cell Classification","text":"<ul> <li>By Dariusz Kleczek</li> <li>Report</li> <li>Video Presentation</li> </ul>"},{"location":"spring2021/projects/#a-case-study-on-weakly-supervised-learning","title":"A Case Study on Weakly Supervised Learning","text":"<ul> <li>By Jacques Thibodeau, Arian Pasquali, and Kevin Koehncke</li> <li>Project Report</li> <li>Source Code</li> <li>Video Presentation</li> </ul>"},{"location":"spring2021/projects/#real-time-nail-biting-alerter","title":"Real-Time Nail Biting Alerter","text":"<ul> <li>By Jean-Marie Prigent</li> <li>Netlify Site</li> <li>Source Code</li> <li>Video Presentation</li> </ul>"},{"location":"spring2021/projects/#youtube-highlighter-for-creators-and-their-supporters","title":"YouTube Highlighter for Creators and Their Supporters","text":"<ul> <li>By Keno Harada</li> <li>Slides</li> <li>Source Code</li> <li>Video Presentation</li> <li>Blog Post (in Japanese)</li> </ul>"},{"location":"spring2021/projects/#neural-rock","title":"Neural Rock","text":"<ul> <li>By Lukas Mosser and co.</li> <li>Source Code</li> <li>Video Presentation</li> </ul>"},{"location":"spring2021/projects/#unity-machine-learning-agents","title":"Unity Machine Learning Agents","text":"<ul> <li>By Paul Solomon</li> <li>Source Code</li> <li>Video Presentation</li> </ul>"},{"location":"spring2021/projects/#sitting-posture-coach","title":"Sitting Posture Coach","text":"<ul> <li>By Peter De Roovere</li> <li>Source Code</li> <li>Video Presentation</li> </ul>"},{"location":"spring2021/projects/#intel-scene-classification","title":"Intel Scene Classification","text":"<ul> <li>By Thomas Paula and Jonathan Salfity</li> <li>Source Code</li> <li>Report</li> <li>Video Presentation</li> </ul>"},{"location":"spring2021/projects/#bird-song-classifier","title":"Bird Song Classifier","text":"<ul> <li>By Wendy Mak</li> <li>Report</li> <li>Streamlit App</li> </ul>"},{"location":"spring2021/projects/#detecting-deforestation-from-satellite-images","title":"Detecting Deforestation from Satellite Images","text":"<ul> <li>By Andre Ferreira and Karthik Bhaskar</li> <li>Video</li> <li>Report</li> <li>Code</li> </ul>"},{"location":"spring2021/projects/#active-learning-a-systematic-investigation","title":"Active Learning: A Systematic Investigation","text":"<ul> <li>By Matthias Pfenninger, Stefan Josef, and Ravindra Bharathi</li> <li>Video</li> <li>Report</li> <li>Code</li> </ul>"},{"location":"spring2021/projects/#drought-watch","title":"Drought Watch","text":"<ul> <li>By Sambhavi Dhanabalan</li> <li>Video: Part 1 and Part 2</li> <li>Report</li> <li>Code</li> </ul>"},{"location":"spring2021/synchronous/","title":"Synchronous Online Course","text":"<p>We offered a paid cohort experience with the following additions to the lecture and lab materials released publicly:</p> <ul> <li>Slack workspace for learners, instructors, and teaching assistants</li> <li>Weekly graded assignment</li> <li>Capstone project reviewed by peers and staff</li> <li>Certificate of completion</li> </ul> <p>Check out the original announcement page.</p>"},{"location":"spring2021/synchronous/#how-do-i-know-if-i-am-in-this-course","title":"How do I know if I am in this course?","text":"<p>If you registered and received an email receipt from Stripe, you're in, and should have been added to our Slack workspace on February 1.</p> <p>Please email us if you have a Stripe receipt but aren't in our Slack.</p>"},{"location":"spring2021/synchronous/#teaching-assistants","title":"Teaching Assistants","text":"<p>This course is only possible with the support of our amazing TAs:</p> <ul> <li>Head TA James Le runs Data Relations for Superb AI and contributes to Data Journalism for Snorkel AI, after getting an MS in Recommendation Systems at RIT.</li> <li>Daniel Cooper is a machine learning engineer at QuantumWork, SaaS for recruiters.</li> <li>Han Lee is a Senior Data Scientist at WalletHub. Prior to that, he worked on various DS, MLE, and quant roles. Previously, he co-managed TEFQX.</li> <li>Nadia Ahmed is a machine learning researcher with The Frontier Development Lab and Trillium Technologies in remote sensing for severe weather and flood events.</li> <li>Andrew Mendez is a Senior Machine Learning Engineer at Clarifai, developing large scale computer vision and machine learning systems for the public sector. Previously he was a ML Engineer at CACI.</li> <li>Vishnu Rachakonda is a Machine Learning Engineer at Tesseract Health, a retinal imaging company, where he builds machine learning models for workflow augmentation and diagnostics in on-device and cloud use cases.</li> <li>Chester Chen is the Director of Data Science Engineering at GoPro. He also founded the SF Big Analytics Meetup.</li> </ul>"},{"location":"spring2021/synchronous/#schedule","title":"Schedule","text":"<p>While we post lectures once a week starting February 1, the first four weeks are review lectures -- stuff you should already know from other courses.</p> <p>On March 1, we get to the Full Stack content, and you will begin doing weekly assignments, discussing in Slack, and thinking about their course project.</p>"},{"location":"spring2021/synchronous/#logistics","title":"Logistics","text":"<p>All learners, instructors, and TAs will be part of a Slack workspace. The Slack community is a crucial part of the course: a place to meet each other, post helpful links, share experiences, ask and answer questions.</p> <p>On Monday, we post the lecture and lab videos for you to watch. Post questions, ideas, articles in Slack as you view the materials.</p> <p>On Thursday, we go live on Zoom to discuss the posted questions and ideas. We have two 30-min slots: 9am and 6pm Pacific Time. We will send everyone a Google Calendar invite with the Zoom meeting information.</p> <p>You have until Friday night to finish the assignment via Gradescope, which will be graded by next Tuesday, so that you have prompt feedback.</p> <p>Labs are not graded and can be done on your own.</p>"},{"location":"spring2021/synchronous/#projects","title":"Projects","text":"<p>The final project is the most important as well as the most fun part of the course. You can pair up or work individually. The project can involve any part of the full stack of deep learning, and should take you roughly 40 hours per person, over 5 weeks.</p> <p>Projects will be presented as five-minute videos and associated reports, and open sourcing the code is highly encouraged. All projects will be posted for peer and staff review.</p> <p>The best projects will be awarded and publicized by Full Stack Deep Learning.</p> <p>If you want to find a partner, please post in the #spring2021-projects Slack channel with your idea or just that you're available to pair up.</p> <p>Project proposals are due on Gradescope a few weeks into the course.</p> <p>Please read more information about the projects.</p>"},{"location":"spring2021/synchronous/#certificate","title":"Certificate","text":"<p>Those who complete the assignments and project will receive a certificate that can, for example, be displayed on LinkedIn.</p>"},{"location":"spring2021/synchronous/#time-commitment","title":"Time Commitment","text":"<p>On average, expect to spend 5-10 hours per week on the course.</p>"}]}