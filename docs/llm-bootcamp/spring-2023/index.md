---
hide:
  - navigation
description: Learn best practices and tools for building LLM-powered apps
embed_image: https://fullstackdeeplearning.com/llm-bootcamp/opengraph.png
title: LLM Bootcamp - Spring 2023
---

<style>

  a.video-card:hover {
    filter: brightness(.5);
  }

  .logo--image {
    border-radius: 50%;
    border: 4px solid #4350af;
  }

</style>

# LLM Bootcamp - Spring 2023

<!--**Our goal is to get you 100% caught up to state-of-the-art and ready to build and deploy LLM apps, no matter what your level of experience with machine learning is.**-->

<!--
**Lectures**

- [âœ¨ Learn to Spell: Prompt Engineering and Other Magic](#learn-to-spell-prompt-engineering)
- [ðŸŽï¸ LLMOps: Deployment and Learning in Production](#llmops-deployment-and-learning-in-production)
- [ðŸ¤· UX for Language User Interfaces](#ux-for-language-user-interfaces)
- [ðŸ”¨ Augmented Language Models](#augmented-language-models)
- [ðŸš€ Launch an LLM App in One Hour](#launch-an-llm-app-in-one-hour)
- [ðŸ”® What's Next?](#whats-next)
- [ðŸ—¿ LLM Foundations](#llm-foundations)
- [ðŸ‘·â€â™‚ï¸ askFSDL Walkthrough](#askfsdl-walkthrough)
-->

## Lectures

<div class="grid-2" markdown>
<div markdown>
### [**Learn to Spell: Prompt Engineering and Other Magic**](prompt-engineering/)

<a href="prompt-engineering" class="video-card">
![Prompt Engineering Lecture Cover](prompt-engineering/cover.jpg)
</a>
<ul>
  <li> High-level intuition: prompting as casting magic spells
  <li> Tips and tricks for effective prompting: decomposition/chain-of-thought, self-criticism, ensembling
  <li> Gotchas: "few-shot learning" and tokenization
</ul>
</div>

<div markdown>
### [**LLMOps: Deployment and Learning in Production**](llmops/)
<a href="llmops" class="video-card">
  ![LLMOps Lecture Cover](llmops/cover.jpg)
</a>
<ul>
  <li> Comparing and evaluating open source and proprietary models
  <li> Iteration and prompt management
  <li> Applying test-driven-development and continuous integration to LLMs
</ul>
</div>

<div markdown>
### [**UX for Language User Interfaces**](ux-for-luis/)
<a href="ux-for-luis" class="video-card">
  ![UX for LUIs Lecture Cover](ux-for-luis/cover.jpg)
</a>
  <ul>
    <li>General principles for user-centered design
    <li>Emerging patterns in UX design for LUIs
    <li>UX case studies: GitHub Copilot and Bing Chat
  </ul>
</div>

<div markdown>
### [**Augmented Language Models**](augmented-language-models/)

<a href="augmented-language-models" class="video-card">
![](augmented-language-models/cover.jpg)
</a>
  <ul>
    <li> Augmenting language model inputs with external knowledge
    <li> Vector indices and embedding management systems
    <li> Augmenting language model outputs with external tools
  </ul>
</div>


<div markdown>
### [**Launch an LLM App in One Hour**](launch-an-llm-app-in-one-hour/)

<a href="launch-an-llm-app-in-one-hour" class="video-card">
![](launch-an-llm-app-in-one-hour/cover.jpg)
</a>
  <ul>
    <li> Why is now the right time to get into AI?
    <li> Techniques and tools for the tinkering and discovery phase: ChatGPT, LangChain, Colab
    <li> A simple stack for quickly launching augmented LLM applications
  </ul>
</div>


<div markdown>
### [**LLM Foundations**](llm-foundations/)

<a href="llm-foundations" class="video-card">
![](llm-foundations/cover.jpg)
</a>
  <ul>
    <li> Speed-run of ML fundamentals
    <li> The Transformer architecture
    <li> Notable LLMs and their datasets
  </ul>
</div>


<div markdown>
### [**Project Walkthrough: askFSDL**](askfsdl-walkthrough/)

<a href="launch-an-llm-app-in-one-hour" class="video-card">
![](askfsdl-walkthrough/cover.jpg)
</a>
  <ul markdown>
  - Walkthrough of a [GitHub repo](https://fsdl.me/askfsdl-github) for sourced Q&A with LLMs
  - Try the bot out [in our Discord](https://fsdl.me/join-discord-askfsdl)
  - Python project tooling, ETL/data processing, deployment on Modal, and monitoring with Gantry
  </ul>
</div>

<div markdown>
### [**What's Next?**](whats-next/)

<a href="whats-next" class="video-card">
![What's Next Lecture Cover](whats-next/cover.jpg)
</a>
  <ul>
    <li> Can we build general purpose robots using multimodal models?
    <li> Will models get bigger or smaller? Are we running out of data?
    <li> How close are we to AGI? Can we make it safe?
  </ul>
</div>

</div>

## Invited Talks

<div class="grid-2">
  <div class="person">
    <img src="/images/speaker-peter-welinder.jpg" class="person--image" height="160px" width="160px" loading="lazy" alt="Photo of Peter Welinder">
    <div><strong>Peter Welinder</strong> is VP of Product and Partnerships at OpenAI.</div>
  </div>
  <div class="person">
    <img src="/images/speaker-harrison-chase.jpg" class="person--image" height="160px" width="160px" loading="lazy" alt="Photo of Harrison Chase">
    <div><strong>Harrison Chase</strong> is the creator of LangChain.</div>
  </div>
  <div class="person">
    <img src="/images/speaker-richard-socher.jpg" class="person--image" height="160px" width="160px" loading="lazy" alt="Photo of Richard Socher">
    <div><strong>Richard Socher</strong> is the co-founder and CEO of you.com.</div>
  </div>
  <div class="person">
    <img src="/images/speaker-reza-shabani.jpg" class="person--image" height="160px" width="160px" loading="lazy" alt="Photo of Reza Shabani">
    <div><strong>Reza Shabani</strong> trains LLMs at repl.it.</div>
  </div>
</div>
