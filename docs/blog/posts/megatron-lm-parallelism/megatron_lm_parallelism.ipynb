{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xrsrke/fsdl-website/blob/main/docs/blog/posts/megatron-lm-parallelism/megatron_lm_parallelism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ4Zg6lqyvyr"
      },
      "source": [
        "# 0. Why?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMTBOuJa2ug2"
      },
      "source": [
        "Large language models are large.\n",
        "\n",
        "They are so large that even\n",
        "the latest and greatest hardware accelerators, like\n",
        "[NVIDIA's H100 GPU](https://lambdalabs.com/blog/nvidia-h100-gpu-deep-learning-performance-analysis),\n",
        "cannot fit all the calculations that transform\n",
        "input text into output text and compute the information\n",
        "used to make the model better during training.\n",
        "\n",
        "That makes training large language models a distributed programming problem,\n",
        "where the work of computing an output is split, or _distributed_,\n",
        "across multiple accelerators or machines.\n",
        "\n",
        "That's common enough for neural networks:\n",
        "it's actually pretty typical for training\n",
        "to require many GPUs,\n",
        "and the simple solution is to split work\n",
        "across data points, which\n",
        "[isn't so hard](https://lightning.ai/docs/pytorch/stable/accelerators/gpu_intermediate.html?highlight=distributed%20data%20parallel).\n",
        "\n",
        "But for the largest models,\n",
        "things are yet worse:\n",
        "you can't fit all the calculations\n",
        "for the outputs of _even a single layer_\n",
        "working on _even a single datapoint_\n",
        "on one accelerator.\n",
        "\n",
        "So training large language models requires\n",
        "multiple layers of parallelization.\n",
        "\n",
        "And despite the increases in scale\n",
        "and the rapid pace of innovation in language model applications\n",
        "in the past few years (and even months!),\n",
        "the best reference for understanding the fundamentals of how that problem\n",
        "is posed and solved is still a paper from 2020:\n",
        "[_Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism_](https://arxiv.org/abs/1909.08053).\n",
        "\n",
        "In this blog post/notebook,\n",
        "we'll walk through the main ideas in that paper.\n",
        "Our goal will be to build up an understanding of how to implement\n",
        "a Megatron-style linear layer.\n",
        "\n",
        "We'll use that understanding to implement\n",
        "`ColumnParallelLinear` in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlrZ_DhHm9LF"
      },
      "source": [
        "## 1. Three Nested Parallelizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocC1sKtm5aj3"
      },
      "source": [
        "### Data Parallelism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk-DM-MB5l1x"
      },
      "source": [
        "Parallelize by splitting up the data.\n",
        "\n",
        "Each worker gets a piece of a batch and is responsible for running their own replica of the model on it.\n",
        "\n",
        "Easy! Fun, even! Elements of a batch should have nothing to do with one another.\n",
        "\n",
        "Synchronization: share gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNtkervc5chV"
      },
      "source": [
        "### Pipeline Parallelism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up5S_KPl5oj8"
      },
      "source": [
        "Parallelize by splitting up the model into distinct steps --\n",
        "by layer, typically.\n",
        "\n",
        "Each worker gets assigned one or more layers.\n",
        "\n",
        "Note, this is also known as \"vertical splitting\",\n",
        "because distributed systems people like\n",
        "to write neural networks going left-to-right\n",
        "instead of bottom-to-top,\n",
        "as is [ancient tradition](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)\n",
        "in the world of NN research.\n",
        "\n",
        "We prefer the term \"pipeline parallelism\"\n",
        "because it's clearer and more evocative.\n",
        "\n",
        "Synchronization: implemented naively,\n",
        "during training a worker needs to wait for\n",
        "the rest of the forward and beginning of the backward pass to complete before they can move forward.\n",
        "\"Bubbles\".\n",
        "\n",
        "Notice that after you've split up your batch into microbatches,\n",
        "you can then further split up the model running on each microbatch:\n",
        "model parallelism is nested within data parallelism.\n",
        "And the only thing different model groups need to share is gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhS52e_n5fJU"
      },
      "source": [
        "### Tensor Parallelism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qexiL-Gb6qVU"
      },
      "source": [
        "Parallelize by splitting up matrix multiplications.\n",
        "\n",
        "Each worker gets assigned a piece of a matrix multiplication.\n",
        "\n",
        "This is the trickiest bit, because more information\n",
        "needs to be communicated between workers, and it's where the Megatron-LM paper makes its intellectual contribution.\n",
        "\n",
        "But notice that if you do it right, tensor parallelism can be nested inside pipeline parallelism,\n",
        "which is nested inside data parallelism.\n",
        "\n",
        "That's three levels of parallelization,\n",
        "and that degree of decomposition is what it takes to scale models to 10s or 100s of billions of parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atfdny6z7_uR"
      },
      "source": [
        "## 2. Megatron-LM: A Recipe for Combining Pipeline & Tensor Parallelism for Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMA9J4-BqMuQ"
      },
      "source": [
        "Data parallelism is straightforward enough --\n",
        "in principle, it's just a matter of taking whatever you're doing for a batch size that you're parallelizing with other techniques\n",
        "and then \"copying\" it to multiple instances\n",
        "that each have their own smaller dataset\n",
        "to draw microbatches from\n",
        "and which share gradients at the end of each batch.\n",
        "\n",
        "Megatron-LM is all about the harder part,\n",
        "what you're parallelizing within a microbatch.\n",
        "\n",
        "At high-level, Megatron-LM first breaks down a model\n",
        "into different stages,\n",
        "with each stage having several layers -\n",
        "that's our pipeline parallelism.\n",
        "\n",
        "Within each layer of a given stage in the pipeline,\n",
        "the computation is divided into smaller sections,\n",
        "with each section assigned to a different GPU -\n",
        "that's our tensor parallelism.\n",
        "\n",
        "To make sure we do our parallelization efficiently,\n",
        "we need to be smart when we define the \"sections\"\n",
        "of our computation.\n",
        "\n",
        "For a matrix multiplication,\n",
        "there are two choices:\n",
        "splitting by row and splitting by column.\n",
        "\n",
        "But for most neural network layers,\n",
        "there's only one sensible choice:\n",
        "split along the neuron dimension.\n",
        "That way, you can calculate the output\n",
        "of your non-linearity without communication\n",
        "between workers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfUGSJTbpAnw"
      },
      "source": [
        "## 3. Letâ€™s implement `ColumnLinearParallel` from scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK7PT5hsXR3v"
      },
      "source": [
        "This is a parallelized version of a linear layer where we parallelize by column. [[Megatron's ColumnLinearParallel]](https://github.com/NVIDIA/Megatron-LM/blob/060415572f4365a2e895f8036c4e37dad0efbdf5/megatron/core/tensor_parallel/layers.py#L418)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecR2MfCGXNRz"
      },
      "source": [
        "### But why `Column` parallel?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHNiu20NV_6G"
      },
      "source": [
        "There are, roughly speaking, two kinds of matrices:\n",
        "- matrices that represent a collection of _vectors_\n",
        "- matrices that represent a collection of _functions to apply to vectors_\n",
        "\n",
        "A single matrix can switch between being one or the other,\n",
        "depending on how it's used,\n",
        "but most of the time a matrix only does one of those two things.\n",
        "\n",
        "For example, a batch of data is a collection of vectors, while the weights of a layer in a neural network are a collection of functions to apply to vectors -- each of which is what some might call a \"neuron\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfwfKJA8XA2-"
      },
      "source": [
        "When we distribute work,\n",
        "we always want to split such that\n",
        "the different pieces of the computation are as independent as possible.\n",
        "\n",
        "For a collection, that's the dimension that goes across\n",
        "different elements of the collection.\n",
        "\n",
        "So when we're parallelizing a batch,\n",
        "that means splitting the different entries in the batch\n",
        "onto different workers.\n",
        "\n",
        "And when we're parallelizing across a layer,\n",
        "we want to split the different neurons onto different workers.\n",
        "\n",
        "In the conventions of PyTorch,\n",
        "that means we want to split the weights along\n",
        "their last dimension, the columns:\n",
        "\n",
        "```python\n",
        "# [batch, n_outputs] = [batch, n_inputs] @ [n_inputs, n_outputs] + [n_outputs]\n",
        "out = inputs @ weights + biases\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgFmSeYGftge",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "torch.random.manual_seed(117)\n",
        "\n",
        "world_size = 4\n",
        "batch_size, input_size, output_size = 10, 16, 12\n",
        "\n",
        "inputs = torch.randn(2, input_size, requires_grad=False)\n",
        "weights = torch.randn(output_size, input_size, requires_grad=True)\n",
        "biases = torch.randn(output_size, requires_grad=True)\n",
        "\n",
        "outputs = torch.matmul(inputs, weights.T) + biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za6iOffpMZr8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def compute_column_parallel_linear(inputs, weights, biases, n_partitions):\n",
        "    num_columns = weights.shape[-1]  #\n",
        "\n",
        "    # partition into groups of \"neurons\"\n",
        "    partition_size = num_columns // n_partitions\n",
        "    w1, w2 = weights[:partition_size, :], weights[partition_size:, :]\n",
        "\n",
        "    # now these can run independently\n",
        "    out1, out2 = torch.matmul(inputs, w1.T), torch.matmul(inputs, w2.T)\n",
        "\n",
        "    # and then we get the final result by combining them -- along the same dimension we split\n",
        "    out = torch.cat([out1, out2], dim=-1)\n",
        "\n",
        "    return out + biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUa_5NQPfy-d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "outputs_parallel = compute_column_parallel_linear(inputs, weights, biases, n_partitions=4)\n",
        "\n",
        "assert torch.equal(outputs, outputs_parallel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6NcLL1tsM8q"
      },
      "source": [
        "\n",
        "In summary, the `ColumnParallelLinear` class divides the work of a linear layer across multiple processes. It does this by dividing the output dimension of the layer among the processes. Each process then computes its portion of the output and the gradients during the forward and backward passes, respectively. After the forward pass, the outputs from all the processes are gathered together to create the final output tensor. During the backward pass, the gradients are distributed across all the processes, and each process uses its portion of the gradient to update its parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ttCxlRsPYai",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ColumnParallelLinear(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        world_size = torch.distributed.get_world_size()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self._output_size_per_partition = output_size // world_size\n",
        "\n",
        "        self.weight = nn.Parameter(torch.randn(\n",
        "            self._output_size_per_partition,\n",
        "            self.input_size,\n",
        "        ))\n",
        "        self.bias = nn.Parameter(torch.randn(\n",
        "            self._output_size_per_partition,\n",
        "        ))\n",
        "\n",
        "    def forward(self, input):\n",
        "        output_parallel = F.linear(Broadcast.apply(input), self.weight, self.bias)\n",
        "        outputs = Gather.apply(output_parallel)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTEwNmY3PSjR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class f(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        torch.distributed.all_reduce(\n",
        "            grad_output,  # modified in-place!\n",
        "            op=torch.distributed.ReduceOp.SUM\n",
        "        )\n",
        "        return grad_output\n",
        "\n",
        "class Broadcast(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        torch.distributed.all_reduce(\n",
        "            grad_output, # modified in-place!\n",
        "            op=torch.distributed.ReduceOp.SUM\n",
        "        )\n",
        "        return grad_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmb6mvcCPPoT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Gather(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        world_size = torch.distributed.get_world_size()\n",
        "\n",
        "        inputs = [torch.empty_like(input) for _ in range(world_size)]\n",
        "\n",
        "        torch.distributed.all_gather(inputs, input)\n",
        "\n",
        "        inputs = torch.cat(inputs, dim=-1)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        rank = torch.distributed.get_rank()\n",
        "\n",
        "        world_size = torch.distributed.get_world_size()\n",
        "\n",
        "        dim_size = grad_output.shape[-1]\n",
        "\n",
        "        dim_size_per_partition = dim_size // world_size\n",
        "\n",
        "        grad_chunks = torch.split(grad_output, dim_size_per_partition, dim=-1)\n",
        "\n",
        "        return grad_chunks[rank]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LqWuX48sRAV"
      },
      "source": [
        "### Explain\n",
        "\n",
        "From the `f` class\n",
        "\n",
        "- `output = F.linear(input, self.weight, self.bias)`: The output partition corresponding to the current process.\n",
        "\n",
        "From the `g` class\n",
        "\n",
        "- `inputs = [torch.empty_like(input) for _ in range(world_size)]`: This line creates an outputs list with empty tensors that have the same shape as `input`. These tensors will be used to store the output of each process.\n",
        "\n",
        "- `torch.distributed.all_gather(inputs, input)`: The `torch.distributed.all_gather` function is called to gather the input from all processes in the distributed group and store them in the `inputs` list.\n",
        "\n",
        "From the `ColumnParallelLinear` class\n",
        "\n",
        "- `self.output_size_per_partition = output_size // world_size`: This line calculates the output size for each partition by dividing the total output size by the number of partitions. This is done because the output dimension of the linear layer is divided among multiple processes, and each process will handle its corresponding portion of the output dimension.\n",
        "\n",
        "- `self.weight = nn.Parameter(torch.empty(self.output_size_per_partition, self.input_size))`: This line initializes the weight parameter for the current process. Since each process is responsible for its own portion of the output dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NJ-H35HKRW0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "outputs.sum().backward()\n",
        "\n",
        "weight_grads = weights.grad.detach().requires_grad_(False)\n",
        "bias_grads = biases.grad.detach().requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlsPgUGFCnRg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def run_parallel(\n",
        "    rank, world_size,\n",
        "    input_size, output_size,\n",
        "    inputs, weights, biases, outputs,\n",
        "    weight_grads, bias_grads\n",
        "):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12359'\n",
        "    torch.distributed.init_process_group(\n",
        "        \"gloo\",\n",
        "        rank=rank,\n",
        "        world_size=world_size\n",
        "    )\n",
        "\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "    torch.random.manual_seed(rank)\n",
        "\n",
        "    model = ColumnParallelLinear(input_size, output_size)\n",
        "\n",
        "    # Partition the weights and biases and assign to the model\n",
        "    partition_size = weights.shape[0] // world_size\n",
        "    partition_start, partition_end = rank * partition_size, (rank + 1) * partition_size\n",
        "\n",
        "    model.weight.data = weights[partition_start: partition_end].detach().requires_grad_(True)\n",
        "    model.bias.data = biases[partition_start: partition_end].detach().requires_grad_(True)\n",
        "\n",
        "    outputs_parallel = model(inputs.detach().requires_grad_(False))\n",
        "    outputs_parallel.sum().backward()\n",
        "\n",
        "    print(f\"rank={rank}, parallel_output.shape: {outputs_parallel.shape}, non_parallel_output.shape: {outputs.shape}\\n\")\n",
        "    print(f\"rank={rank}, is the forward correct? {torch.allclose(outputs_parallel, outputs)}\\n\")\n",
        "    print(f\"rank={rank}, is the gradient of the weight correct? {torch.allclose(model.weight.grad, weight_grads[rank])}\\n\")\n",
        "    print(f\"rank={rank}, is the gradient of the bias correct? {torch.allclose(model.bias.grad, bias_grads[rank])}\\n\")\n",
        "\n",
        "    torch.distributed.destroy_process_group()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DVBJavwCtif",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from torch.multiprocessing import Process\n",
        "\n",
        "processes = []\n",
        "\n",
        "for rank in range(world_size):\n",
        "    p = Process(target=run_parallel, args=(\n",
        "        rank, world_size,\n",
        "        input_size, output_size,\n",
        "        # Because PyTorch does not support sending tensors\n",
        "        # that require gradients through inter-process communication\n",
        "        # we need to detach them from the computational graph\n",
        "        inputs, weights.detach(), biases.detach(), outputs.detach(),\n",
        "        weight_grads, bias_grads\n",
        "    ))\n",
        "    processes.append(p)\n",
        "    p.start()\n",
        "\n",
        "for p in processes:\n",
        "    p.join()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write a ParallelMLP from scratch"
      ],
      "metadata": {
        "id": "Ir9yKE-ESAxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Scatter(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        rank = torch.distributed.get_rank()\n",
        "        world_size = torch.distributed.get_world_size()\n",
        "        last_dim_size = input.shape[-1]\n",
        "        n_chunks = last_dim_size // world_size\n",
        "        input_chunks = torch.split(input, n_chunks, dim=-1)\n",
        "        return input_chunks[rank]\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        world_size = torch.distributed.get_world_size()\n",
        "        grad_outputs = [torch.empty_like(grad_output) for _ in range(world_size)]\n",
        "        torch.distributed.all_gather(grad_outputs, grad_output)\n",
        "        grad_outputs = torch.cat(grad_outputs, dim=-1)\n",
        "        return grad_outputs\n",
        "\n",
        "class Reduce(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        world_size = torch.distributed.get_world_size()\n",
        "        if world_size == 1:\n",
        "            return input\n",
        "        torch.distributed.all_reduce(input)\n",
        "        return input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output"
      ],
      "metadata": {
        "id": "pPnQ0vIgRm5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RowParallelLinear(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        world_size = torch.distributed.get_world_size()\n",
        "        input_size_per_partition = input_size // world_size\n",
        "\n",
        "        self.weight = nn.Parameter(torch.randn(\n",
        "            output_size,\n",
        "            input_size_per_partition\n",
        "        ))\n",
        "        self.bias = nn.Parameter(torch.randn(output_size))\n",
        "\n",
        "    def forward(self, input):\n",
        "        dist.barrier()\n",
        "        input_parallel = Scatter.apply(input)\n",
        "        output_parallel = F.linear(input_parallel, self.weight)\n",
        "        outputs = Reduce.apply(output_parallel)\n",
        "        return outputs + self.bias"
      ],
      "metadata": {
        "id": "qyx9YCMzSEbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_parallel(\n",
        "    rank, world_size,\n",
        "    input_size, output_size,\n",
        "    inputs, weights, biases, outputs,\n",
        "    weight_grads, bias_grads\n",
        "):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12359'\n",
        "    torch.distributed.init_process_group(\n",
        "        \"gloo\",\n",
        "        rank=rank,\n",
        "        world_size=world_size\n",
        "    )\n",
        "\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "    torch.random.manual_seed(rank)\n",
        "\n",
        "    hidden_size = output_size * 4\n",
        "    model = nn.Sequential(\n",
        "        ColumnParallelLinear(input_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        RowParallelLinear(hidden_size, output_size),\n",
        "    )\n",
        "\n",
        "    def load_data(model, layer_idx, idx):\n",
        "        if layer_idx == 0:\n",
        "            partition_size = weights[idx].shape[0] // world_size\n",
        "        elif layer_idx == 2:\n",
        "            partition_size = weights[idx].shape[1] // world_size\n",
        "\n",
        "        partition_start, partition_end = rank * partition_size, (rank + 1) * partition_size\n",
        "\n",
        "        if layer_idx == 0:\n",
        "            model[layer_idx].weight.data = weights[idx][partition_start: partition_end].detach().requires_grad_(True)\n",
        "            model[layer_idx].bias.data = biases[idx][partition_start:partition_end].detach().requires_grad_(True)\n",
        "        elif layer_idx == 2:\n",
        "            model[layer_idx].weight.data = weights[idx][:, partition_start:partition_end].detach().requires_grad_(True)\n",
        "            model[layer_idx].bias.data = biases[idx][:partition_end].detach().requires_grad_(True)\n",
        "        return model\n",
        "\n",
        "    model = load_data(model, layer_idx=0, idx=0)\n",
        "    model = load_data(model, layer_idx=2, idx=1)\n",
        "\n",
        "    outputs_parallel = model(inputs)\n",
        "    outputs_parallel.sum().backward()\n",
        "\n",
        "    print(f\"rank={rank}, parallel_output.shape: {outputs_parallel.shape}, non_parallel_output.shape: {outputs.shape}\\n\")\n",
        "    print(f\"rank={rank}, is the forward correct? {torch.allclose(outputs_parallel, outputs, rtol=0.01)}\\n\")\n",
        "\n",
        "    for layer_idx, grad_idx in [[0, 0], [2, 1]]:\n",
        "        if layer_idx == 0:\n",
        "            partition_size = weight_grads[grad_idx].shape[0] // world_size\n",
        "            grad_chunks = torch.split(weight_grads[grad_idx], partition_size, dim=0)\n",
        "            bias_chunks = torch.split(bias_grads[grad_idx], partition_size, dim=0)\n",
        "        elif layer_idx == 2:\n",
        "            partition_size = weight_grads[grad_idx].shape[1] // world_size\n",
        "            grad_chunks = torch.split(weight_grads[grad_idx], partition_size, dim=1)\n",
        "\n",
        "        print(f\"rank={rank}, is the gradient of the weight correct? {torch.allclose(model[layer_idx].weight.grad, grad_chunks[rank])}\\n\")\n",
        "        if layer_idx == 0:\n",
        "            print(f\"rank={rank}, is the gradient of the bias correct? {torch.allclose(model[layer_idx].bias.grad, bias_chunks[rank])}\\n\")\n",
        "        else:\n",
        "            print(f\"rank={rank}, is the gradient of the bias correct? {torch.allclose(model[layer_idx].bias.grad, bias_grads[grad_idx])}\\n\")\n",
        "\n",
        "    torch.distributed.destroy_process_group()"
      ],
      "metadata": {
        "id": "QNEtktbcSPAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "processes = []\n",
        "world_size = 4\n",
        "batch_size, input_size, output_size = 10, 16, 12\n",
        "hidden_size = output_size * 4\n",
        "\n",
        "inputs = torch.randn(batch_size, input_size, requires_grad=False)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, output_size),\n",
        ")\n",
        "outputs = model(inputs)\n",
        "outputs.sum().backward()\n",
        "\n",
        "weights = [\n",
        "    model[0].weight.data.detach(),\n",
        "    model[2].weight.data.detach(),\n",
        "]\n",
        "biases = [model[0].bias.data.detach(), model[2].bias.data.detach()]\n",
        "weight_grads = [\n",
        "    model[0].weight.grad.detach().requires_grad_(False),\n",
        "    model[2].weight.grad.detach().requires_grad_(False)\n",
        "]\n",
        "bias_grads = [\n",
        "    model[0].bias.grad.detach().requires_grad_(False),\n",
        "    model[2].bias.grad.detach().requires_grad_(False)\n",
        "\n",
        "]\n",
        "\n",
        "for rank in range(world_size):\n",
        "    p = Process(target=run_parallel, args=(\n",
        "        rank, world_size,\n",
        "        input_size, output_size,\n",
        "        # Because PyTorch does not support sending tensors\n",
        "        # that require gradients through inter-process communication\n",
        "        # we need to detach them from the computational graph\n",
        "        inputs, deepcopy(weights), deepcopy(biases), outputs.detach(),\n",
        "        deepcopy(weight_grads), deepcopy(bias_grads)\n",
        "    ))\n",
        "    processes.append(p)\n",
        "    p.start()\n",
        "\n",
        "for p in processes:\n",
        "    p.join()"
      ],
      "metadata": {
        "id": "Y6NYku0sSZE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTpZl22yo3yV"
      },
      "source": [
        "## 2. Distributed Communication\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiGKGzQBrMyE"
      },
      "source": [
        "When we train a model in a distributed manner, there are four atomic operations in distributed communication that we need to perform\n",
        "\n",
        "- Broadcast: We start with a tensor in one process and send it to all the other processes within the group. This is like sharing a piece of information with everyone in the group.\n",
        "- Scatter: We take a tensor from one process and distribute its elements or chunks to all the other processes in the group. This is like dividing up a task among all the members in a team.\n",
        "- Gather: We gather data from all the processes in the group and assemble it into a single tensor at the destination process. This is like collecting everyoneâ€™s input and putting it together in one place.\n",
        "- Reduce: We take data from all processes in the group, apply a specific operation to it (like summing, multiplying, finding the minimum or maximum), and then store the result in the destination process. This is like combining everyoneâ€™s efforts and producing a single output\n",
        "\n",
        "However, we canâ€™t just directly use these operations from PyTorch like `torch.distributed.broadcast`. This is because in training, letâ€™s say we are broadcasting a tensor `x` from device 0 to all devices 1, 2, and 3 during the forward pass. We must also support the reverse order during the backward pass. This means we have to write a broadcast operation that can handle both forward and backward passes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23AT94Hffomd"
      },
      "outputs": [],
      "source": [
        "def is_grad_enable(input):\n",
        "    return torch.is_grad_enabled() and input.requires_grad\n",
        "\n",
        "def broadcast(inputs):\n",
        "    return inputs.clone()\n",
        "\n",
        "def reduce(inputs):\n",
        "    world_size_of_parallel_group = torch.distributed.get_world_size()\n",
        "\n",
        "    if world_size_of_parallel_group == 1:\n",
        "        return inputs\n",
        "\n",
        "    torch.distributed.all_reduce(\n",
        "        inputs,\n",
        "        op=torch.distributed.ReduceOp.SUM\n",
        "    )\n",
        "\n",
        "    return inputs\n",
        "\n",
        "class Broadcast(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return broadcast(input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return reduce(grad_output)\n",
        "\n",
        "def broadcast_with_backward(inputs):\n",
        "    if is_grad_enable(inputs):\n",
        "        outputs = Broadcast.apply(inputs)\n",
        "    else:\n",
        "        outputs = broadcast(inputs)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI9MfFS0p_1M"
      },
      "source": [
        "In the forward pass, all the workers - different parts of our computer system - start with the same model parameters. One worker, usually the boss or â€˜masterâ€™, gives these parameters to everyone else at the beginning of each cycle (`torch.distributed.broadcast()`).\n",
        "\n",
        "In the backward pass, each worker does its own calculation. They all figure out their own gradients - basically, these are pointers that show how to tweak the model to improve it. After everyoneâ€™s done their calculations, they pool together their gradients. Each gradient represents the best direction to adjust the weight to minimize the loss with respect to its mini-batch. What we want is to find an average direction that works best for all the model replicas, so we pool all these gradients together (`torch.distributed.all_reduce()`). Then, we use this big pooled gradient to tweak the modelâ€™s parameters, making it a bit better with each cycle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6N9yTKMRf-a"
      },
      "source": [
        "### 5. Pipeline Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O_rfVoLRf-a"
      },
      "source": [
        "So, let's say we have a big model with 10 layers, like a Transformer model. And we've got 5 devices (like GPUs) to run our model. We want to split the model into 5 parts (which we're gonna call 'partitions') and each part will run on one device.\n",
        "\n",
        "But here's the problem. In a Transformer model, each layer needs the result from the previous layer before it can do its work. It's like a relay race, you can't start running until you've got the baton from the runner before you. So if we split our model into 5 parts, then the second part can't start until the first part is done, the third part can't start until the second part is done, and so on. That means that most of the time, most of our devices are just sitting around doing nothing. That's a bummer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow2VlHE-Rf-a"
      },
      "source": [
        "![image.png](attachment:680b6f85-3a24-47d2-956a-9ef8bacc48c1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKOf8-XvRf-a"
      },
      "source": [
        "So what can we do? Here's where GPipe comes in. Instead of feeding a big batch of data to our model all at once, GPipe splits that batch into smaller chunks, which we're gonna call 'micro-batches'. And here's the trick: while one micro-batch is being processed by the second part of our model, the next micro-batch can start being processed by the first part of the model.\n",
        "\n",
        "This way, there's always something for each part of the model to do. It's like a factory assembly line. As soon as one car is done with one station, it moves to the next station and a new car moves into the first station. This keeps all our devices busy (although they might still have some idle time, like when a worker in the factory is waiting for the next car to arrive).\n",
        "\n",
        "So how does GPipe know what to do at each moment? That's the job of the GPipe's scheduler. The scheduler works in 'clock cycles'. For each clock cycle, it figures out which partitions should be active and which micro-batch each partition should work on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Vb1gE2Rf-a"
      },
      "source": [
        "3 microbatches\n",
        "\n",
        "3 layers\n",
        "\n",
        "each layer are split into 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "yU2CMLwqRf-a"
      },
      "outputs": [],
      "source": [
        "n_microbatches = 3\n",
        "n_partritions = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFH_jvwHRf-a"
      },
      "source": [
        "Because it takes `n_microbatches` clock cycles for all micro-batches to pass through the first partition. Once the last micro-batch enters the first partition, it needs to go through the remaining partitions. Since there are `n_partritions` partitions, this requires `n_partritions-1` additional clock cycles because the first clock cycle is already counted when the micro-batch enters the first partition.\n",
        "\n",
        "Therefore, the total number of clock cycles is `n_microbatches+n_partritions-1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "sSL9eu-KRf-i"
      },
      "outputs": [],
      "source": [
        "n_clock_cycles = n_microbatches+n_partritions-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "JPULuUzkRf-i"
      },
      "outputs": [],
      "source": [
        "n_clock_cycles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "BuLiWqQ9Rf-i"
      },
      "outputs": [],
      "source": [
        "for clock_idx in range(n_clock_cycles):\n",
        "    start_partrition = max(clock_idx+1-n_microbatches, 0)\n",
        "    end_partrition = min(clock_idx+1, n_partritions)\n",
        "\n",
        "    tasks = []\n",
        "    for partrition_idx in range(start_partrition, end_partrition):\n",
        "        microbatch_idx = clock_idx-partrition_idx\n",
        "        task = (microbatch_idx, partrition_idx)\n",
        "        tasks.append(task)\n",
        "\n",
        "    print(f\"Clock cycle {clock_idx}: {tasks}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3P58IITRf-i"
      },
      "source": [
        "**Explain**\n",
        "\n",
        "`min(clock_idx+1, n_partritions)`\n",
        "- For each clock cycle, a new partrition actives in the pipeline. If we are currently in clock_idx, it means that clock_idx partritions have already been actived.\n",
        "- The next partritions will be `clock_idx+1`. However, we cannot exceed the total number of partitions (`n_partitions`), so we use the min function to limit the range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVPT2EgFRf-i"
      },
      "source": [
        "### 6. Let's build a pipeline parallelism from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "qOtG4-J7Rf-i"
      },
      "outputs": [],
      "source": [
        "def clock_cycles(n_microbatches, n_partritions):\n",
        "    n_clock_cycles = n_partritions + n_microbatches - 1\n",
        "    for clock_idx in range(n_clock_cycles):\n",
        "        start_partrition = max(clock_idx+1-n_microbatches, 0)\n",
        "        end_partrition = min(clock_idx+1, n_partritions)\n",
        "\n",
        "        tasks = []\n",
        "        for partrition_idx in range(start_partrition, end_partrition):\n",
        "            microbatch_idx = clock_idx-partrition_idx\n",
        "            task = (microbatch_idx, partrition_idx)\n",
        "            tasks.append(task)\n",
        "\n",
        "        yield tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hpD5WEW0Rf-i"
      },
      "outputs": [],
      "source": [
        "for schedules in clock_cycles(n_microbatches, n_partritions):\n",
        "    print(schedules)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable, List, Tuple, Annotated, Optional, Generator, Dict, Callable, Any\n",
        "from dataclasses import dataclass\n",
        "from contextlib import contextmanager\n",
        "from queue import Queue\n",
        "from threading import Thread\n",
        "import time\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.multiprocessing as mp"
      ],
      "metadata": {
        "id": "IBYscGQ8TJGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class QueueOutput:\n",
        "    task: Callable\n",
        "    output: Any\n",
        "    is_done: bool = False\n",
        "\n",
        "\n",
        "def wait_and_execute(device: torch.device, in_queue: Queue, out_queue: Queue):\n",
        "    \"\"\"Wait for a task and execute it.\"\"\"\n",
        "    while True:\n",
        "        task = in_queue.get()\n",
        "\n",
        "        if task.is_done is True:\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            output = task.compute()\n",
        "        except Exception:\n",
        "            raise RuntimeError(f\"Failed to execute a task on {device}\")\n",
        "            out_queue.put(QueueOutput(task=task, output=None, is_done=False))\n",
        "            continue\n",
        "\n",
        "        out_queue.put(QueueOutput(task=task, output=output, is_done=True))\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def spawn_worker(\n",
        "    devices: List[torch.device],\n",
        ") -> Generator[\n",
        "    Tuple[\n",
        "        Annotated[List[Queue], \"A list of tasks to be executed\"],\n",
        "        Annotated[List[Queue], \"A list of tasks has been executed\"],\n",
        "    ],\n",
        "    None,\n",
        "    None,\n",
        "]:\n",
        "    \"\"\"Spawn new worker threads.\"\"\"\n",
        "    in_queues: List[Queue] = []\n",
        "    out_queues: List[Queue] = []\n",
        "\n",
        "    workers: Dict[torch.device, Tuple[Queue, Queue]] = {}\n",
        "\n",
        "    for device in devices:\n",
        "        # TODO: remove device\n",
        "        try:\n",
        "            in_queue, out_queue = workers[device]\n",
        "        except KeyError:\n",
        "            in_queue = Queue()\n",
        "            out_queue = Queue()\n",
        "            workers[device] = (in_queue, out_queue)\n",
        "\n",
        "            thread = Thread(target=wait_and_execute, args=(device, in_queue, out_queue), daemon=True)\n",
        "            thread.start()\n",
        "\n",
        "        in_queues.append(in_queue)\n",
        "        out_queues.append(out_queue)\n",
        "\n",
        "    yield (in_queues, out_queues)"
      ],
      "metadata": {
        "id": "GXOMEQLHZRct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Task:\n",
        "    def __init__(self, compute: Callable[[], torch.Tensor], is_done: bool = False):\n",
        "        self._compute = compute\n",
        "        self.is_done = is_done\n",
        "\n",
        "    def compute(self) -> torch.Tensor:\n",
        "        return self._compute()"
      ],
      "metadata": {
        "id": "VM_oj6DNZa9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pipeline:\n",
        "    \"\"\"A base class for pipeline.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        batches: List[torch.Tensor],\n",
        "        partitions: List[nn.Sequential],\n",
        "        devices: Optional[List[torch.device]] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"Initialize the pipeline.\n",
        "\n",
        "        Args:\n",
        "            batches (List[Batch]): A list of micro-batches.\n",
        "            partitions (List[nn.Sequential]): A partitioned model.\n",
        "            devices (Optional[List[torch.device]], optional): A list of devices. Defaults to None.\n",
        "            scheduler (BaseScheduler, optional): _description_. Defaults to DetermisticScheduler().\n",
        "        \"\"\"\n",
        "        self.batches = batches\n",
        "        self.partitions = partitions\n",
        "        self.devices = devices\n",
        "\n",
        "    def fit(self):\n",
        "        batches = self.batches\n",
        "        partitions = self.partitions\n",
        "        devices = self.devices\n",
        "\n",
        "        n_batches = len(batches)\n",
        "        n_partitions = len(partitions)\n",
        "\n",
        "        with spawn_worker(devices) as (in_queues, out_queues):\n",
        "            for schedule in clock_cycles(n_batches, n_partitions):\n",
        "                self._compute(schedule, in_queues, out_queues)\n",
        "\n",
        "    def _compute(self, schedule: List[Tuple[int, int]], in_queues: List[Queue], out_queues: List[Queue]):\n",
        "        \"\"\"Compute the partitions.\"\"\"\n",
        "        batches = self.batches\n",
        "        partitions = self.partitions\n",
        "\n",
        "        for microbatch_idx, partition_idx in schedule:\n",
        "            batch = batches[microbatch_idx]\n",
        "            partrition = partitions[partition_idx]\n",
        "\n",
        "            def compute(batch, partrition):\n",
        "                def wrapper():\n",
        "                    return partrition(batch)\n",
        "\n",
        "                return wrapper\n",
        "\n",
        "            task = Task(compute=compute(batch, partrition))\n",
        "            in_queues[partition_idx].put(task)\n",
        "\n",
        "        for microbatch_idx, partition_idx in schedule:\n",
        "            queue_output = out_queues[partition_idx].get()\n",
        "            task, output = queue_output.task, queue_output.output\n",
        "\n",
        "            # put the output back to the batch\n",
        "            batches[microbatch_idx] = output"
      ],
      "metadata": {
        "id": "DZcbGUsRayFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Test the forward and backward time's line of the pipeline**"
      ],
      "metadata": {
        "id": "5ve4uNOqgg0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_MICROBATCHES = 3\n",
        "N_PARTITIONS = 2\n",
        "\n",
        "forward_timeline = []\n",
        "backward_timeline = []\n",
        "\n",
        "def backward_hook(module, grad_input, grad_output):\n",
        "    backward_timeline.append((module.microbatch_idx - 1, module.partition_idx))\n",
        "    module.microbatch_idx -= 1\n",
        "\n",
        "class AddOne(nn.Module):\n",
        "    def __init__(self, partition_idx, is_logging):\n",
        "        super().__init__()\n",
        "        self.microbatch_idx = 0\n",
        "        self.partition_idx = partition_idx\n",
        "        self.is_logging = is_logging\n",
        "        self.net = nn.Linear(1, 1)\n",
        "        self.register_backward_hook(backward_hook)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.is_logging:\n",
        "            time.sleep(0.5)\n",
        "            forward_timeline.append((self.microbatch_idx, self.partition_idx))\n",
        "            self.microbatch_idx += 1\n",
        "\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def loss_func(x):\n",
        "    return x.mean()"
      ],
      "metadata": {
        "id": "xLZ7n-EJg8KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.arange(0, N_MICROBATCHES, dtype=torch.float32, requires_grad=True)\n",
        "microbatches = [x.unsqueeze(0) for x in batch.unbind()]\n",
        "partitions = [nn.Sequential(AddOne(partition_idx=x, is_logging=True)) for x in range(N_PARTITIONS)]\n",
        "devices = [torch.device(\"cpu\") for _ in range(N_PARTITIONS)]"
      ],
      "metadata": {
        "id": "eOfq6kIUg_Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(microbatches, partitions, devices)\n",
        "\n",
        "assert pipeline.batches == microbatches\n",
        "assert pipeline.partitions == partitions"
      ],
      "metadata": {
        "id": "O57Is-IahHQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit()\n",
        "\n",
        "assert forward_timeline == [(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (2, 1)]\n",
        "\n",
        "outputs = microbatches"
      ],
      "metadata": {
        "id": "FSym-soqhKuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forward_timeline"
      ],
      "metadata": {
        "id": "tlP1D7bIhXxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in outputs:\n",
        "    loss = loss_func(x)\n",
        "    loss.backward()\n",
        "\n",
        "assert backward_timeline == [(2, 1), (2, 0), (1, 1), (1, 0), (0, 1), (0, 0)] or backward_timeline == [\n",
        "    (2, 1),\n",
        "    (2, 0),\n",
        "    (1, 1),\n",
        "    (0, 1),\n",
        "    (1, 0),\n",
        "    (0, 0),\n",
        "]"
      ],
      "metadata": {
        "id": "aoT-helahZKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backward_timeline"
      ],
      "metadata": {
        "id": "Mjh2AjRlhcf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Let's put it all together**"
      ],
      "metadata": {
        "id": "J9Hg8gs0herH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Will write explanation here*\n",
        "\n",
        "Train a ParallelMLP using pipeline parallelism.\n",
        "\n",
        "Tests include test the output and the gradiens with non-parallel version"
      ],
      "metadata": {
        "id": "6ARSO8jIiTf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(rank, world_size, input_size, hidden_size, output_size, microbatches, weights, biases, outputs, weight_grads, bias_grads):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12359'\n",
        "    torch.distributed.init_process_group(\n",
        "        \"gloo\",\n",
        "        rank=rank,\n",
        "        world_size=world_size\n",
        "    )\n",
        "\n",
        "    partitions = [\n",
        "        nn.Sequential(ColumnParallelLinear(input_size, hidden_size), nn.ReLU()),\n",
        "        nn.Sequential(RowParallelLinear(hidden_size, output_size)),\n",
        "    ]\n",
        "\n",
        "    partitions = load_param(rank, world_size, weights, biases, partitions)\n",
        "    devices = [torch.device(\"cpu\") for _ in range(len(partitions))]\n",
        "    pipeline = Pipeline(microbatches, partitions, devices)\n",
        "\n",
        "    assert pipeline.batches == microbatches\n",
        "    assert pipeline.partitions == partitions\n",
        "\n",
        "    pipeline.fit()\n",
        "\n",
        "    parallel_outputs = microbatches\n",
        "    print(f\"rank={rank}, outputs.shape: {len(parallel_outputs)}\\n\")\n",
        "    print(parallel_outputs[0].shape)\n",
        "\n",
        "    for x, y in zip(outputs, parallel_outputs):\n",
        "        assert torch.allclose(x, y, rtol=0.01)\n",
        "\n",
        "    for x in parallel_outputs:\n",
        "        x.sum().backward()\n",
        "\n",
        "    for layer_idx, grad_idx in [[0, 0], [1, 1]]:\n",
        "        if layer_idx == 0:\n",
        "            partition_size = weight_grads[grad_idx].shape[0] // world_size\n",
        "            grad_chunks = torch.split(weight_grads[grad_idx], partition_size, dim=0)\n",
        "            bias_chunks = torch.split(bias_grads[grad_idx], partition_size, dim=0)\n",
        "        elif layer_idx == 1:\n",
        "            partition_size = weight_grads[grad_idx].shape[1] // world_size\n",
        "            grad_chunks = torch.split(weight_grads[grad_idx], partition_size, dim=1)\n",
        "\n",
        "        print(f\"rank={rank}, is the gradient of the weight correct? {torch.allclose(partitions[layer_idx][0].weight.grad, grad_chunks[rank])}\\n\")\n",
        "        if layer_idx == 0:\n",
        "            print(f\"rank={rank}, is the gradient of the bias correct? {torch.allclose(partitions[layer_idx][0].bias.grad, bias_chunks[rank])}\\n\")\n",
        "        else:\n",
        "            print(f\"rank={rank}, is the gradient of the bias correct? {torch.allclose(partitions[layer_idx][0].bias.grad, bias_grads[grad_idx])}\\n\")\n"
      ],
      "metadata": {
        "id": "DAxXM44vhyOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "world_size = 4\n",
        "batch_size, input_size, output_size = 10, 16, 12\n",
        "hidden_size = output_size * world_size\n",
        "\n",
        "batch = torch.randn(batch_size, input_size, dtype=torch.float32)\n",
        "microbatches = [x.unsqueeze(0) for x in batch.unbind(dim=0)]\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, output_size),\n",
        ")\n",
        "outputs = model(batch)\n",
        "outputs.sum().backward()\n",
        "\n",
        "def extract_params(model):\n",
        "    weights = [model[0].weight.data.detach(), model[2].weight.data.detach()]\n",
        "    biases = [model[0].bias.data.detach(), model[2].bias.data.detach()]\n",
        "    return weights, biases\n",
        "\n",
        "def extract_grads(model):\n",
        "    weight_grads = [\n",
        "        model[0].weight.grad.detach().requires_grad_(False),\n",
        "        model[2].weight.grad.detach().requires_grad_(False)\n",
        "    ]\n",
        "    bias_grads = [\n",
        "        model[0].bias.grad.detach().requires_grad_(False),\n",
        "        model[2].bias.grad.detach().requires_grad_(False)\n",
        "\n",
        "    ]\n",
        "    return weight_grads, bias_grads\n",
        "\n",
        "weights, biases = extract_params(model)\n",
        "weight_grads, bias_grads = extract_grads(model)"
      ],
      "metadata": {
        "id": "Z4_Z_XhDh3qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_param(rank, world_size, weights, biases, partitions):\n",
        "    def calculate_start_end_idx(rank, idx):\n",
        "        if idx == 0: # column parallel\n",
        "            partition_size = weights[idx].shape[0] // world_size\n",
        "        elif idx == 1: # row parallel\n",
        "            partition_size = weights[idx].shape[1] // world_size\n",
        "        return rank * partition_size, (rank + 1) * partition_size\n",
        "\n",
        "    def load(model, idx):\n",
        "        partition_start, partition_end = calculate_start_end_idx(rank, idx)\n",
        "        if idx == 0:  # column parallel\n",
        "            model[idx][0].weight.data = weights[idx][partition_start: partition_end].detach().requires_grad_(True)\n",
        "            model[idx][0].bias.data = biases[idx][partition_start:partition_end].detach().requires_grad_(True)\n",
        "        elif idx == 1:  # row parallel\n",
        "            model[idx][0].weight.data = weights[idx][:, partition_start:partition_end].detach().requires_grad_(True)\n",
        "            model[idx][0].bias.data = biases[idx][:partition_end].detach().requires_grad_(True)\n",
        "        return model\n",
        "\n",
        "    partitions = load(partitions, idx=0)\n",
        "    partitions = load(partitions, idx=1)\n",
        "    return partitions"
      ],
      "metadata": {
        "id": "PdS3_rJeiHlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.multiprocessing import Process\n",
        "processes = []\n",
        "\n",
        "for rank in range(world_size):\n",
        "  p = Process(target=run_pipeline, args=(\n",
        "        rank, world_size,\n",
        "        input_size, hidden_size, output_size,\n",
        "        microbatches, deepcopy(weights), deepcopy(biases),\n",
        "        outputs.detach().requires_grad_(False),\n",
        "        deepcopy(weight_grads), deepcopy(bias_grads),\n",
        "    ))\n",
        "  p.start()\n",
        "  processes.append(p)\n",
        "\n",
        "for p in processes:\n",
        "  p.join()"
      ],
      "metadata": {
        "id": "3NMtSuM3iAvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt9e-ADvRf-i"
      },
      "source": [
        "### 7. Are we done?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47ro6V7MRf-j"
      },
      "source": [
        "Alright, so after learning about how pipeline parallelism works, you might think \"Cool, I got this!\". But hold on a sec, because there's a lot more to it. Setting up pipeline parallelism, like using GPipe, can get pretty complex and bring up some tricky problems to solve.\n",
        "\n",
        "Think about having a bunch of devices working together. They need to talk to each other, right? But if they're not careful about when and how they communicate, they might end up waiting on each other even when they don't really need to. That's kind of like being on a group chat where everyone needs to wait for one person to respond before they can keep talking. Not so fun, huh?\n",
        "\n",
        "Then there's the problem of skip connections. That's when a layer in the model gets to skip ahead and pass its stuff directly to a layer further down the line. It's like being able to skip a question on a test and come back to it later. But how does we figure out which parts are the skip connections and get them where they need to go?\n",
        "\n",
        "And how do we even decide how to split the model into partitions in the first place? Not all layers are the same. Some might need more memory than others, so we can't just split the model evenly. It's like dividing up chores at home, but some chores take longer than others. If we're not careful, some devices might run out of memory, while others are sitting around twiddling their thumbs.\n",
        "\n",
        "There's also a tricky issue with how PyTorch works. In pipeline parallelism, we need to make sure that the backward pass (that's when the model is learning from its mistakes) on one device finishes before the backward pass on the previous device starts. But PyTorch isn't aware of this requirement. So how do we make sure this happens?\n",
        "\n",
        "And what about gradient checkpointing? That's a technique where we save some intermediary results so we can use them later, like keeping your place in a book with a bookmark. But this can get complicated because we're only building the computational graph (that's like the roadmap of how our data moves through the model) during the forward pass, not the backward pass. So how do we schedule these checkpoints?\n",
        "\n",
        "Even more, what happens if a node (another term for device) crashes or stops working? How do we get back to where we were before the crash? And can all the nodes get back to the same point at the same time?\n",
        "\n",
        "And there's even more! In a big cluster (that's a group of devices working together), we're not the only ones running experiments. Other teams are too. So what happens if another team needs some of the resources that we're using? How can we add and remove nodes dynamically for our training task without messing everything up?\n",
        "\n",
        "And guess what? This isn't even the half of it! You only truly get the full picture when you roll up your sleeves and start building these things from the ground up. So, here's a heads up for our next tutorial: We're gonna build something called a Multi-Processing Unit (MPU) in Megatron from scratch. It's kind of like the boss of your GPU, telling it where to send stuff for different types of parallelism - like tensor parallelism, pipeline parallelism, and data parallelism. Think of it like a traffic cop, directing the flow of data.\n",
        "\n",
        "Stay tuned, and let's get ready to tackle this beast in the next tutorial!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ht9GHVVViqRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}