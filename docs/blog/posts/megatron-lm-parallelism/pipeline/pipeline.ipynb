{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203eb800-0e79-4633-a1bc-fd0da4bf0842",
   "metadata": {},
   "source": [
    "### Demystifying Pipeline Parallelism: Building It From Scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d95adbf-0274-4ac2-95c7-83507df05987",
   "metadata": {},
   "source": [
    "In the world of neural networks, size matters. As scaling laws suggests, the larger the model, the better the performance. But when you have a giant model that won't fit in the memory of a single device, things get complicated. This is where pipeline parallelism comes into play, acting as a super-efficient assembly line for large neural network models. In this blog post, we will walk through the concept and build a a toy pipeline parallelism gpipe from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf2afce-6d06-415f-a32e-e06ad1482aa7",
   "metadata": {},
   "source": [
    "### ****Naive Pipeline Parallelism vs. GPipe****\n",
    "\n",
    "Pipeline parallelism is a process that can be distilled down to a few core steps:\n",
    "\n",
    "- Step 1: **Partition the Model**: Our big model is divided into smaller partitions. Each partition corresponds to a section of the neural network and runs on a separate device.\n",
    "- Step 2: **Micro-Batching**: We split our training data mini-batch into several smaller micro-batches.\n",
    "- Step 3: **Forward and Backward Passes**: These partitions and micro-batches go through both the forward and backward computation passes.\n",
    "- Step 4: **Gradient Averaging**: Once the whole pipeline finishes, we collect the gradients and average them to update the model.\n",
    "\n",
    "To illustrate, let's imagine we have a big model with 10 layers, like a Transformer model, and we've got five devices to run our model. We want to split this model into five parts, or 'partitions', and each part will run on one device.\n",
    "\n",
    "There's a catch, though. In a Transformer model, each layer needs the result from the previous layer before it can do its work. It's like a relay race, you can't start running until you've got the baton from the runner before you. So if we split our model into 5 parts, then the second part can't start until the first part is done, the third part can't start until the second part is done, and so on. That means that most of the time, most of our devices are just sitting around doing nothing. That's a bummer!\n",
    "\n",
    "So what can we do? Here's where GPipe comes in. Instead of feeding a big batch of data to our model all at once, GPipe splits that batch into smaller chunks, which we're gonna call 'micro-batches'. And here's the trick: while one micro-batch is being processed by the second part of our model, the next micro-batch can start being processed by the first part of the model.\n",
    "\n",
    "This way, there's always something for each part of the model to do. It's like a factory assembly line. As soon as one car is done with one station, it moves to the next station and a new car moves into the first station. This keeps all our devices busy (although they might still have some idle time, like when a worker in the factory is waiting for the next car to arrive).\n",
    "\n",
    "The GPipe's scheduler orchestrates this process. It works in 'clock cycles', figuring out which partitions should be active and which micro-batch each partition should work on for each clock cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302c293-d130-4d6f-a480-3cb71d43ca22",
   "metadata": {},
   "source": [
    "### Cracking the Schedule Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4ba53-b19f-4fa4-89d7-8bc9968d95a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "A \"clock cycle\" is like a unit of time for our pipeline. Each clock cycle activates a new partition and passes it a micro-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7304440c-1d8c-4c7d-95bd-10f4cdd0b6c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_partritions = 3\n",
    "n_microbatches = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d44a0b-872d-40d3-816a-cc09c8681aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clock_cycles = n_partritions + n_microbatches - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445f59c7-765b-4347-affd-a1a6c3a20645",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clock_cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec056a4e-82c0-4b5d-9e3e-47cd5aa237d3",
   "metadata": {},
   "source": [
    "If we have m micro-batches and n partitions, it'll take `n_partition + n_microbatches - 1` clock cycles to get everything through the pipeline. Why is that , because it takes m clock cycles for all micro-batches to pass through the first partition. Once the last micro-batch enters the first partition, it needs to go through the remaining partitions. Since there are n partitions, this requires n-1 additional clock cycles because the first clock cycle is already counted when the micro-batch enters the first partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524cc310-2920-4a01-ad86-70a77e02abe7",
   "metadata": {},
   "source": [
    "In pipeline parallelism, for each clock cycle, a new partrition actives in the pipeline. If we are currently in `clock_idx`, it means that `clock_idx` partritions have already been actived. \n",
    "\n",
    "The next partritions will be `clock_idx+1`. However, we cannot exceed the total number of partitions (`n_partitions`), so we use the min function to limit the range.\n",
    "\n",
    "So, what happens in each clock cycle? Good question! In each clock cycle, we determine which partitions are active and what they should be working on. Our scheduler assigns tasks in the form of `(microbatch_index, partition_index)` for each clock cycle. This basically tells each device what chunk of the neural network it should process and with which micro-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e0bd9-2853-4853-ae12-0888ca7f03ad",
   "metadata": {},
   "source": [
    "For example, (4, 1) means the 5th micro-batch is going through the second partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557223ed-e660-4a1b-afd7-a858d43de09b",
   "metadata": {},
   "source": [
    "### Behind the Scenes: Worker Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880539df-0cfe-42be-a6ea-4d866c5a8a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
