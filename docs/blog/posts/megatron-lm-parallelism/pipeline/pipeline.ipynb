{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# just some utils, readers, please ignore this\n",
        "def load_param(rank, world_size, weights, biases, partitions):\n",
        "    def calculate_start_end_idx(rank, idx):\n",
        "        if idx == 0: # column parallel\n",
        "            partition_size = weights[idx].shape[0] // world_size\n",
        "        elif idx == 1: # row parallel\n",
        "            partition_size = weights[idx].shape[1] // world_size\n",
        "        return rank * partition_size, (rank + 1) * partition_size\n",
        "\n",
        "    def load(model, idx):\n",
        "        partition_start, partition_end = calculate_start_end_idx(rank, idx)\n",
        "        if idx == 0:  # column parallel\n",
        "            model[idx][0].weight.data = weights[idx][partition_start: partition_end].detach().requires_grad_(True)\n",
        "            model[idx][0].bias.data = biases[idx][partition_start:partition_end].detach().requires_grad_(True)\n",
        "        elif idx == 1:  # row parallel\n",
        "            model[idx][0].weight.data = weights[idx][:, partition_start:partition_end].detach().requires_grad_(True)\n",
        "            model[idx][0].bias.data = biases[idx][:partition_end].detach().requires_grad_(True)\n",
        "        return model\n",
        "\n",
        "    partitions = load(partitions, idx=0)\n",
        "    partitions = load(partitions, idx=1)\n",
        "    return partitions"
      ],
      "metadata": {
        "id": "uIlNNqM8iNgG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "from contextlib import contextmanager\n",
        "from queue import Queue\n",
        "from threading import Thread\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributed as dist\n",
        "\n",
        "\n",
        "from minitron.linear import ColumnParallelLinear, RowParallelLinear"
      ],
      "metadata": {
        "id": "0YKfGey3i4eO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the world of neural networks, size matters. As scaling laws suggests, the larger the model, the better the performance. But when you have a giant model that won't fit in the memory of a single device, things get complicated. This is where pipeline parallelism comes into play, acting as a super-efficient assembly line for large neural network models. In this blog post, we will walk through the concept and build a a toy pipeline parallelism gpipe from scratch."
      ],
      "metadata": {
        "id": "gUnRZ3q5Qcrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ****Naive Pipeline Parallelism vs. GPipe****\n",
        "\n",
        "Pipeline parallelism is a process that can be distilled down to a few core steps:\n",
        "\n",
        "- Step 1: **Partition the Model**: Our big model is divided into smaller partitions. Each partition corresponds to a section of the neural network and runs on a separate device.\n",
        "- Step 2: **Micro-Batching**: We split our training data mini-batch into several smaller micro-batches.\n",
        "- Step 3: **Forward and Backward Passes**: These partitions and micro-batches go through both the forward and backward computation passes.\n",
        "- Step 4: **Gradient Averaging**: Once the whole pipeline finishes, we collect the gradients and average them to update the model.\n",
        "\n",
        "To illustrate, let's imagine we have a big model with 10 layers, like a Transformer model, and we've got five devices to run our model. We want to split this model into five parts, or 'partitions', and each part will run on one device.\n",
        "\n",
        "There's a catch, though. In a Transformer model, each layer needs the result from the previous layer before it can do its work. It's like a relay race, you can't start running until you've got the baton from the runner before you. So if we split our model into 5 parts, then the second part can't start until the first part is done, the third part can't start until the second part is done, and so on. That means that most of the time, most of our devices are just sitting around doing nothing. That's a bummer!\n",
        "\n",
        "So what can we do? Here's where GPipe comes in. Instead of feeding a big batch of data to our model all at once, GPipe splits that batch into smaller chunks, which we're gonna call 'micro-batches'. And here's the trick: while one micro-batch is being processed by the second part of our model, the next micro-batch can start being processed by the first part of the model.\n",
        "\n",
        "This way, there's always something for each part of the model to do. It's like a factory assembly line. As soon as one car is done with one station, it moves to the next station and a new car moves into the first station. This keeps all our devices busy (although they might still have some idle time, like when a worker in the factory is waiting for the next car to arrive).\n",
        "\n",
        "The GPipe's scheduler orchestrates this process. It works in 'clock cycles', figuring out which partitions should be active and which micro-batch each partition should work on for each clock cycle."
      ],
      "metadata": {
        "id": "GryLZUMnQffX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cracking the Schedule Algorithm"
      ],
      "metadata": {
        "id": "NyElfUCWifuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A \"clock cycle\" is like a unit of time for our pipeline. Each clock cycle activates a new partition and passes it a micro-batch.\n",
        "\n",
        "`n_clock_cycles = n_partritions + n_microbatches - 1`\n",
        "\n",
        "If we have `m` micro-batches and `n` partitions, it'll take `m + n - 1` clock cycles to get everything through the pipeline. Why is that , because it takes `m` clock cycles for all micro-batches to pass through the first partition. Once the last micro-batch enters the first partition, it needs to go through the remaining partitions. Since there are `n` partitions, this requires `n-1` additional clock cycles because the first clock cycle is already counted when the micro-batch enters the first partition.\n",
        "\n",
        "`end_partrition = min(clock_idx+1, n_partritions)`\n",
        "\n",
        "In pipeline parallelism, for each clock cycle, a new partrition actives in the pipeline. If we are currently in `clock_idx`, it means that `clock_idx` partritions have already been actived.\n",
        "\n",
        "The next partritions will be `clock_idx+1`. However, we cannot exceed the total number of partitions (`n_partitions`), so we use the min function to limit the range.\n",
        "\n",
        "So, what happens in each clock cycle? Good question! In each clock cycle, we determine which partitions are active and what they should be working on. Our scheduler assigns tasks in the form of `(microbatch_index, partition_index)` for each clock cycle. This basically tells each device what chunk of the neural network it should process and with which micro-batch."
      ],
      "metadata": {
        "id": "7VMO8VJ9ikZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clock_cycles(n_microbatches, n_partritions):\n",
        "    n_clock_cycles = n_partritions + n_microbatches - 1\n",
        "    for clock_idx in range(n_clock_cycles):\n",
        "        start_partrition = max(clock_idx+1-n_microbatches, 0)\n",
        "        end_partrition = min(clock_idx+1, n_partritions)\n",
        "\n",
        "        tasks = []\n",
        "        for partrition_idx in range(start_partrition, end_partrition):\n",
        "            microbatch_idx = clock_idx-partrition_idx\n",
        "            task = (microbatch_idx, partrition_idx)\n",
        "            tasks.append(task)\n",
        "\n",
        "        yield tasks"
      ],
      "metadata": {
        "id": "FaZF56zJip3s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_microbatches = 5\n",
        "n_partitions = 3\n",
        "\n",
        "tasks = clock_cycles(n_microbatches, n_partitions)"
      ],
      "metadata": {
        "id": "4VCvT6VqiqnB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[task for task in tasks]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pRtGuhWiyn7",
        "outputId": "4a2e253c-3226-43e6-d492-796458a39644"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 0)],\n",
              " [(1, 0), (0, 1)],\n",
              " [(2, 0), (1, 1), (0, 2)],\n",
              " [(3, 0), (2, 1), (1, 2)],\n",
              " [(4, 0), (3, 1), (2, 2)],\n",
              " [(4, 1), (3, 2)],\n",
              " [(4, 2)]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Behind the Scenes: Worker Threads"
      ],
      "metadata": {
        "id": "qd_NJW8ei7MJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key players are worker threads. Each pipeline stage or GPU has a dedicated worker thread running in the background. This thread continually checks an input queue for new tasks. When a fresh task appears, the worker grabs it and sends it to the GPU for execution. The key is that workers run independently in threads, executing tasks as they appear in the input queue. The main thread coordinates by assigning new tasks and shuttling outputs between stages. And there you have it, pipeline parallelism.\n",
        "\n",
        "Just to demonstrate the idea, here we will only provide an implementation in which a single worker thread runs in the background and manages all pipeline stages."
      ],
      "metadata": {
        "id": "H8K0j8BUi9rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wait_and_execute(in_queue: Queue, out_queue: Queue):\n",
        "    \"\"\"Wait for a task and execute it.\"\"\"\n",
        "    while True:\n",
        "        task = in_queue.get()\n",
        "\n",
        "        try:\n",
        "            output = task()\n",
        "        except Exception:\n",
        "            raise Exception(\"task failed\")\n",
        "\n",
        "        out_queue.put(output)\n"
      ],
      "metadata": {
        "id": "TMlDbv06jAg6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Queues follow a first-in, first-out order, which is exactly what we need. Newly added tasks get executed first. Once the GPU finishes a task, the result gets placed in an output queue."
      ],
      "metadata": {
        "id": "NvG_I0JjjC_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@contextmanager\n",
        "def spawn_worker():\n",
        "    in_queue = Queue()\n",
        "    out_queue = Queue()\n",
        "\n",
        "    thread = Thread(target=wait_and_execute, args=(in_queue, out_queue), daemon=True)\n",
        "    thread.start()\n",
        "\n",
        "    yield (in_queue, out_queue)"
      ],
      "metadata": {
        "id": "oLeQYMxYjbKf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A sneak to the real worly, and distributed systems"
      ],
      "metadata": {
        "id": "6KSQuqlfjKDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say you have one node that connects to 8 GPUs. In the early training, only a few GPUs are active based on the schedule from the gpipe scheduler. But as we go to the middle of training, all of these GPUs become active. So how do we dynamically change the number of worker threads during training? Because if the number of workers is less than the number of active GPUs, then idle GPUs have to wait for at least one worker to become available, so it can pick up tasks from the queue and send them to the idle GPUs for execution.\n",
        "\n",
        "But here is the catch - in distributed systems, things fail all the time. What if worker threads fail? Okay, let's say we build a pool watcher that monitors if worker threads die and spawns new ones. Okay, but what if the pool watcher itself fails? It could cause deadlock, or even fail to execute a task. So how do we automatically retry failed tasks? You know, since in pipeline parallelism the next pipeline stage depends on the previous stage, even a single node failure could cause all other nodes to stop working..."
      ],
      "metadata": {
        "id": "sAJhWne1jNNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Issuing Tasks On-the-Fly"
      ],
      "metadata": {
        "id": "1KMGpfKbjP3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that during each clock cycle, the scheduler generates a list of active pipeline stages along with their corresponding microbatch for that cycle. So for every clock cycle, the main thread appends these tasks to the appropriate worker threads' input queues on the go.\n",
        "\n",
        "The scheduler generates a list of active pipeline stages and their corresponding microbatches for each clock cycle. The main thread is responsible for coordinating the execution across pipeline stages. For each clock cycle, the main thread appends the task for each pipeline stage to the input queue of the appropriate worker thread on-the-fly."
      ],
      "metadata": {
        "id": "XFmwpgo5jSry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self):\n",
        "    batches = self.batches\n",
        "    partitions = self.partitions\n",
        "\n",
        "    n_batches = len(batches)\n",
        "    n_partitions = len(partitions)\n",
        "\n",
        "    with spawn_worker() as (in_queue, out_queue):\n",
        "        for schedule in clock_cycles(n_batches, n_partitions):\n",
        "            self._compute(schedule, in_queue, out_queue)"
      ],
      "metadata": {
        "id": "Fl5Pfl6ljYOb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once a worker thread finishes executing a task, it places the result in its output queue. The main thread retrieves the result from the worker's output queue and enqueues it into the input queue of the worker thread responsible for the next pipeline stage. So it goes like:\n",
        "\n",
        "- Step 1: Worker finishes task, puts result in output queue\n",
        "- Step 2: Main thread takes result from output queue\n",
        "- Step 3: Main thread puts result in next worker's input queue"
      ],
      "metadata": {
        "id": "duOCXh8wjf5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _compute(self, schedule: List[Tuple[int, int]], in_queue: Queue, out_queue: Queue):\n",
        "    \"\"\"Compute the partitions.\"\"\"\n",
        "    for microbatch_idx, partition_idx in schedule:\n",
        "        batch = self.batches[microbatch_idx]\n",
        "        partrition = self.partitions[partition_idx]\n",
        "\n",
        "        def compute(batch, partrition):\n",
        "            def wrapper():\n",
        "                return partrition(batch)\n",
        "            return wrapper\n",
        "\n",
        "        in_queue.put(compute(batch, partrition))\n",
        "\n",
        "    for microbatch_idx, partition_idx in schedule:\n",
        "        output = out_queue.get()\n",
        "        # put the output back to input list\n",
        "        self.batches[microbatch_idx] = output"
      ],
      "metadata": {
        "id": "FZevUyvCjoJ3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, a `queue.get()` will block the CPU execution of the thread until there's at least one item in the queue. It then retrieves the item, removes the item from the queue, and executes the next line. Hence, after a worker thread sends its task to the GPU for execution and waits for the result, the worker thread will put the result into its output. The main thread, which works in parallel with the worker thread, also waits for its output and places it into batches, where we store the output activations."
      ],
      "metadata": {
        "id": "DQjKRq91j2nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pipeline:\n",
        "    \"\"\"A base class for pipeline.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        batches: List[torch.Tensor],\n",
        "        partitions: List[nn.Sequential],\n",
        "    ):\n",
        "        self.batches = batches\n",
        "        self.partitions = partitions"
      ],
      "metadata": {
        "id": "ayCVqK98jyx7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Pipeline.fit = fit\n",
        "Pipeline._compute = _compute"
      ],
      "metadata": {
        "id": "HDdlADmRkAjQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's test the pipeline. We will split the tests into two parts: one for the execution timeline of the forward pass and backward pass, and one to test running pipeline parallelism with tensor parallelism"
      ],
      "metadata": {
        "id": "quIhv0upksG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(rank, world_size, input_size, hidden_size, output_size, microbatches, weights, biases, outputs, weight_grads, bias_grads):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12359'\n",
        "    dist.init_process_group(\n",
        "        \"gloo\",\n",
        "        rank=rank,\n",
        "        world_size=world_size\n",
        "    )\n",
        "\n",
        "    partitions = [\n",
        "        nn.Sequential(ColumnParallelLinear(input_size, hidden_size), nn.ReLU()),\n",
        "        nn.Sequential(RowParallelLinear(hidden_size, output_size)),\n",
        "    ]\n",
        "\n",
        "    partitions = load_param(rank, world_size, weights, biases, partitions)\n",
        "    pipeline = Pipeline(microbatches, partitions)\n",
        "\n",
        "    assert pipeline.batches == microbatches\n",
        "    assert pipeline.partitions == partitions\n",
        "\n",
        "    pipeline.fit()\n",
        "\n",
        "    parallel_outputs = microbatches\n",
        "\n",
        "    for x, y in zip(outputs, parallel_outputs):\n",
        "        assert torch.allclose(x, y, rtol=0.01)\n",
        "\n",
        "    for x in parallel_outputs:\n",
        "        x.sum().backward()\n",
        "\n",
        "    for layer_idx, grad_idx in [[0, 0], [1, 1]]:\n",
        "        if layer_idx == 0:\n",
        "            partition_size = weight_grads[grad_idx].shape[0] // world_size\n",
        "            grad_chunks = torch.split(weight_grads[grad_idx], partition_size, dim=0)\n",
        "            bias_chunks = torch.split(bias_grads[grad_idx], partition_size, dim=0)\n",
        "        elif layer_idx == 1:\n",
        "            partition_size = weight_grads[grad_idx].shape[1] // world_size\n",
        "            grad_chunks = torch.split(weight_grads[grad_idx], partition_size, dim=1)\n",
        "\n",
        "        assert torch.allclose(partitions[layer_idx][0].weight.grad, grad_chunks[rank], rtol=1e-3)\n",
        "        if layer_idx == 0:\n",
        "            assert torch.allclose(partitions[layer_idx][0].bias.grad, bias_chunks[rank], rtol=1e-3)\n",
        "        else:\n",
        "            assert torch.allclose(partitions[layer_idx][0].bias.grad, bias_grads[grad_idx], rtol=1e-3)\n",
        "\n",
        "\n",
        "def test_pipeline():\n",
        "    N_MICROBATCHES = 3\n",
        "    N_PARTITIONS = 2\n",
        "\n",
        "    forward_timeline = []\n",
        "    backward_timeline = []\n",
        "\n",
        "    def backward_hook(module, grad_input, grad_output):\n",
        "        backward_timeline.append((module.microbatch_idx - 1, module.partition_idx))\n",
        "        module.microbatch_idx -= 1\n",
        "\n",
        "    class AddOne(nn.Module):\n",
        "        def __init__(self, partition_idx, is_logging):\n",
        "            super().__init__()\n",
        "            self.microbatch_idx = 0\n",
        "            self.partition_idx = partition_idx\n",
        "            self.is_logging = is_logging\n",
        "            self.net = nn.Linear(1, 1)\n",
        "            self.register_backward_hook(backward_hook)\n",
        "\n",
        "        def forward(self, x):\n",
        "            if self.is_logging:\n",
        "                forward_timeline.append((self.microbatch_idx, self.partition_idx))\n",
        "                self.microbatch_idx += 1\n",
        "\n",
        "            return self.net(x)\n",
        "\n",
        "    def create_non_parallel_model(partitions):\n",
        "        non_parallel_model = nn.Sequential(*[AddOne(partition_idx=x, is_logging=False) for x in range(len(partitions))])\n",
        "        for non_parallel_layer, original_partition in zip(non_parallel_model, partitions):\n",
        "            non_parallel_layer.load_state_dict(original_partition[0].state_dict())\n",
        "        return non_parallel_model\n",
        "\n",
        "    def create_non_parallel_batch(batch):\n",
        "        non_parallel_batch = batch.detach().clone()\n",
        "        non_parallel_batch.grad = None\n",
        "        return non_parallel_batch\n",
        "\n",
        "    def loss_func(x):\n",
        "        return x.mean()\n",
        "\n",
        "    batch = torch.arange(0, N_MICROBATCHES, dtype=torch.float32, requires_grad=True)\n",
        "    microbatches = [x.unsqueeze(0) for x in batch.unbind()]\n",
        "    partitions = [nn.Sequential(AddOne(partition_idx=x, is_logging=True)) for x in range(N_PARTITIONS)]\n",
        "\n",
        "    non_parallel_model = create_non_parallel_model(partitions)\n",
        "    non_parallel_batch = create_non_parallel_batch(batch)\n",
        "\n",
        "    pipeline = Pipeline(microbatches, partitions)\n",
        "\n",
        "    assert pipeline.batches == microbatches\n",
        "    assert pipeline.partitions == partitions\n",
        "\n",
        "    pipeline.fit()\n",
        "\n",
        "    assert forward_timeline == [(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (2, 1)] or [(0, 0), (1, 0), (0, 1), (1, 1), (2, 0), (2, 1)]\n",
        "\n",
        "    outputs = microbatches\n",
        "    non_parallel_outputs = [non_parallel_model(x.unsqueeze(0)) for x in non_parallel_batch.unbind()]\n",
        "\n",
        "    for x, y in zip(outputs, non_parallel_outputs):\n",
        "        assert torch.allclose(x, y)\n",
        "\n",
        "    for x in outputs:\n",
        "        loss = loss_func(x)\n",
        "        loss.backward()\n",
        "\n",
        "    # NOTE: In the third clock cycle, two partitions are active:\n",
        "    # (1, 0) and (0, 1). We log them based on\n",
        "    # which one finishes first, so sometimes\n",
        "    # one partition finishes before the other\n",
        "    assert backward_timeline == [(2, 1), (2, 0), (1, 1), (1, 0), (0, 1), (0, 0)] or backward_timeline == [\n",
        "        (2, 1),\n",
        "        (2, 0),\n",
        "        (1, 1),\n",
        "        (0, 1),\n",
        "        (1, 0),\n",
        "        (0, 0),\n",
        "    ]\n",
        "\n",
        "    for x in non_parallel_outputs:\n",
        "        loss = loss_func(x)\n",
        "        loss.backward()\n",
        "\n",
        "    assert batch.grad is not None\n",
        "\n",
        "    for partition in partitions:\n",
        "        for param in partition.parameters():\n",
        "            assert param.grad is not None\n",
        "\n",
        "    for partition_idx in range(N_PARTITIONS):\n",
        "        for w1, w2 in zip(partitions[partition_idx].parameters(), non_parallel_model[partition_idx].parameters()):\n",
        "            assert torch.allclose(w1.grad, w2.grad)"
      ],
      "metadata": {
        "id": "ffksX-eBkZQh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBN7maOxkqPN",
        "outputId": "b1ff3172-6800-4319-c9e8-03468e3bef5a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_tensor_parallelism_with_pipeline():\n",
        "    world_size = 4\n",
        "    batch_size, input_size, output_size = 10, 16, 12\n",
        "    hidden_size = output_size * world_size\n",
        "\n",
        "    batch = torch.randn(batch_size, input_size, dtype=torch.float32)\n",
        "    microbatches = [x.unsqueeze(0) for x in batch.unbind(dim=0)]\n",
        "\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, output_size),\n",
        "    )\n",
        "    outputs = model(batch)\n",
        "    outputs.sum().backward()\n",
        "\n",
        "    def extract_params(model):\n",
        "        weights = [deepcopy(model[0].weight.data.detach()), deepcopy(model[2].weight.data.detach())]\n",
        "        biases = [deepcopy(model[0].bias.data.detach()), deepcopy(model[2].bias.data.detach())]\n",
        "        return weights, biases\n",
        "\n",
        "    def extract_grads(model):\n",
        "        weight_grads = [\n",
        "            model[0].weight.grad.detach().requires_grad_(False),\n",
        "            model[2].weight.grad.detach().requires_grad_(False)\n",
        "        ]\n",
        "        bias_grads = [\n",
        "            model[0].bias.grad.detach().requires_grad_(False),\n",
        "            model[2].bias.grad.detach().requires_grad_(False)\n",
        "\n",
        "        ]\n",
        "        return weight_grads, bias_grads\n",
        "\n",
        "    weights, biases = extract_params(model)\n",
        "    weight_grads, bias_grads = extract_grads(model)\n",
        "\n",
        "    processes = []\n",
        "    for rank in range(world_size):\n",
        "        p = mp.Process(target=run_pipeline, args=(\n",
        "            rank, world_size,\n",
        "            input_size, hidden_size, output_size,\n",
        "            microbatches, weights, biases,\n",
        "            outputs.detach().requires_grad_(False),\n",
        "            weight_grads, bias_grads,\n",
        "        ))\n",
        "        processes.append(p)\n",
        "        p.start()\n",
        "\n",
        "    for p in processes:\n",
        "        p.join()\n"
      ],
      "metadata": {
        "id": "EmNC-EiMQfqJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tensor_parallelism_with_pipeline()"
      ],
      "metadata": {
        "id": "hPvcrntcQjr_"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}