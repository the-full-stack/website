{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f089f5cf-d837-423b-bc1d-928833f043c5",
   "metadata": {
    "id": "nK6DS4NV-Yde"
   },
   "source": [
    "## 4. Divvying up GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc3a41-f823-4c84-9dc1-6a279e19a7b5",
   "metadata": {
    "id": "jtPysHco90VJ"
   },
   "source": [
    "In order to carry out computations across multiple devices, each device initiates multiple processes, each of which handles a specific GPU. This way, each process can directly send computational tasks to its designated GPU. To make this work, Megatron-LM organizes GPUs into three groups:\n",
    "\n",
    "- Data Parallel Group: Each GPU in this group handles the same part of the model, but works on different mini-batches. During backpropagation, each GPU calculates the gradient for its part of the model. These gradients are then averaged to get the overall gradient for updating the model’s parameters.\n",
    "- Tensor Parallel Groups: In this group, each GPU handles different parts of the same layer (or multiple layers). Each GPU computes the output for its designated part and these partial outputs are combined to get the complete output of the layer.\n",
    "- Pipeline Parallel Groups: The GPUs in this group handle different stages of the forward and backward passes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39070f5b-b1b3-4c84-aeae-e0cdd80f4bf0",
   "metadata": {
    "id": "8wgsStSspcBl"
   },
   "source": [
    "### Data Parallel Groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe3d16e-89b0-45da-a3ca-f460da3b515a",
   "metadata": {
    "id": "pwMZVichqV3t"
   },
   "source": [
    "\n",
    "Megatron-LM uses three variables to set up pipeline parallelism:\n",
    "\n",
    "- `tensor_model_parallel_size`: The number of GPUs across which a layer will be split in tensor parallelism\n",
    "- `pipeline_model_parallel_size`: It represents the number of stages in the pipeline\n",
    "- `data_parallel_size`: It represents the number of model replicas in data parallelism.\n",
    "\n",
    "And then each process keeps a variable for each parallelism group to keep track of which group it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ae83d-f8fe-428b-a66b-005bbd913641",
   "metadata": {
    "id": "HCpkZNx8e-7h"
   },
   "outputs": [],
   "source": [
    "world_size = 30 # the total number of workers\n",
    "tensor_model_parallel_size = 2\n",
    "pipeline_model_parallel_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85afd6-008f-4287-a094-259a3c6a76b3",
   "metadata": {
    "id": "vK8MxEyZqb18"
   },
   "source": [
    "\n",
    "In pipeline parallelism, a model is split into `pipeline_model_parallel_size` stages.\n",
    "\n",
    "Because Megatron-LM incorporates both tensor parallelism and pipeline parallelism, so each stage has `tensor_model_parallel_size` GPUs to parallelize the tensor operations in that stage. So, the total number of GPUs required to parallelize a model would be:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16dd7b1-594e-4adf-9ba9-f749dbcf14b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqYofkMcfFM7",
    "outputId": "5f441747-daad-412a-e6fe-8558e2f143f1"
   },
   "outputs": [],
   "source": [
    "num_workers_for_each_model = tensor_model_parallel_size * pipeline_model_parallel_size\n",
    "num_workers_for_each_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c89d4-1b0a-4fdd-95a4-6941a6f8ae57",
   "metadata": {
    "id": "A2pEvJJ1qcq-"
   },
   "source": [
    "\n",
    "And then to calculate the number of model replicates in data parallelism, we divide the total number of GPUs (`world_size`) by the number of GPUs used for each model (`num_gpus_for_each_model`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cab01f-5de2-43e6-a59d-dc14f558121e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElB8YVt-fJZx",
    "outputId": "320e73c8-00d9-4d56-f089-9d91b71c3ae1"
   },
   "outputs": [],
   "source": [
    "data_parallel_size = world_size // num_workers_for_each_model\n",
    "data_parallel_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a4475-1c5f-4e49-9632-442544038825",
   "metadata": {
    "id": "1SyKMThHqhaU"
   },
   "source": [
    "\n",
    "Already, so we will have five model replicas. Next, let’s setup all data parallel groups. [[Megatron's Data Parallel Groups]](https://github.com/NVIDIA/Megatron-LM/blob/3316e811cc5335ee24c2d203416d864edcf2f7a8/megatron/core/parallel_state.py#L54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb8020-5ced-4281-b813-24a9d5831719",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcI-N6F6fMJd",
    "outputId": "d87c1242-6e6a-41fe-db19-9dcb8f5d5aa9"
   },
   "outputs": [],
   "source": [
    "num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size # 4\n",
    "\n",
    "data_parallel_groups = []\n",
    "\n",
    "for i in range(pipeline_model_parallel_size):\n",
    "    start_rank = i*num_pipeline_model_parallel_groups\n",
    "    end_rank = (i+1)*num_pipeline_model_parallel_groups\n",
    "    print(f\"stage={i}, start_rank={start_rank}, end_rank={end_rank}\")\n",
    "\n",
    "    for j in range(tensor_model_parallel_size):\n",
    "        ranks = list(range(start_rank+j, end_rank, tensor_model_parallel_size))\n",
    "        data_parallel_groups.append(ranks)\n",
    "        print(f\"partition {j}, ranks={ranks}\")\n",
    "\n",
    "    print(\"-------\")\n",
    "\n",
    "data_parallel_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac04bb-cd86-4453-86e6-6d50fbec8597",
   "metadata": {
    "id": "BinZYPN1q0Tt"
   },
   "source": [
    "\n",
    "![Data Parallel Groups](/images/megatron/megatron-gpus-allocation.jpg)\n",
    "\n",
    "Already, stay calm. Let’s break it down\n",
    "\n",
    "- `for i in range(pipeline_model_parallel_size)`: We iterate through all the stages in the pipeline\n",
    "- `for j in range(tensor_model_parallel_size)`: Within each stage, a layer is divided into `tensor_model_parallel_size` partitions. There will be `tensor_model_parallel_size` data parallel groups in each stage.\n",
    "- `range(start_rank + j, end_rank, tensor_model_parallel_size)`: We iterate through the next group each time, so the starting GPU will be `start_rank + j`. Since our model layer is divided into `tensor_model_parallel_size` parts, each part is assigned to a different GPU. This means the same part of the model in different GPUs is `tensor_model_parallel_size` ranks apart. So, by using a step size of `tensor_model_parallel_size`, we are able to get the same part of the model from different GPUs.\n",
    "\n",
    "Since discussing the setup of parallel groups for tensor parallel and pipeline parallel would make it too long, let’s assume that we have already set up all three groups. Now, the question is: How do we allocate GPUs to CPUs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6941794c-4350-45bc-b5ca-7f29a5da8e04",
   "metadata": {
    "id": "16iLC_S8ptRG"
   },
   "source": [
    "### Allocate workers to processors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab3eee-e484-46a1-a0bb-b363b9c68fec",
   "metadata": {
    "id": "XZsJaGWtqw2n"
   },
   "source": [
    "\n",
    "So, here’s the deal: a CPU starts up multiple processes. Each of these processes gets tied to a GPU because GPUs are way faster at deep learning tasks than CPUs. So, each process sends its task over to its assigned GPU. But how does a process get tied to a specific GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359ac67-586e-4d82-b62f-9cb5af857385",
   "metadata": {
    "id": "WkhBGOOPq9Vi"
   },
   "source": [
    "Well, it’s done in a round-robin way across all available GPUs. This approach makes it really flexible if you change the number of processes or GPUs. And it also works when there are more processes than GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce62414-8c98-4bbc-9d91-5b894234a7e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HfnO1h0fZyg",
    "outputId": "b935e125-2cb3-4d1b-a207-9630cfdea530"
   },
   "outputs": [],
   "source": [
    "num_gpus = 4\n",
    "process_to_gpu = []\n",
    "\n",
    "for rank in range(world_size):\n",
    "    process_to_gpu.append(rank % num_gpus)\n",
    "\n",
    "[print(f\"rank: {rank} -> gpu: {gpu}\") for rank, gpu in enumerate(process_to_gpu)];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06249d11-8da9-4849-a19b-4f5e2375d61e",
   "metadata": {
    "id": "jE6lkIPuphf2"
   },
   "source": [
    "### MPU (Model Parallel Unit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d65890-e338-44e7-a56e-86ed0161bee7",
   "metadata": {
    "id": "FufaIWcPrEFZ"
   },
   "source": [
    "![Data Parallel Groups](/images/megatron/3d-parallelism.png)\n",
    "\n",
    "\n",
    "The MPU class is the one that handles all this GPU allocation. It puts each GPU into the right parallel group, either tensor parallel, model parallel, or pipeline parallel. [[Megatron's MPU]](https://github.com/NVIDIA/Megatron-LM/blob/9288b6b73dbc6bbc58c616a2f06b38381711e847/megatron/mpu/initialize.py#L62)\n",
    "\n",
    "In a distributed training setting, all nodes run the same code. So, this GPU allocation script gets executed on all nodes in the cluster. PyTorch sets up the communication channels based on the environment variable RANK for each node. After setting up the parallel groups, the MPU class keeps track of which parallel group a CPU belongs to by storing it in a local variable.\n",
    "\n",
    "Now, the pipeline parallelism needs to be set up. But since `data_parallel_size` depends on the number of GPUs per model, so we only need two variables to initialize the pipeline: `tensor_model_parallel_size` and `pipeline_model_parallel_size`. Now let’s put them all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e872a-2301-4ce2-9420-7bb3bdc41a95",
   "metadata": {
    "id": "V0w-NUWOXtPa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "class MPU:\n",
    "    def __init__(\n",
    "        self,\n",
    "        rank,\n",
    "        world_size,\n",
    "        tensor_model_parallel_size,\n",
    "        pipeline_model_parallel_size,\n",
    "        master_addr,\n",
    "        master_port,\n",
    "        backend\n",
    "    ):\n",
    "        if not torch.distributed.is_initialized():\n",
    "            os.environ[\"MASTER_ADDR\"] = str(master_addr)\n",
    "            os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "\n",
    "            self.set_device(rank)\n",
    "            torch.distributed.init_process_group(\n",
    "                rank=rank,\n",
    "                world_size=world_size,\n",
    "                backend=backend,\n",
    "            )\n",
    "\n",
    "        current_rank = torch.distributed.get_rank()\n",
    "        world_size = torch.distributed.get_world_size()\n",
    "        self.debug = True\n",
    "\n",
    "        self.num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size\n",
    "        self._data_parallel_group = None\n",
    "\n",
    "        # init data parallel group\n",
    "        self.init_data_parallel_group(\n",
    "            rank=current_rank,\n",
    "            tensor_model_parallel_size=tensor_model_parallel_size,\n",
    "            pipeline_model_parallel_size=pipeline_model_parallel_size\n",
    "        )\n",
    "        # init tensor parallel and pipeline parallel groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82696ff-b013-4a77-b32e-5f14e4c3f2e8",
   "metadata": {
    "id": "qlSizjJ1fkCt"
   },
   "outputs": [],
   "source": [
    "def set_device(self, rank):\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 0:\n",
    "        device = rank % num_gpus\n",
    "        torch.cuda.set_device(device)\n",
    "\n",
    "\n",
    "def init_data_parallel_group(\n",
    "    self,\n",
    "    rank,\n",
    "    tensor_model_parallel_size,\n",
    "    pipeline_model_parallel_size\n",
    "):\n",
    "    for i in range(pipeline_model_parallel_size):\n",
    "        start_rank = i * self.num_pipeline_model_parallel_groups\n",
    "        end_rank = (i + 1) * self.num_pipeline_model_parallel_groups\n",
    "\n",
    "        for j in range(tensor_model_parallel_size):\n",
    "            ranks = list(range(\n",
    "                start_rank+j,\n",
    "                end_rank,\n",
    "                tensor_model_parallel_size\n",
    "            ))\n",
    "\n",
    "            if rank in ranks:\n",
    "                group = torch.distributed.new_group(ranks=ranks)\n",
    "                self._data_parallel_group = group\n",
    "\n",
    "\n",
    "MPU.set_device = set_device\n",
    "MPU.init_data_parallel_group = init_data_parallel_group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
